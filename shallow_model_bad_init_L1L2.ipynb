{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
=======
   "execution_count": 2,
>>>>>>> 92b3a9be2e1987f165b21fb1b4a07ac2f3627cde
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "\n",
    "import dlc_practical_prologue as prologue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###QUESTIONS TO TA \n",
    "Need to implement bad init & L2/L2 penalties on other files ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate pairs"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 3,
>>>>>>> 92b3a9be2e1987f165b21fb1b4a07ac2f3627cde
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000 #nb of pairs\n",
    "\n",
    "#generate pairs\n",
    "train_input, train_target, train_classes, test_input, test_target, test_classes = prologue.generate_pair_sets(N)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 4,
>>>>>>> 92b3a9be2e1987f165b21fb1b4a07ac2f3627cde
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize the input\n",
    "train_input/=255\n",
    "test_input/=255"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 5,
>>>>>>> 92b3a9be2e1987f165b21fb1b4a07ac2f3627cde
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_target = torch.empty(N,2)\n",
    "new_test_target = torch.empty(N,2)\n",
    "for i in range(N):\n",
    "    if train_target[i] == 1 :\n",
    "        new_train_target[i,0] = 0\n",
    "        new_train_target[i,1] = 1\n",
    "        \n",
    "    else:\n",
    "        new_train_target[i,0] = 1\n",
    "        new_train_target[i,1] = 0\n",
    "        \n",
    "    if test_target[i] == 1:\n",
    "        new_test_target[i,0] = 0\n",
    "        new_test_target[i,1] = 1\n",
    "        \n",
    "    else:\n",
    "        new_test_target[i,0] = 1\n",
    "        new_test_target[i,1] = 0"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 6,
>>>>>>> 92b3a9be2e1987f165b21fb1b4a07ac2f3627cde
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_shallow_model():\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(392, 400),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(400, 500),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(500, 600),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(600, 2),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
=======
   "execution_count": 7,
>>>>>>> 92b3a9be2e1987f165b21fb1b4a07ac2f3627cde
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, nb_hidden):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(392, 784, kernel_size=4)\n",
    "        self.conv2 = nn.Conv1d(784, 1568, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(256, nb_hidden)\n",
    "        self.fc2 = nn.Linear(nb_hidden, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=3))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2))\n",
    "        x = F.relu(self.fc1(x.view(-1, 256)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 8,
>>>>>>> 92b3a9be2e1987f165b21fb1b4a07ac2f3627cde
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_input, train_target,lr):\n",
    "    epoch = 25\n",
    "    eta = 0.2\n",
    "    mini_batches = 100\n",
    "    optimizer = optim.SGD(model.parameters(), lr)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    for e in range(epoch):\n",
    "        sum_loss = 0\n",
    "        \n",
    "        for b in range(0, train_input.size(0), mini_batches):\n",
    "            \n",
    "            output = model(train_input.narrow(0, b, mini_batches).reshape(mini_batches, 1, -1))\n",
    "            #print('shapes',output.squeeze(1).shape, train_target.narrow(0, b, mini_batches).shape)\n",
    "            loss = criterion(output.squeeze(1), train_target.narrow(0, b, mini_batches))\n",
    "            loss.requires_grad_()\n",
    "            #print(\"output\", output.squeeze(1), \"train\", train_target.narrow(0, b, mini_batches))\n",
    "            model.zero_grad()\n",
    "            #print(\"output\", output, \"shape\", output.shape)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_loss = sum_loss + loss.item()\n",
    "            \n",
    "            \"\"\"#print(\"LOSS\",loss.item())\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    p -= eta*p.grad\n",
    "                    #print(\"grads\", p.grad)\"\"\"\n",
    "        #print(e, sum_loss)\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
=======
   "execution_count": 9,
>>>>>>> 92b3a9be2e1987f165b21fb1b4a07ac2f3627cde
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors(model, test_input, target):\n",
    "    nb_errors = 0\n",
    "    mini_batch_size = 100\n",
    "    \n",
    "    for b in range(0, test_input.size(0), mini_batch_size):\n",
    "        output = model(test_input.narrow(0, b, mini_batch_size).reshape(mini_batch_size, 1, -1))\n",
    "        _, predicted_class = output.max(2)\n",
    "        #print(output)\n",
    "        #print(predicted_class, output, target)\n",
    "        #print(\"pred classes\",predicted_class.shape, \"output\", output.shape, \"target\", target.shape)\n",
    "        for k in range(mini_batch_size):\n",
    "            \n",
    "            if target[b + k, predicted_class[k]] <= 0:\n",
    "                nb_errors = nb_errors + 1\n",
    "\n",
    "    return nb_errors"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 15,
=======
   "execution_count": 10,
>>>>>>> 92b3a9be2e1987f165b21fb1b4a07ac2f3627cde
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "0 7.80133992433548\n",
      "1 5.569158852100372\n",
      "2 4.414204299449921\n",
      "3 3.7425976395606995\n",
      "4 3.035616874694824\n",
      "5 2.345231682062149\n",
      "6 2.2845343947410583\n",
      "7 2.276906929910183\n",
      "8 1.5679255351424217\n",
      "9 1.4029917940497398\n",
      "10 1.269764930009842\n",
      "11 1.0711813792586327\n",
      "12 1.467989757657051\n",
      "13 1.225755751132965\n",
      "14 2.0219085067510605\n",
      "15 2.0890659019351006\n",
      "16 2.4437570571899414\n",
      "17 2.2342616841197014\n",
      "18 1.8210053741931915\n",
      "19 0.9244418367743492\n",
      "20 0.46517558582127094\n",
      "21 0.26245072297751904\n",
      "22 0.15792075055651367\n",
      "23 0.05945642024744302\n",
      "24 0.6033265644218773\n",
      "STD : -1\n",
      "train error 2.20%% 22/1000\n",
      "test error 21.80%% 218/1000\n",
      "0 7.257252335548401\n",
      "1 6.516279101371765\n",
      "2 5.628133445978165\n",
      "3 4.742849707603455\n",
      "4 4.146132439374924\n",
      "5 3.94431072473526\n",
      "6 3.6273955702781677\n",
      "7 3.28507824242115\n",
      "8 3.3830168694257736\n",
      "9 2.8406858891248703\n",
      "10 2.463107466697693\n",
      "11 2.2415971159934998\n",
      "12 2.224022351205349\n",
      "13 2.5200663208961487\n",
      "14 2.0433821007609367\n",
      "15 2.073823794722557\n",
      "16 1.6224189512431622\n",
      "17 1.6603910624980927\n",
      "18 1.7207282781600952\n",
      "19 2.9219618290662766\n",
      "20 3.0239953696727753\n",
      "21 2.945360779762268\n",
      "22 2.454807862639427\n",
      "23 1.689131423830986\n",
      "24 1.2088237255811691\n",
      "STD : 0.001\n",
      "train error 3.20%% 32/1000\n",
      "test error 22.40%% 224/1000\n",
      "0 7.083018660545349\n",
      "1 5.944345563650131\n",
      "2 5.5359788835048676\n",
      "3 4.392794579267502\n",
      "4 3.7504941523075104\n",
      "5 2.9885413348674774\n",
      "6 2.3990764021873474\n",
      "7 2.642763778567314\n",
      "8 2.1643001437187195\n",
      "9 1.2549060508608818\n",
      "10 1.2266038618981838\n",
      "11 1.7014166489243507\n",
      "12 1.3593422174453735\n",
      "13 1.0484264083206654\n",
      "14 1.8140119649469852\n",
      "15 1.791243277490139\n",
      "16 2.505662225186825\n",
      "17 1.7233177050948143\n",
      "18 1.814207762479782\n",
      "19 1.3641503527760506\n",
      "20 0.9267179444432259\n",
      "21 0.609141306951642\n",
      "22 0.2501910086721182\n",
      "23 0.09330589062301442\n",
      "24 0.12831478482985403\n",
      "STD : 0.01\n",
      "train error 0.20%% 2/1000\n",
      "test error 19.60%% 196/1000\n",
      "0 40.950268387794495\n",
      "1 4.830056816339493\n",
      "2 3.591493844985962\n",
      "3 2.6245854794979095\n",
      "4 1.7265730127692223\n",
      "5 1.9194951131939888\n",
      "6 3.7871899157762527\n",
      "7 1.939150795340538\n",
      "8 1.1386423967778683\n",
      "9 1.000099092721939\n",
      "10 1.052975220605731\n",
      "11 1.6152268685400486\n",
      "12 1.7738556191325188\n",
      "13 1.9232978597283363\n",
      "14 2.066736876964569\n",
      "15 2.4061352387070656\n",
      "16 1.5830968096852303\n",
      "17 0.7844901122152805\n",
      "18 0.2936301473528147\n",
      "19 0.1452508692163974\n",
      "20 0.09363655775086954\n",
      "21 0.038765568286180496\n",
      "22 0.01971921478252625\n",
      "23 0.007185809696238721\n",
      "24 0.0027084351786470506\n",
      "STD : 0.1\n",
      "train error 0.00%% 0/1000\n",
      "test error 21.30%% 213/1000\n",
      "0 74019.24829101562\n",
      "1 18141.460815429688\n",
      "2 10854.754211425781\n",
      "3 8507.581085205078\n",
      "4 8365.566108703613\n",
      "5 4296.4123458862305\n",
      "6 1360.2537770271301\n",
      "7 196.8494794368744\n",
      "8 79.03309226036072\n",
      "9 3.0018052756786346\n",
      "10 2.768897756934166\n",
      "11 0.0\n",
      "12 0.0\n",
      "13 0.0\n",
      "14 0.0\n",
      "15 0.0\n",
      "16 0.0\n",
      "17 0.0\n",
      "18 0.0\n",
      "19 0.0\n",
      "20 0.0\n",
      "21 0.0\n",
      "22 0.0\n",
      "23 0.0\n",
      "24 0.0\n",
      "STD : 1.0\n",
      "train error 0.00%% 0/1000\n",
      "test error 21.50%% 215/1000\n",
      "0 398476752.0\n",
      "1 230593024.0\n",
      "2 167150956.0\n",
      "3 118839859.0\n",
      "4 85438328.0\n",
      "5 64459007.5\n",
      "6 49875534.5\n",
      "7 41917939.0\n",
      "8 37206406.125\n",
      "9 37061232.0\n",
      "10 28812688.6875\n",
      "11 18528867.375\n",
      "12 12182118.28125\n",
      "13 6564032.109375\n",
      "14 3845209.1142578125\n",
      "15 2117159.0444335938\n",
      "16 1312076.6684570312\n",
      "17 672464.5380859375\n",
      "18 393827.8708496094\n",
      "19 226196.13940429688\n",
      "20 150177.65113830566\n",
      "21 59801.4208984375\n",
      "22 31233.07421875\n",
      "23 8353.927734375\n",
      "24 0.0\n",
      "STD : 10.0\n",
      "train error 0.00%% 0/1000\n",
      "test error 25.10%% 251/1000\n"
=======
      "Learning rate : 0.001\n",
      "train error 44.90%% 449/1000\n",
      "test error 47.40%% 474/1000\n",
      "Learning rate : 0.005\n",
      "train error 44.90%% 449/1000\n",
      "test error 47.40%% 474/1000\n",
      "Learning rate : 0.01\n",
      "train error 44.90%% 449/1000\n",
      "test error 47.40%% 474/1000\n",
      "Learning rate : 0.05\n",
      "train error 44.90%% 449/1000\n",
      "test error 47.40%% 474/1000\n",
      "Learning rate : 0.1\n",
      "train error 22.90%% 229/1000\n",
      "test error 22.10%% 221/1000\n",
      "Learning rate : 0.5\n",
      "train error 7.10%% 71/1000\n",
      "test error 19.70%% 197/1000\n"
>>>>>>> 92b3a9be2e1987f165b21fb1b4a07ac2f3627cde
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "##########################bad initialization (gaussian)\n",
    "lr = 0.01\n",
    "\n",
    "for std in [ -1, 1e-3, 1e-2, 1e-1, 1e-0, 1e1 ]:\n",
=======
    "# good result : lr = 0.1 : no overfitting, good test error ~20%, optimizer: SGD\n",
    "\n",
    "for lr in [0.001,0.005, 0.01, 0.05, 0.1, 0.5]:\n",
>>>>>>> 92b3a9be2e1987f165b21fb1b4a07ac2f3627cde
    "    model = create_shallow_model()\n",
    "    if std > 0:\n",
    "        with torch.no_grad():\n",
    "            for p in model.parameters(): p.normal_(0, std)\n",
    "    train_model(model, train_input, new_train_target, lr)\n",
    "    nb_train_errors = compute_nb_errors(model, train_input, new_train_target)\n",
    "    nb_test_errors = compute_nb_errors(model, test_input, new_test_target)\n",
    "    print(\"STD :\", std)\n",
    "    print('train error {:0.2f}%% {:d}/{:d}'.format((100 * nb_train_errors) / train_input.size(0),\n",
    "                                                       nb_train_errors, train_input.size(0)))\n",
    "\n",
    "    print('test error {:0.2f}%% {:d}/{:d}'.format((100 * nb_test_errors) / test_input.size(0),\n",
    "                                                       nb_test_errors, test_input.size(0)))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 9,
=======
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train error 21.00%% 210/1000\n",
      "test error 22.10%% 221/1000\n",
      "train error 20.50%% 205/1000\n",
      "test error 22.30%% 223/1000\n",
      "train error 22.30%% 223/1000\n",
      "test error 21.90%% 219/1000\n",
      "train error 20.50%% 205/1000\n",
      "test error 22.80%% 228/1000\n",
      "train error 22.60%% 226/1000\n",
      "test error 21.40%% 214/1000\n",
      "train error 22.10%% 221/1000\n",
      "test error 21.70%% 217/1000\n",
      "train error 20.10%% 201/1000\n",
      "test error 21.60%% 216/1000\n",
      "train error 21.50%% 215/1000\n",
      "test error 22.20%% 222/1000\n",
      "train error 20.20%% 202/1000\n",
      "test error 21.90%% 219/1000\n",
      "train error 21.00%% 210/1000\n",
      "test error 22.30%% 223/1000\n"
     ]
    }
   ],
   "source": [
    "lr = 0.1\n",
    "\n",
    "for i in range(10):\n",
    "    model = create_shallow_model()\n",
    "    train_model(model, train_input, new_train_target, lr)\n",
    "    nb_train_errors = compute_nb_errors(model, train_input, new_train_target)\n",
    "    nb_test_errors = compute_nb_errors(model, test_input, new_test_target)\n",
    "    print('train error {:0.2f}%% {:d}/{:d}'.format((100 * nb_train_errors) / train_input.size(0),\n",
    "                                                       nb_train_errors, train_input.size(0)))\n",
    "\n",
    "    print('test error {:0.2f}%% {:d}/{:d}'.format((100 * nb_test_errors) / test_input.size(0),\n",
    "                                                       nb_test_errors, test_input.size(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate : 0.5\n",
      "train error 8.00%% 80/1000\n",
      "test error 19.90%% 199/1000\n"
     ]
    }
   ],
   "source": [
    "model = create_shallow_model()\n",
    "train_model(model, train_input, new_train_target, 0.3)\n",
    "nb_train_errors = compute_nb_errors(model, train_input, new_train_target)\n",
    "nb_test_errors = compute_nb_errors(model, test_input, new_test_target)\n",
    "print(\"Learning rate :\", lr)\n",
    "print('train error {:0.2f}%% {:d}/{:d}'.format((100 * nb_train_errors) / train_input.size(0),\n",
    "                                                   nb_train_errors, train_input.size(0)))\n",
    "\n",
    "print('test error {:0.2f}%% {:d}/{:d}'.format((100 * nb_test_errors) / test_input.size(0),\n",
    "                                                   nb_test_errors, test_input.size(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
>>>>>>> 92b3a9be2e1987f165b21fb1b4a07ac2f3627cde
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 6.580929756164551\n",
      "1 4.926900207996368\n",
      "2 4.116966634988785\n",
      "3 3.264237642288208\n",
      "4 2.3518806248903275\n",
      "5 1.5837607309222221\n",
      "6 1.6383278146386147\n",
      "7 0.6674557290971279\n",
      "8 0.7428202861919999\n",
      "9 0.4830613834783435\n",
      "10 1.228068140335381\n",
      "11 3.2640504352748394\n",
      "12 2.5662366300821304\n",
      "13 1.6057788357138634\n",
      "14 0.862783458083868\n",
      "15 0.3368785111233592\n",
      "16 0.0916593843139708\n",
      "17 0.02615104930009693\n",
      "18 0.01167623873334378\n",
      "19 0.006920088402694091\n",
      "20 0.004542273352853954\n",
      "21 0.003180042956955731\n",
      "22 0.0022755504105589353\n",
      "23 0.0016596688292338513\n",
      "24 0.0012301817150728311\n",
      "Learning rate : 0.001\n",
      "train error 0.00%% 0/1000\n",
      "test error 17.30%% 173/1000\n",
      "0 6.530229687690735\n",
      "1 5.237324982881546\n",
      "2 4.057473689317703\n",
      "3 3.074220359325409\n",
      "4 2.1467612087726593\n",
      "5 1.817570075392723\n",
      "6 1.9920425713062286\n",
      "7 1.1087472550570965\n",
      "8 0.4834884162992239\n",
      "9 0.19103442528285086\n",
      "10 0.12525236292276531\n",
      "11 0.9090851387009025\n",
      "12 0.7017223257571459\n",
      "13 0.7068292405456305\n",
      "14 0.40713976277038455\n",
      "15 0.2423475212417543\n",
      "16 0.49904884677380323\n",
      "17 0.4760927853640169\n",
      "18 0.7109684068709612\n",
      "19 0.8382590524852276\n",
      "20 0.672085796482861\n",
      "21 1.0856729969382286\n",
      "22 0.8319884706288576\n",
      "23 0.623696974478662\n",
      "24 0.5535168908536434\n",
      "Learning rate : 0.005\n",
      "train error 0.00%% 0/1000\n",
      "test error 18.50%% 185/1000\n",
      "0 8.334282457828522\n",
      "1 5.971377909183502\n",
      "2 4.355798721313477\n",
      "3 3.5821255147457123\n",
      "4 3.1023935228586197\n",
      "5 2.410156637430191\n",
      "6 2.1376958787441254\n",
      "7 2.11236235499382\n",
      "8 1.8977770060300827\n",
      "9 1.780299335718155\n",
      "10 1.7195256650447845\n",
      "11 1.3147760406136513\n",
      "12 1.1327202375978231\n",
      "13 1.0392046384513378\n",
      "14 1.2166925631463528\n",
      "15 1.3441546931862831\n",
      "16 1.2728564590215683\n",
      "17 1.1708073429763317\n",
      "18 1.4556380659341812\n",
      "19 1.1670857034623623\n",
      "20 0.6652975454926491\n",
      "21 0.5523180440068245\n",
      "22 0.4094432257115841\n",
      "23 0.2920744540169835\n",
      "24 0.29311908641830087\n",
      "Learning rate : 0.01\n",
      "train error 0.80%% 8/1000\n",
      "test error 21.00%% 210/1000\n",
      "0 549.2123863697052\n",
      "1 8.269269108772278\n",
      "2 7.602931201457977\n",
      "3 6.928936839103699\n",
      "4 6.881197929382324\n",
      "5 6.883624017238617\n",
      "6 6.884720981121063\n",
      "7 6.881496906280518\n",
      "8 6.883766770362854\n",
      "9 6.882901072502136\n",
      "10 6.88300484418869\n",
      "11 6.8832221031188965\n",
      "12 6.883055627346039\n",
      "13 6.883248329162598\n",
      "14 6.883182525634766\n",
      "15 6.883225739002228\n",
      "16 6.883242428302765\n",
      "17 6.883237540721893\n",
      "18 6.883236110210419\n",
      "19 6.883266448974609\n",
      "20 6.883232235908508\n",
      "21 6.8832443952560425\n",
      "22 6.883229672908783\n",
      "23 6.883223652839661\n",
      "24 6.883234620094299\n",
      "Learning rate : 0.05\n",
      "train error 44.90%% 449/1000\n",
      "test error 47.50%% 475/1000\n",
      "0 9592.81902885437\n",
      "1 31.222294867038727\n",
      "2 12.947248458862305\n",
      "3 6.936166822910309\n",
      "4 7.274049699306488\n",
      "5 6.914782345294952\n",
      "6 6.894595503807068\n",
      "7 6.925191700458527\n",
      "8 6.946558117866516\n",
      "9 7.013451874256134\n",
      "10 6.89859002828598\n",
      "11 6.888708829879761\n",
      "12 6.893589437007904\n",
      "13 6.884791672229767\n",
      "14 6.890512585639954\n",
      "15 6.8861759305000305\n",
      "16 6.8893542885780334\n",
      "17 6.887507736682892\n",
      "18 6.888858377933502\n",
      "19 6.888301253318787\n",
      "20 6.88876473903656\n",
      "21 6.8887128829956055\n",
      "22 6.88884049654007\n",
      "23 6.888917088508606\n",
      "24 6.888956367969513\n",
      "Learning rate : 0.1\n",
      "train error 44.90%% 449/1000\n",
      "test error 47.50%% 475/1000\n"
     ]
    }
   ],
   "source": [
    "# good result : lr = 0.01 : no overfitting, good test error ~20%\n",
    "\n",
    "################good initialization (automatic from pytorch)\n",
    "\n",
    "for lr in [0.001,0.005, 0.01, 0.05, 0.1]:\n",
    "    model = create_shallow_model()\n",
    "    train_model(model, train_input, new_train_target, lr)\n",
    "    nb_train_errors = compute_nb_errors(model, train_input, new_train_target)\n",
    "    nb_test_errors = compute_nb_errors(model, test_input, new_test_target)\n",
    "    print(\"Learning rate :\", lr)\n",
    "    print('train error {:0.2f}%% {:d}/{:d}'.format((100 * nb_train_errors) / train_input.size(0),\n",
    "                                                       nb_train_errors, train_input.size(0)))\n",
    "\n",
    "    print('test error {:0.2f}%% {:d}/{:d}'.format((100 * nb_test_errors) / test_input.size(0),\n",
    "                                                       nb_test_errors, test_input.size(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_penalties(model, train_input, train_target,lr, L1, value): #L1 = 1 if L1 // =0 if L2\n",
    "    epoch = 25\n",
    "    eta = 0.2\n",
    "    mini_batches = 100\n",
    "    optimizer = optim.Adam(model.parameters(), lr)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    for e in range(epoch):\n",
    "        sum_loss = 0\n",
    "        \n",
    "        for b in range(0, train_input.size(0), mini_batches):\n",
    "            \n",
    "            output = model(train_input.narrow(0, b, mini_batches).reshape(mini_batches, 1, -1))\n",
    "            loss = criterion(output.squeeze(1), train_target.narrow(0, b, mini_batches))\n",
    "            \n",
    "            if L1 == 1:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                with torch.no_grad():\n",
    "                    for p in model.parameters():\n",
    "                        p.sub_(p.sign() * p.abs().clamp(max = value))\n",
    "\n",
    "            else:\n",
    "                for p in model.parameters():\n",
    "                    sum_loss += value * p.pow(2).sum()\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        if not L1:\n",
    "            print(e, sum_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(8.1650, grad_fn=<AddBackward0>)\n",
      "1 tensor(15.4124, grad_fn=<AddBackward0>)\n",
      "2 tensor(19.4175, grad_fn=<AddBackward0>)\n",
      "3 tensor(21.4183, grad_fn=<AddBackward0>)\n",
      "4 tensor(22.4817, grad_fn=<AddBackward0>)\n",
      "5 tensor(23.1393, grad_fn=<AddBackward0>)\n",
      "6 tensor(23.7343, grad_fn=<AddBackward0>)\n",
      "7 tensor(24.2546, grad_fn=<AddBackward0>)\n",
      "8 tensor(24.7391, grad_fn=<AddBackward0>)\n",
      "9 tensor(25.1636, grad_fn=<AddBackward0>)\n",
      "10 tensor(25.6614, grad_fn=<AddBackward0>)\n",
      "11 tensor(26.2733, grad_fn=<AddBackward0>)\n",
      "12 tensor(26.6009, grad_fn=<AddBackward0>)\n",
      "13 tensor(26.9907, grad_fn=<AddBackward0>)\n",
      "14 tensor(27.4904, grad_fn=<AddBackward0>)\n",
      "15 tensor(27.9117, grad_fn=<AddBackward0>)\n",
      "16 tensor(28.4873, grad_fn=<AddBackward0>)\n",
      "17 tensor(29.1701, grad_fn=<AddBackward0>)\n",
      "18 tensor(29.6556, grad_fn=<AddBackward0>)\n",
      "19 tensor(30.4824, grad_fn=<AddBackward0>)\n",
      "20 tensor(31.1777, grad_fn=<AddBackward0>)\n",
      "21 tensor(32.2511, grad_fn=<AddBackward0>)\n",
      "22 tensor(33.3157, grad_fn=<AddBackward0>)\n",
      "23 tensor(34.2330, grad_fn=<AddBackward0>)\n",
      "24 tensor(35.2836, grad_fn=<AddBackward0>)\n",
      "L2 : 0.001\n",
      "train error 0.80%% 8/1000\n",
      "test error 21.20%% 212/1000\n",
      "0 tensor(16.3302, grad_fn=<AddBackward0>)\n",
      "1 tensor(30.7918, grad_fn=<AddBackward0>)\n",
      "2 tensor(38.8584, grad_fn=<AddBackward0>)\n",
      "3 tensor(42.8630, grad_fn=<AddBackward0>)\n",
      "4 tensor(45.3836, grad_fn=<AddBackward0>)\n",
      "5 tensor(47.1127, grad_fn=<AddBackward0>)\n",
      "6 tensor(48.4115, grad_fn=<AddBackward0>)\n",
      "7 tensor(49.3978, grad_fn=<AddBackward0>)\n",
      "8 tensor(50.1313, grad_fn=<AddBackward0>)\n",
      "9 tensor(50.8571, grad_fn=<AddBackward0>)\n",
      "10 tensor(51.4843, grad_fn=<AddBackward0>)\n",
      "11 tensor(52.0587, grad_fn=<AddBackward0>)\n",
      "12 tensor(52.8795, grad_fn=<AddBackward0>)\n",
      "13 tensor(53.6637, grad_fn=<AddBackward0>)\n",
      "14 tensor(54.4260, grad_fn=<AddBackward0>)\n",
      "15 tensor(55.1596, grad_fn=<AddBackward0>)\n",
      "16 tensor(56.5370, grad_fn=<AddBackward0>)\n",
      "17 tensor(57.8321, grad_fn=<AddBackward0>)\n",
      "18 tensor(59.8177, grad_fn=<AddBackward0>)\n",
      "19 tensor(60.8144, grad_fn=<AddBackward0>)\n",
      "20 tensor(61.9653, grad_fn=<AddBackward0>)\n",
      "21 tensor(62.9384, grad_fn=<AddBackward0>)\n",
      "22 tensor(63.8818, grad_fn=<AddBackward0>)\n",
      "23 tensor(64.5837, grad_fn=<AddBackward0>)\n",
      "24 tensor(64.8070, grad_fn=<AddBackward0>)\n",
      "L2 : 0.002\n",
      "train error 2.30%% 23/1000\n",
      "test error 20.40%% 204/1000\n",
      "0 tensor(32.9447, grad_fn=<AddBackward0>)\n",
      "1 tensor(61.9164, grad_fn=<AddBackward0>)\n",
      "2 tensor(78.7769, grad_fn=<AddBackward0>)\n",
      "3 tensor(87.1886, grad_fn=<AddBackward0>)\n",
      "4 tensor(91.9111, grad_fn=<AddBackward0>)\n",
      "5 tensor(95.0459, grad_fn=<AddBackward0>)\n",
      "6 tensor(97.7916, grad_fn=<AddBackward0>)\n",
      "7 tensor(100.9818, grad_fn=<AddBackward0>)\n",
      "8 tensor(103.3453, grad_fn=<AddBackward0>)\n",
      "9 tensor(103.8973, grad_fn=<AddBackward0>)\n",
      "10 tensor(106.7857, grad_fn=<AddBackward0>)\n",
      "11 tensor(109.6695, grad_fn=<AddBackward0>)\n",
      "12 tensor(112.5976, grad_fn=<AddBackward0>)\n",
      "13 tensor(114.9448, grad_fn=<AddBackward0>)\n",
      "14 tensor(117.0672, grad_fn=<AddBackward0>)\n",
      "15 tensor(119.1296, grad_fn=<AddBackward0>)\n",
      "16 tensor(120.7622, grad_fn=<AddBackward0>)\n",
      "17 tensor(123.6172, grad_fn=<AddBackward0>)\n",
      "18 tensor(126.7221, grad_fn=<AddBackward0>)\n",
      "19 tensor(128.3640, grad_fn=<AddBackward0>)\n",
      "20 tensor(130.0989, grad_fn=<AddBackward0>)\n",
      "21 tensor(132.9390, grad_fn=<AddBackward0>)\n",
      "22 tensor(136.1192, grad_fn=<AddBackward0>)\n",
      "23 tensor(138.4556, grad_fn=<AddBackward0>)\n",
      "24 tensor(139.8603, grad_fn=<AddBackward0>)\n",
      "L2 : 0.004\n",
      "train error 0.20%% 2/1000\n",
      "test error 21.20%% 212/1000\n",
      "0 tensor(40.9360, grad_fn=<AddBackward0>)\n",
      "1 tensor(76.5869, grad_fn=<AddBackward0>)\n",
      "2 tensor(96.9604, grad_fn=<AddBackward0>)\n",
      "3 tensor(107.0304, grad_fn=<AddBackward0>)\n",
      "4 tensor(113.0066, grad_fn=<AddBackward0>)\n",
      "5 tensor(117.4013, grad_fn=<AddBackward0>)\n",
      "6 tensor(121.1387, grad_fn=<AddBackward0>)\n",
      "7 tensor(124.3476, grad_fn=<AddBackward0>)\n",
      "8 tensor(127.2145, grad_fn=<AddBackward0>)\n",
      "9 tensor(129.3477, grad_fn=<AddBackward0>)\n",
      "10 tensor(131.5298, grad_fn=<AddBackward0>)\n",
      "11 tensor(135.4455, grad_fn=<AddBackward0>)\n",
      "12 tensor(139.3317, grad_fn=<AddBackward0>)\n",
      "13 tensor(141.5187, grad_fn=<AddBackward0>)\n",
      "14 tensor(145.0447, grad_fn=<AddBackward0>)\n",
      "15 tensor(148.0884, grad_fn=<AddBackward0>)\n",
      "16 tensor(151.3048, grad_fn=<AddBackward0>)\n",
      "17 tensor(153.6618, grad_fn=<AddBackward0>)\n",
      "18 tensor(157.1443, grad_fn=<AddBackward0>)\n",
      "19 tensor(160.7307, grad_fn=<AddBackward0>)\n",
      "20 tensor(163.1700, grad_fn=<AddBackward0>)\n",
      "21 tensor(165.9035, grad_fn=<AddBackward0>)\n",
      "22 tensor(169.7306, grad_fn=<AddBackward0>)\n",
      "23 tensor(172.6330, grad_fn=<AddBackward0>)\n",
      "24 tensor(174.7937, grad_fn=<AddBackward0>)\n",
      "L2 : 0.005\n",
      "train error 0.40%% 4/1000\n",
      "test error 22.30%% 223/1000\n",
      "0 tensor(81.8027, grad_fn=<AddBackward0>)\n",
      "1 tensor(152.7104, grad_fn=<AddBackward0>)\n",
      "2 tensor(194.0549, grad_fn=<AddBackward0>)\n",
      "3 tensor(215.2207, grad_fn=<AddBackward0>)\n",
      "4 tensor(229.1665, grad_fn=<AddBackward0>)\n",
      "5 tensor(238.8210, grad_fn=<AddBackward0>)\n",
      "6 tensor(247.4678, grad_fn=<AddBackward0>)\n",
      "7 tensor(255.2547, grad_fn=<AddBackward0>)\n",
      "8 tensor(259.6812, grad_fn=<AddBackward0>)\n",
      "9 tensor(269.8727, grad_fn=<AddBackward0>)\n",
      "10 tensor(278.6103, grad_fn=<AddBackward0>)\n",
      "11 tensor(286.6386, grad_fn=<AddBackward0>)\n",
      "12 tensor(293.0009, grad_fn=<AddBackward0>)\n",
      "13 tensor(300.6236, grad_fn=<AddBackward0>)\n",
      "14 tensor(306.8584, grad_fn=<AddBackward0>)\n",
      "15 tensor(314.1422, grad_fn=<AddBackward0>)\n",
      "16 tensor(327.0240, grad_fn=<AddBackward0>)\n",
      "17 tensor(340.7215, grad_fn=<AddBackward0>)\n",
      "18 tensor(352.1655, grad_fn=<AddBackward0>)\n",
      "19 tensor(361.1159, grad_fn=<AddBackward0>)\n",
      "20 tensor(367.9956, grad_fn=<AddBackward0>)\n",
      "21 tensor(374.6091, grad_fn=<AddBackward0>)\n",
      "22 tensor(379.9515, grad_fn=<AddBackward0>)\n",
      "23 tensor(383.9550, grad_fn=<AddBackward0>)\n",
      "24 tensor(386.9471, grad_fn=<AddBackward0>)\n",
      "L2 : 0.01\n",
      "train error 0.20%% 2/1000\n",
      "test error 20.70%% 207/1000\n",
      "0 tensor(164.8807, grad_fn=<AddBackward0>)\n",
      "1 tensor(310.8308, grad_fn=<AddBackward0>)\n",
      "2 tensor(392.5411, grad_fn=<AddBackward0>)\n",
      "3 tensor(434.5797, grad_fn=<AddBackward0>)\n",
      "4 tensor(459.6608, grad_fn=<AddBackward0>)\n",
      "5 tensor(475.3894, grad_fn=<AddBackward0>)\n",
      "6 tensor(486.6953, grad_fn=<AddBackward0>)\n",
      "7 tensor(497.3469, grad_fn=<AddBackward0>)\n",
      "8 tensor(511.2381, grad_fn=<AddBackward0>)\n",
      "9 tensor(521.2809, grad_fn=<AddBackward0>)\n",
      "10 tensor(538.2411, grad_fn=<AddBackward0>)\n",
      "11 tensor(557.9506, grad_fn=<AddBackward0>)\n",
      "12 tensor(575.4529, grad_fn=<AddBackward0>)\n",
      "13 tensor(588.2983, grad_fn=<AddBackward0>)\n",
      "14 tensor(602.5394, grad_fn=<AddBackward0>)\n",
      "15 tensor(612.3878, grad_fn=<AddBackward0>)\n",
      "16 tensor(616.3331, grad_fn=<AddBackward0>)\n",
      "17 tensor(626.5191, grad_fn=<AddBackward0>)\n",
      "18 tensor(642.2127, grad_fn=<AddBackward0>)\n",
      "19 tensor(663.0876, grad_fn=<AddBackward0>)\n",
      "20 tensor(680.2079, grad_fn=<AddBackward0>)\n",
      "21 tensor(695.7726, grad_fn=<AddBackward0>)\n",
      "22 tensor(713.7827, grad_fn=<AddBackward0>)\n",
      "23 tensor(738.9873, grad_fn=<AddBackward0>)\n",
      "24 tensor(758.0581, grad_fn=<AddBackward0>)\n",
      "L2 : 0.02\n",
      "train error 0.10%% 1/1000\n",
      "test error 21.10%% 211/1000\n"
     ]
    }
   ],
   "source": [
    "########L2 penalty\n",
    "lr = 0.01\n",
    "for lambda_l2 in [0.001, 0.002, 0.004, 0.005, 0.010, 0.020]:\n",
    "    model = create_shallow_model()\n",
    "    train_model_penalties(model, train_input, new_train_target, lr, 0, lambda_l2)\n",
    "    nb_train_errors = compute_nb_errors(model, train_input, new_train_target)\n",
    "    nb_test_errors = compute_nb_errors(model, test_input, new_test_target)\n",
    "    print(\"L2 :\", lambda_l2)\n",
    "    print('train error {:0.2f}%% {:d}/{:d}'.format((100 * nb_train_errors) / train_input.size(0),\n",
    "                                                       nb_train_errors, train_input.size(0)))\n",
    "\n",
    "    print('test error {:0.2f}%% {:d}/{:d}'.format((100 * nb_test_errors) / test_input.size(0),\n",
    "                                                       nb_test_errors, test_input.size(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 : 0.001\n",
      "train error 4.90%% 49/1000\n",
      "test error 22.10%% 221/1000\n",
      "L1 : 0.002\n",
      "train error 2.80%% 28/1000\n",
      "test error 22.30%% 223/1000\n",
      "L1 : 0.004\n",
      "train error 22.40%% 224/1000\n",
      "test error 29.80%% 298/1000\n",
      "L1 : 0.005\n",
      "train error 12.10%% 121/1000\n",
      "test error 25.30%% 253/1000\n",
      "L1 : 0.01\n",
      "train error 44.90%% 449/1000\n",
      "test error 47.40%% 474/1000\n",
      "L1 : 0.02\n",
      "train error 44.90%% 449/1000\n",
      "test error 47.40%% 474/1000\n"
     ]
    }
   ],
   "source": [
    "########L1 penalty\n",
    "lr = 0.01\n",
    "for lambda_l1 in [0.001, 0.002, 0.004, 0.005, 0.010, 0.020]:\n",
    "    model = create_shallow_model()\n",
    "    train_model_penalties(model, train_input, new_train_target, lr, 1, lambda_l1)\n",
    "    nb_train_errors = compute_nb_errors(model, train_input, new_train_target)\n",
    "    nb_test_errors = compute_nb_errors(model, test_input, new_test_target)\n",
    "    print(\"L1 :\", lambda_l1)\n",
    "    print('train error {:0.2f}%% {:d}/{:d}'.format((100 * nb_train_errors) / train_input.size(0),\n",
    "                                                       nb_train_errors, train_input.size(0)))\n",
    "\n",
    "    print('test error {:0.2f}%% {:d}/{:d}'.format((100 * nb_test_errors) / test_input.size(0),\n",
    "                                                       nb_test_errors, test_input.size(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
