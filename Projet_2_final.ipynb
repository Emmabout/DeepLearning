{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x2114d843448>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_set():\n",
    "    training_set = torch.empty(1000,2).uniform_(0,1)  #x et y\n",
    "    training_classes = torch.empty(1000)\n",
    "    testing_set = torch.empty(1000,2).uniform_(0,1)  #x et y\n",
    "    testing_classes = torch.empty(1000)\n",
    "\n",
    "    r = torch.empty(1,1).fill_(1/(2*math.pi)).pow(1/2)\n",
    "\n",
    "    for i in range (1000):\n",
    "        if ((training_set[i] - torch.Tensor([0.5,0.5])).pow(2).sum()).pow(1/2).item() < r.item():\n",
    "            training_classes[i] = 1\n",
    "        else:\n",
    "            training_classes[i] = 0\n",
    "\n",
    "        if ((testing_set[i] - torch.Tensor([0.5,0.5])).pow(2).sum()).pow(1/2).item() < r.item():\n",
    "            testing_classes[i] = 1\n",
    "        else:\n",
    "            testing_classes[i] = 0\n",
    "    return training_set, training_classes, testing_set, testing_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set, training_classes, testing_set, testing_classes = generate_set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LINEAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LINEAR MODULE (FULLY CONNECTED LAYER)\n",
    "class Linear(object):\n",
    "    def __init__(self, nb_data_in, nb_data_out):\n",
    "        k = math.sqrt(1/nb_data_in)\n",
    "        self.weight = torch.empty(nb_data_out,nb_data_in).uniform_(-k,k)\n",
    "        self.bias = torch.empty(nb_data_out,1).uniform_(-k,k)\n",
    "        self.grad_weight = None\n",
    "        self.grad_bias = None\n",
    "        self.input = None\n",
    "        \n",
    "    def updateparam(self, lr):\n",
    "        for i in range(len(self.grad_weight)): \n",
    "            self.weight -= lr * self.grad_weight\n",
    "            self.bias -= lr * self.grad_bias\n",
    "    \n",
    "    def forward(self , input):\n",
    "        self.input = input\n",
    "        y_correct_dim = ((self.weight).matmul(input.t())+self.bias)\n",
    "        return y_correct_dim.t()\n",
    "    \n",
    "    def backward(self, gradwrtoutput):\n",
    "        gradaccumulated = gradwrtoutput.matmul(self.weight)\n",
    "        self.grad_bias = gradwrtoutput.t()\n",
    "        self.grad_weight = gradwrtoutput.t().matmul(self.input)\n",
    "        return gradaccumulated\n",
    "        \n",
    "    def param(self):\n",
    "        output = [[self.weight, self.grad_weight], [self.bias, self.grad_bias]]\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RELU MODULE\n",
    "class ReLU():\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return torch.max(input,torch.zeros_like(input))\n",
    "        \n",
    "    def backward(self, gradwrtoutput): \n",
    "        dx = ((self.input)>=0).float()\n",
    "        return dx*gradwrtoutput \n",
    "\n",
    "    def param(self): \n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TANH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TANH MODULE\n",
    "class Tanh():\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return torch.tanh(input)\n",
    "    \n",
    "    def backward(self, gradwrtoutput):\n",
    "        return (1 - torch.tanh(self.input).pow(2))*gradwrtoutput \n",
    "\n",
    "    def param(self):\n",
    "        return [] #Pas de param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOSSMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOSSMSE MODULE\n",
    "    \n",
    "class LossMSE():\n",
    "    \n",
    "    def forward(self, input, target): \n",
    "        loss = torch.mean((input-target).pow(2))\n",
    "        return loss\n",
    "        \n",
    "    def backward(self, input, target):\n",
    "        target = target.unsqueeze(0)\n",
    "        #print(type(input), input.size(), input, type(target),target.size(), target)\n",
    "        dloss = (2*(input - target))/(input.size(1))\n",
    "        return dloss\n",
    "\n",
    "    def param(self):\n",
    "        return [] #No param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_into_2(classes):\n",
    "    t = torch.empty(classes.size(0),2).zero_()\n",
    "    for n in range (classes.size(0)):\n",
    "        t[n,int(classes[n].item())] = 1\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEQUENTIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential():\n",
    "    def __init__(self, *input):\n",
    "        self.input = input\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for inp in self.input:\n",
    "            x = inp.forward(x)\n",
    "        return x\n",
    "        \n",
    "    def backward(self, d_loss): \n",
    "        for inp in reversed(self.input):\n",
    "            d_loss = inp.backward(d_loss)\n",
    "            \n",
    "    def update(self,lr):\n",
    "        for inp in self.input:\n",
    "            if hasattr(inp, 'updateparam'):\n",
    "                inp.updateparam(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, input, target, lr):\n",
    "    loss = LossMSE()\n",
    "    liste = [0]*nb_epochs\n",
    "    for e in range(nb_epochs):\n",
    "        sum_loss = 0\n",
    "        for b in range(input.shape[0]):\n",
    "            output = model.forward(input[b].unsqueeze(0))\n",
    "            d_loss = loss.backward(output, target[b])\n",
    "            \n",
    "            model.backward(d_loss)\n",
    "            model.update(lr)\n",
    "        \n",
    "            sum_loss += loss.forward(output, target[b])\n",
    "        liste[e] = sum_loss\n",
    "        #print(e,sum_loss)\n",
    "        \n",
    "    plt.plot(liste)\n",
    "    plt.title(\"Evolution of Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors(model, input, target):\n",
    "    nb_errors = 0\n",
    "\n",
    "    for j in range(input.shape[0]):\n",
    "        output = model.forward(input[j].unsqueeze(0))\n",
    "        pred = output.squeeze().max(0)[1].item()\n",
    "        #print(output)\n",
    "        if target[j, pred].item() < 0.5: \n",
    "                nb_errors = nb_errors + 1\n",
    "    return nb_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progressBar(current, total, barLength = 20):\n",
    "    percent = float(current) * 100 / total\n",
    "    arrow   = '-' * int(percent/100 * barLength - 1) + '>'\n",
    "    spaces  = ' ' * (barLength - len(arrow))\n",
    "    print('Progress: [%s%s] %d %%' % (arrow, spaces, percent), end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "###ANALYSE THE RESULTS\n",
    "def analyse_results(train_errors, test_errors):\n",
    "    input_size = 1000\n",
    "    train_errors = torch.Tensor(train_errors)\n",
    "    test_errors = torch.Tensor(test_errors)\n",
    "    print('Average train error {:0.2f}% {:0.2f}/{:d}'.format((100 * train_errors.mean()) / input_size,\n",
    "                                                          train_errors.mean(), input_size))\n",
    "    print(\"Train error standard deviation : {:0.2f}%\".format((100 * train_errors.std()) / input_size,\n",
    "                                                          train_errors.std(), input_size))\n",
    "\n",
    "    print('Average test error {:0.2f}% {:0.2f}/{:d}'.format((100 * test_errors.mean()) / input_size,\n",
    "                                                          test_errors.mean(), input_size))\n",
    "    print(\"Test error standard deviation : {:0.2f}%\".format((100 * test_errors.std()) / input_size,\n",
    "                                                          test_errors.std(), input_size))\n",
    "\n",
    "    train_err = [x*100 / input_size for x in train_errors]\n",
    "    test_err = [x*100 / input_size for x in test_errors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: [----->              ] 33 %\r"
     ]
    }
   ],
   "source": [
    "nb_iterations = 3\n",
    "progressBar(0,nb_iterations)\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "for i in range (nb_iterations):\n",
    "    training_set, training_classes, testing_set, testing_classes = generate_set()\n",
    "    train_target = class_into_2(training_classes);\n",
    "    test_target = class_into_2(testing_classes);\n",
    "\n",
    "    model = Sequential(Linear(2,25),\n",
    "                       Tanh(),\n",
    "                       Linear(25,2))\n",
    "    lr = 0.01\n",
    "    nb_epochs = 5\n",
    "\n",
    "    train_model(model, training_set, train_target, lr)\n",
    "\n",
    "    nb_train_error = compute_nb_errors(model, training_set, train_target)\n",
    "    nb_test_error = compute_nb_errors(model, testing_set, test_target)\n",
    "    progressBar(i+1,nb_iterations)\n",
    "    \n",
    "    train_errors.append(nb_train_error)\n",
    "    test_errors.append(nb_test_error)\n",
    "\n",
    "plt.show()\n",
    "analyse_results(train_errors, test_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set, training_classes, testing_set, testing_classes = generate_set()\n",
    "train_target = class_into_2(training_classes);\n",
    "test_target = class_into_2(testing_classes);\n",
    "\n",
    "model = Sequential(Linear(2,25),\n",
    "                   ReLU(),\n",
    "                   Linear(25,2))\n",
    "lr = 0.01\n",
    "nb_epochs = 15\n",
    "\n",
    "train_model(model, training_set, train_target, lr)\n",
    "\n",
    "train_errors = compute_nb_errors(model, training_set, train_target)\n",
    "test_errors = compute_nb_errors(model, testing_set, test_target)\n",
    "\n",
    "print('Train error {:0.2f}% {:0.2f}/{:d}'.format((100 * train_errors) / training_set.size(0),\n",
    "                                                          train_errors, training_set.size(0)))\n",
    "print('Test error {:0.2f}% {:0.2f}/{:d}'.format((100 * test_errors) / testing_set.size(0),\n",
    "                                                          test_errors, testing_set.size(0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set, training_classes, testing_set, testing_classes = generate_set()\n",
    "train_target = class_into_2(training_classes);\n",
    "test_target = class_into_2(testing_classes);\n",
    "\n",
    "model = Sequential(Linear(2,25),\n",
    "                   ReLU(),\n",
    "                   Linear(25,25),\n",
    "                   ReLU(),\n",
    "                   Linear(25,25),\n",
    "                   ReLU(),\n",
    "                   Linear(25,25),\n",
    "                   ReLU(),\n",
    "                   Linear(25,2))\n",
    "lr = 0.01\n",
    "nb_epochs = 15\n",
    "\n",
    "train_model(model, training_set, train_target, lr)\n",
    "\n",
    "train_errors = compute_nb_errors(model, training_set, train_target)\n",
    "test_errors = compute_nb_errors(model, testing_set, test_target)\n",
    "\n",
    "print('Train error {:0.2f}% {:0.2f}/{:d}'.format((100 * train_errors) / training_set.size(0),\n",
    "                                                          train_errors, training_set.size(0)))\n",
    "print('Test error {:0.2f}% {:0.2f}/{:d}'.format((100 * test_errors) / testing_set.size(0),\n",
    "                                                          test_errors, testing_set.size(0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
