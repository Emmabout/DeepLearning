{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x1d8e7aa4588>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LINEAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LINEAR MODULE (FULLY CONNECTED LAYER)\n",
    "#mini batch : x[mini_batch_size * D]D=2\n",
    "class Linear(object):\n",
    "    def __init__(self, dimension, nb_data_in, nb_data_out):\n",
    "        #x = nb_data_in x dim in my calculation, but here transpose (dim x nb_data_in)\n",
    "        #y = nb_data_out x dim in my calculation, but here transpose (dim x nb_data_out)\n",
    "        k = math.sqrt(1/nb_data_in)\n",
    "        self.weight = torch.empty(nb_data_out,nb_data_in).uniform_(-k,k)\n",
    "        self.bias = torch.empty(nb_data_out,dimension).uniform_(-k,k)\n",
    "        self.grad_weight = None\n",
    "        self.grad_bias = None\n",
    "        self.input = None\n",
    "        \n",
    "    def updateparam(self, lr):\n",
    "        for i in range(len(g_w)): \n",
    "            self.weight -= lr * self.grad_weight[i]\n",
    "            self.bias -= lr * self.grad_bias[i]\n",
    "    \n",
    "    def forward(self , *input, nb_layer):\n",
    "        #All the calculation were done considering x = nb_data_in x dim (as seen in the lesson)\n",
    "        #So to simplify the comprehension, we do a first transpose, do the the calculations with this\n",
    "        #then transpose again after, to have the correct dimension output\n",
    "\n",
    "        #save x with dim = dim x nb_data_in\n",
    "        if(nb_layer > 1):\n",
    "            input = input[0]\n",
    "            #pour le premier, on aura en entrée un ou plusieurs tensors (donne tuple quand rentré dans f)\n",
    "        \n",
    "        #save x with dim = dim x nb_data_in    \n",
    "        self.input = input\n",
    "        output = []\n",
    "        \n",
    "        for x in input:\n",
    "            #x = dim x nb_data_in -> nb_data_in x dim\n",
    "            x_correct_dim = x.t()\n",
    "            #y = nb_data_out x dim\n",
    "            y_correct_dim = None\n",
    "\n",
    "            #dim analysis: [nb_data_out x nb_data_in] x [nb_data_in x dim] + [nb_data_out x dim]\n",
    "            y_correct_dim = ((self.weight).matmul(x_correct_dim)+self.bias)\n",
    "\n",
    "            #append y = [dim x nb_data_out] to respect lin module\n",
    "            output.append(y_correct_dim.t()) \n",
    "        return tuple(output)\n",
    "    \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        gradwrtoutput = gradwrtoutput[0]\n",
    "        dl_dx  = None\n",
    "        dl_dw = []\n",
    "        dl_db = []\n",
    "        gradaccumulated = []\n",
    "        \n",
    "        for i in range (len(gradwrtoutput)):\n",
    "            #X = dim x nb_data_in -> nb_data_in x dim\n",
    "            x_correct_dim = self.input[i].t()\n",
    "            \n",
    "            #X = dim x nb_data_out -> nb_data_out x dim\n",
    "            grad_correct_dim = gradwrtoutput[i].t()\n",
    "            \n",
    "            #dl_dx = w^T x dl_dy (gradwrtoutput)\n",
    "            #dim analysis: nb_data_in x dim = [nb_data_in x nb_data_out] x [nb_data_out x dim]\n",
    "            dl_dx = (self.weight).t().matmul(grad_correct_dim)\n",
    "            #nb_data_in x dim -> dim x nb_data_in\n",
    "            gradaccumulated.append(dl_dx.t())\n",
    "            \n",
    "            #dl_db = dl_dy\n",
    "            #dim analysis: nb_data_out x dim (car b = nb_data_out x dim)\n",
    "            dl_db.append(grad_correct_dim)\n",
    "            \n",
    "            #dl_dw = dl_dy x X^T\n",
    "            #dim analysis: nb_data_out x nb_data_in = [nb_data_out x dim] x [dim x nb_data_in]\n",
    "            dl_dw.append(grad_correct_dim.matmul(x_correct_dim.t()))\n",
    "            \n",
    "        self.grad_weight = dl_dw\n",
    "        self.grad_bias = dl_db\n",
    "\n",
    "        return tuple(gradaccumulated)\n",
    "        \n",
    "    def param(self):\n",
    "        output = [[self.weight, self.grad_weight], [self.bias, self.grad_bias]]\n",
    "        return output\n",
    "    \n",
    "    #https://pytorch.org/docs/stable/nn.html#linear-layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "(tensor([[0.4168, 0.6865]]),)\n",
      "dim =  1  input = 2\n",
      "nb_out =  3\n",
      "forward\n",
      "(tensor([[ 0.2704, -0.6901, -0.2762]]),)\n"
     ]
    }
   ],
   "source": [
    "a = torch.empty(1,2).normal_()\n",
    "l = []\n",
    "l.append(a)\n",
    "i = tuple(l)\n",
    "print('input')\n",
    "print(i)\n",
    "\n",
    "print('dim = ', i[0].size(0),' input =', i[0].size(1))\n",
    "nb_out = 3\n",
    "print('nb_out = ', nb_out)\n",
    "\n",
    "lin = Linear(i[0].size(0),i[0].size(1),nb_out)\n",
    "f = lin.forward(i,nb_layer = 2)\n",
    "print('forward')\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad\n",
      "(tensor([[ 0.2142,  1.3402, -0.6543]]),)\n",
      "gradaccumulated\n",
      "(tensor([[-0.0925, -0.2774]]),)\n"
     ]
    }
   ],
   "source": [
    "g = []\n",
    "grad = torch.empty(i[0].size(0),nb_out).normal_()\n",
    "g.append(grad)\n",
    "gr = tuple(g)\n",
    "print('grad')\n",
    "print(gr)\n",
    "output = lin.backward(gr)\n",
    "print('gradaccumulated')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RELU MODULE\n",
    "class ReLU():\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "    \n",
    "    def forward(self, *input):\n",
    "        input = input[0]\n",
    "        liste = []\n",
    "        self.input = input\n",
    "\n",
    "        for x in input:\n",
    "            liste.append(torch.max(x,torch.zeros_like(x))) \n",
    "        output = tuple(liste)\n",
    "        return output\n",
    "        \n",
    "    def backward(self, *gradwrtoutput): \n",
    "        #add input as param ? \n",
    "        #No, parameters are w and b when linear, but input is saved has if we have to give backward and input\n",
    "        #in the case they're both tuple of tensors of size not know\n",
    "        #it's complicated (maybe impossible) to code it with the * (let the size be whatever we want)\n",
    "        gradwrtoutput = gradwrtoutput[0]\n",
    "        derivative = []\n",
    "        gradaccumulated = []\n",
    "\n",
    "        for x in self.input:\n",
    "            dx = (x>=0).float()\n",
    "            derivative.append(dx)\n",
    "        for i in range (len(derivative)):\n",
    "            gradaccumulated.append(derivative[i]*gradwrtoutput[i])\n",
    "        output = tuple(gradaccumulated)\n",
    "        return output   \n",
    "\n",
    "    def param(self): \n",
    "        return []\n",
    "    \n",
    "#backward should get as input a tensor or a tuple of tensors containing the gradient of the \n",
    "#loss with respect to the module’s output, accumulate the gradient wrt the parameters, \n",
    "#and return a tensor or a tuple of tensors containing the gradient of the loss wrt the module’s input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "(tensor([[ 0.6993, -0.6388]]),)\n",
      "dim =  1  input = 2\n",
      "nb_out =  3\n",
      "forward linear\n",
      "(tensor([[ 0.3345,  0.3300, -0.4223]]),)\n",
      "forward ReLU\n",
      "(tensor([[0.3345, 0.3300, 0.0000]]),)\n"
     ]
    }
   ],
   "source": [
    "a = torch.empty(1,2).normal_()\n",
    "l = []\n",
    "l.append(a)\n",
    "i = tuple(l)\n",
    "print('input')\n",
    "print(i)\n",
    "\n",
    "print('dim = ', i[0].size(0),' input =', i[0].size(1))\n",
    "nb_out = 3\n",
    "print('nb_out = ', nb_out)\n",
    "\n",
    "lin = Linear(i[0].size(0),i[0].size(1),nb_out)\n",
    "f = lin.forward(i,nb_layer = 2)\n",
    "print('forward linear')\n",
    "print(f)\n",
    "\n",
    "R = ReLU()\n",
    "r = R.forward(f)\n",
    "print('forward ReLU')\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad\n",
      "(tensor([[ 0.4437,  0.4750, -0.5213]]),)\n",
      "grad acc ReLu\n",
      "(tensor([[0.4437, 0.4750, -0.0000]]),)\n",
      "grad acc Lin\n",
      "(tensor([[ 0.4498, -0.1868]]),)\n"
     ]
    }
   ],
   "source": [
    "g = []\n",
    "grad = torch.empty(r[0].size(0),r[0].size(1)).normal_()\n",
    "g.append(grad)\n",
    "gr = tuple(g)\n",
    "print('grad')\n",
    "print(gr)\n",
    "\n",
    "output = R.backward(gr)\n",
    "print('grad acc ReLu')\n",
    "print(output)\n",
    "\n",
    "out = lin.backward(output)\n",
    "print('grad acc Lin')\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TANH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TANH MODULE\n",
    "class Tanh():\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        \n",
    "    def forward(self, *input):\n",
    "        input = input[0]\n",
    "        output = []\n",
    "        self.input = input\n",
    "        for x in input:\n",
    "            output.append(torch.tanh(x)) \n",
    "        return tuple(output)\n",
    "    \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        gradwrtoutput = gradwrtoutput[0]\n",
    "        output = []\n",
    "        for i in range (len(gradwrtoutput)):\n",
    "            # derivative = (1 - torch.tanh(x).pow(2))\n",
    "            output.append((1 - torch.tanh(self.input[i]).pow(2))*gradwrtoutput[i])\n",
    "        return tuple(output)   \n",
    "\n",
    "    def param(self):\n",
    "        return [] #Pas de param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "(tensor([[ 0.7674, -1.0612]]),)\n",
      "dim =  1  input = 2\n",
      "nb_out =  3\n",
      "forward linear\n",
      "(tensor([[-0.9336, -0.2106,  0.9268]]),)\n",
      "forward Tanh\n",
      "(tensor([[-0.7323, -0.2075,  0.7291]]),)\n"
     ]
    }
   ],
   "source": [
    "a = torch.empty(1,2).normal_()\n",
    "l = []\n",
    "l.append(a)\n",
    "i = tuple(l)\n",
    "print('input')\n",
    "print(i)\n",
    "\n",
    "print('dim = ', i[0].size(0),' input =', i[0].size(1))\n",
    "nb_out = 3\n",
    "print('nb_out = ', nb_out)\n",
    "\n",
    "lin = Linear(i[0].size(0),i[0].size(1),nb_out)\n",
    "f = lin.forward(i,nb_layer = 2)\n",
    "print('forward linear')\n",
    "print(f)\n",
    "\n",
    "T = Tanh()\n",
    "t = T.forward(f)\n",
    "print('forward Tanh')\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad\n",
      "(tensor([[-0.4403, -0.3640, -1.7230]]),)\n",
      "grad acc Tanh\n",
      "(tensor([[-0.2042, -0.3483, -0.8071]]),)\n",
      "grad acc Lin\n",
      "(tensor([[-0.5901, -0.0546]]),)\n"
     ]
    }
   ],
   "source": [
    "g = []\n",
    "grad = torch.empty(r[0].size(0),r[0].size(1)).normal_()\n",
    "g.append(grad)\n",
    "gr = tuple(g)\n",
    "print('grad')\n",
    "print(gr)\n",
    "\n",
    "output = T.backward(gr)\n",
    "print('grad acc Tanh')\n",
    "print(output)\n",
    "\n",
    "out = lin.backward(output)\n",
    "print('grad acc Lin')\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOSSMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOSSMSE MODULE\n",
    "    \n",
    "class LossMSE():\n",
    "    #def __init__(self):\n",
    "        #Besoin d'aucun je pense\n",
    "        #self.loss = None\n",
    "        #self.gradloss = None\n",
    "    \n",
    "    def forward(self, *input, target): \n",
    "        input = input[0]\n",
    "        #Si plusieurs input et target comment on fait?\n",
    "        #On aura un tuple de tensor qui consistera l'input, puis target = tuple of tensor target\n",
    "        liste = torch.Tensor([])\n",
    "        #Utilisation de cette écriture pour pouvoir faire somme sur \"liste\" (l = [] marche pas avec sum())\n",
    "        for i in range (len(input)):\n",
    "            liste = torch.cat((liste,(input[i]-target[i]).pow(2)))\n",
    "        loss = torch.sum(liste)/len(input)\n",
    "        #self.loss = loss même pas besoin\n",
    "        return loss\n",
    "        \n",
    "    def backward(self, *input, target):\n",
    "        gradaccumulated = []\n",
    "        input = input[0]\n",
    "        dloss = []\n",
    "        \n",
    "        for i,x in enumerate(input):\n",
    "            dloss.append(2*(x - target[i])/len(input))\n",
    "        return tuple(dloss)\n",
    "\n",
    "    def param(self):\n",
    "        return [] #No param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "(tensor([[1., 1.]]), tensor([[2., 2.]]))\n",
      "target\n",
      "(tensor([[-1., -1.]]), tensor([[-1., -1.]]))\n",
      "loss\n",
      "tensor(13.)\n",
      "dloss\n",
      "(tensor([[2., 2.]]), tensor([[3., 3.]]))\n"
     ]
    }
   ],
   "source": [
    "N = LossMSE()\n",
    "x = torch.empty(1,2).fill_(1)\n",
    "y = torch.empty(1,2).fill_(2)\n",
    "l = []\n",
    "l.append(x)\n",
    "l.append(y)\n",
    "i = tuple(l)\n",
    "print(\"input\")\n",
    "print(i)\n",
    "\n",
    "t1 = torch.empty(1,2).fill_(-1)\n",
    "t2 = torch.empty(1,2).fill_(-1)\n",
    "ta = []\n",
    "ta.append(t1)\n",
    "ta.append(t2)\n",
    "t = tuple(ta)\n",
    "print(\"target\")\n",
    "print(t)\n",
    "lo = N.forward(i, target = t)\n",
    "print('loss')\n",
    "print(lo)\n",
    "dl = N.backward(i, target= t)\n",
    "print('dloss')\n",
    "print(dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 1 layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "(tensor([[-0.1575, -1.2043]]),)\n",
      "dim =  1  input = 2\n",
      "nb_out =  3\n",
      "forward linear\n",
      "(tensor([[-0.0053, -0.3449,  0.0054]]),)\n",
      "forward Tanh\n",
      "(tensor([[-0.0053, -0.3319,  0.0054]]),)\n"
     ]
    }
   ],
   "source": [
    "a = torch.empty(1,2).normal_()\n",
    "l = []\n",
    "l.append(a)\n",
    "i = tuple(l)\n",
    "print('input')\n",
    "print(i)\n",
    "\n",
    "print('dim = ', i[0].size(0),' input =', i[0].size(1))\n",
    "nb_out = 3\n",
    "print('nb_out = ', nb_out)\n",
    "\n",
    "lin = Linear(i[0].size(0),i[0].size(1),nb_out)\n",
    "f = lin.forward(i,nb_layer = 2)\n",
    "print('forward linear')\n",
    "print(f)\n",
    "\n",
    "T = Tanh()\n",
    "output = T.forward(f)\n",
    "print('forward Tanh')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "(tensor([[-1., -1., -1.]]),)\n",
      "loss\n",
      "tensor(2.4467)\n",
      "dloss\n",
      "(tensor([[1.9895, 1.3362, 2.0108]]),)\n",
      "grad acc Tanh\n",
      "(tensor([[1.9894, 1.1891, 2.0108]]),)\n",
      "grad acc Lin\n",
      "(tensor([[1.7043, 1.6799]]),)\n"
     ]
    }
   ],
   "source": [
    "N = LossMSE()\n",
    "t1 = torch.empty(output[0].size(0),output[0].size(1)).fill_(-1)\n",
    "\n",
    "ta = []\n",
    "ta.append(t1)\n",
    "t = tuple(ta)\n",
    "print(\"target\")\n",
    "print(t)\n",
    "\n",
    "lo = N.forward(output, target = t)\n",
    "print('loss')\n",
    "print(lo)\n",
    "dl = N.backward(output, target= t)\n",
    "print('dloss')\n",
    "print(dl)\n",
    "\n",
    "backt = T.backward(dl)\n",
    "print('grad acc Tanh')\n",
    "print(backt)\n",
    "\n",
    "gradtot = lin.backward(backt)\n",
    "print('grad acc Lin')\n",
    "print(gradtot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1713, 0.3169],\n",
      "        [0.5001, 0.4302],\n",
      "        [0.3824, 0.2675]])\n",
      "[tensor([[-0.3134, -2.3958],\n",
      "        [-0.1873, -1.4319],\n",
      "        [-0.3168, -2.4215]])]\n",
      "tensor([[0.4034],\n",
      "        [0.2519],\n",
      "        [0.3878]])\n",
      "[tensor([[1.9894],\n",
      "        [1.1891],\n",
      "        [2.0108]])]\n"
     ]
    }
   ],
   "source": [
    "[w,g_w],[b,g_b] = lin.param()\n",
    "print(w)\n",
    "print(g_w)\n",
    "print(b)\n",
    "print(g_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 1 layer SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "(tensor([[-2.2146, -0.6439]]),)\n",
      "dim =  1  input = 2\n",
      "nb_out =  3\n",
      "forward linear\n",
      "(tensor([[1.8012, 1.6285, 1.9005]]),)\n",
      "forward Tanh\n",
      "(tensor([[0.9469, 0.9258, 0.9563]]),)\n",
      "\n",
      "target\n",
      "(tensor([[-1., -1., -1.]]),)\n",
      "loss\n",
      "tensor(11.3265)\n",
      "dloss\n",
      "(tensor([[3.8939, 3.8517, 3.9126]]),)\n",
      "grad acc Tanh\n",
      "(tensor([[0.4023, 0.5500, 0.3347]]),)\n",
      "grad acc Lin\n",
      "(tensor([[-0.6299, -0.3469]]),)\n",
      "\n",
      "learning rate\n",
      "0.1\n",
      "w, g_w, b and g_b\n",
      "tensor([[-0.5408,  0.1284],\n",
      "        [-0.4650, -0.3815],\n",
      "        [-0.4679, -0.5637]])\n",
      "[tensor([[-0.8909, -0.2590],\n",
      "        [-1.2181, -0.3542],\n",
      "        [-0.7412, -0.2155]])]\n",
      "tensor([[0.6862],\n",
      "        [0.3531],\n",
      "        [0.5013]])\n",
      "[tensor([[0.4023],\n",
      "        [0.5500],\n",
      "        [0.3347]])]\n",
      "\n",
      "tensor([[-0.5408,  0.1284],\n",
      "        [-0.4650, -0.3815],\n",
      "        [-0.4679, -0.5637]])\n",
      "tensor([[0.6862],\n",
      "        [0.3531],\n",
      "        [0.5013]])\n",
      "w and b updated manually\n",
      "tensor([[-0.7190,  0.0766],\n",
      "        [-0.7086, -0.4524],\n",
      "        [-0.6161, -0.6068]])\n",
      "tensor([[0.7667],\n",
      "        [0.4631],\n",
      "        [0.5682]])\n",
      "w and b updated function\n",
      "tensor([[-0.7190,  0.0766],\n",
      "        [-0.7086, -0.4524],\n",
      "        [-0.6161, -0.6068]])\n",
      "tensor([[0.7667],\n",
      "        [0.4631],\n",
      "        [0.5682]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.empty(1,2).normal_()\n",
    "l = []\n",
    "l.append(a)\n",
    "i = tuple(l)\n",
    "print('input')\n",
    "print(i)\n",
    "\n",
    "print('dim = ', i[0].size(0),' input =', i[0].size(1))\n",
    "nb_out = 3\n",
    "print('nb_out = ', nb_out)\n",
    "\n",
    "lin = Linear(i[0].size(0),i[0].size(1),nb_out)\n",
    "f = lin.forward(i,nb_layer = 2)\n",
    "print('forward linear')\n",
    "print(f)\n",
    "\n",
    "T = Tanh()\n",
    "output = T.forward(f)\n",
    "print('forward Tanh')\n",
    "print(output)\n",
    "\n",
    "N = LossMSE()\n",
    "t1 = torch.empty(output[0].size(0),output[0].size(1)).fill_(-1)\n",
    "\n",
    "print()\n",
    "ta = []\n",
    "ta.append(t1)\n",
    "t = tuple(ta)\n",
    "print(\"target\")\n",
    "print(t)\n",
    "\n",
    "lo = N.forward(output, target = t)\n",
    "print('loss')\n",
    "print(lo)\n",
    "dl = N.backward(output, target= t)\n",
    "print('dloss')\n",
    "print(dl)\n",
    "\n",
    "backt = T.backward(dl)\n",
    "print('grad acc Tanh')\n",
    "print(backt)\n",
    "\n",
    "gradtot = lin.backward(backt)\n",
    "print('grad acc Lin')\n",
    "print(gradtot)\n",
    "\n",
    "print()\n",
    "print('learning rate')\n",
    "lr = 0.1\n",
    "print(lr)\n",
    "[[wp,g_w],[bp,g_b]] = lin.param()\n",
    "print('w, g_w, b and g_b')\n",
    "print(wp)\n",
    "print(g_w)\n",
    "print(bp)\n",
    "print(g_b)\n",
    "\n",
    "print()\n",
    "\n",
    "lin.updateparam(lr)\n",
    "\n",
    "for i in range(len(g_w)): \n",
    "    wp += lr * g_w[i]\n",
    "    bp += lr * g_b[i]\n",
    "    \n",
    "print('w and b updated manually')\n",
    "print(wp)\n",
    "print(bp)\n",
    "\n",
    "print('w and b updated function')\n",
    "[[w,g_w],[b,g_b]] = lin.param()\n",
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient 2 layer Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "(tensor([[-0.8601,  2.3773]]),)\n",
      "dim =  1  input = 2\n",
      "to dim =  1  nb_out_1 = 3\n",
      "to dim =  1  nb_out_2 = 4\n",
      "25\n",
      "\n",
      "iteration 0\n",
      "loss\n",
      "tensor(4.7981)\n",
      "\n",
      "iteration 1\n",
      "loss\n",
      "tensor(3.2041)\n",
      "\n",
      "iteration 2\n",
      "loss\n",
      "tensor(1.7052)\n",
      "\n",
      "iteration 3\n",
      "loss\n",
      "tensor(0.3651)\n",
      "\n",
      "iteration 4\n",
      "loss\n",
      "tensor(0.2186)\n",
      "\n",
      "iteration 5\n",
      "loss\n",
      "tensor(0.1624)\n",
      "\n",
      "iteration 6\n",
      "loss\n",
      "tensor(0.1304)\n",
      "\n",
      "iteration 7\n",
      "loss\n",
      "tensor(0.1093)\n",
      "\n",
      "iteration 8\n",
      "loss\n",
      "tensor(0.0941)\n",
      "\n",
      "iteration 9\n",
      "loss\n",
      "tensor(0.0828)\n",
      "\n",
      "iteration 10\n",
      "loss\n",
      "tensor(0.0738)\n",
      "\n",
      "iteration 11\n",
      "loss\n",
      "tensor(0.0667)\n",
      "\n",
      "iteration 12\n",
      "loss\n",
      "tensor(0.0608)\n",
      "\n",
      "iteration 13\n",
      "loss\n",
      "tensor(0.0558)\n",
      "\n",
      "iteration 14\n",
      "loss\n",
      "tensor(0.0516)\n",
      "\n",
      "iteration 15\n",
      "loss\n",
      "tensor(0.0480)\n",
      "\n",
      "iteration 16\n",
      "loss\n",
      "tensor(0.0449)\n",
      "\n",
      "iteration 17\n",
      "loss\n",
      "tensor(0.0421)\n",
      "\n",
      "iteration 18\n",
      "loss\n",
      "tensor(0.0397)\n",
      "\n",
      "iteration 19\n",
      "loss\n",
      "tensor(0.0375)\n",
      "\n",
      "iteration 20\n",
      "loss\n",
      "tensor(0.0355)\n",
      "\n",
      "iteration 21\n",
      "loss\n",
      "tensor(0.0338)\n",
      "\n",
      "iteration 22\n",
      "loss\n",
      "tensor(0.0322)\n",
      "\n",
      "iteration 23\n",
      "loss\n",
      "tensor(0.0307)\n",
      "\n",
      "iteration 24\n",
      "loss\n",
      "tensor(0.0294)\n"
     ]
    }
   ],
   "source": [
    "a = torch.empty(1,2).normal_()\n",
    "l = []\n",
    "l.append(a)\n",
    "inp = tuple(l)\n",
    "print('input')\n",
    "print(inp)\n",
    "\n",
    "print('dim = ', inp[0].size(0),' input =', inp[0].size(1))\n",
    "nb_out1 = 3\n",
    "print('to dim = ', inp[0].size(0),' nb_out_1 =', nb_out1)\n",
    "nb_out2 = 4\n",
    "print('to dim = ', inp[0].size(0),' nb_out_2 =', nb_out2)\n",
    "\n",
    "\n",
    "nb_iterations = 25\n",
    "print(nb_iterations)\n",
    "\n",
    "lin1 = Linear(inp[0].size(0),inp[0].size(1),nb_out1)\n",
    "lin2 = Linear(inp[0].size(0),nb_out1,nb_out2)\n",
    "T1 = Tanh()\n",
    "T2 = Tanh()\n",
    "N = LossMSE()\n",
    "\n",
    "for i in range (nb_iterations):\n",
    "    print()\n",
    "    print('iteration', i)\n",
    "    \n",
    "    f1 = lin1.forward(inp,nb_layer = 2)\n",
    "    t1 = T1.forward(f1)\n",
    "    f2 = lin2.forward(t1,nb_layer = 3)\n",
    "    output = T2.forward(f2)\n",
    "\n",
    "    t1 = torch.empty(output[0].size(0),output[0].size(1)).fill_(-1)\n",
    "    ta = []\n",
    "    ta.append(t1)\n",
    "    t = tuple(ta)\n",
    "    lo = N.forward(output, target = t)\n",
    "    print('loss')\n",
    "    print(lo)\n",
    "    dl = N.backward(output, target= t)\n",
    "    \n",
    "    bt2 = T2.backward(dl)\n",
    "    bf2 = lin2.backward(bt2)\n",
    "    bt1 = T1.backward(bf2)\n",
    "    bf1 = lin1.backward(bt1)\n",
    "\n",
    "    lr = 0.1\n",
    "    lin1.updateparam(lr)\n",
    "    lin2.updateparam(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient 2 layer ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "(tensor([[ 0.7204, -1.3981]]),)\n",
      "dim =  1  input = 2\n",
      "to dim =  1  nb_out_1 = 3\n",
      "to dim =  1  nb_out_2 = 4\n",
      "25\n",
      "\n",
      "iteration 0\n",
      "loss\n",
      "tensor(4.0806)\n",
      "\n",
      "iteration 1\n",
      "loss\n",
      "tensor(4.0346)\n",
      "\n",
      "iteration 2\n",
      "loss\n",
      "tensor(4.)\n",
      "\n",
      "iteration 3\n",
      "loss\n",
      "tensor(4.)\n",
      "\n",
      "iteration 4\n",
      "loss\n",
      "tensor(4.)\n",
      "\n",
      "iteration 5\n",
      "loss\n",
      "tensor(4.)\n",
      "\n",
      "iteration 6\n",
      "loss\n",
      "tensor(4.)\n",
      "\n",
      "iteration 7\n",
      "loss\n",
      "tensor(4.)\n",
      "\n",
      "iteration 8\n",
      "loss\n",
      "tensor(4.)\n",
      "\n",
      "iteration 9\n",
      "loss\n",
      "tensor(4.)\n",
      "\n",
      "iteration 10\n",
      "loss\n",
      "tensor(4.)\n",
      "\n",
      "iteration 11\n",
      "loss\n",
      "tensor(4.)\n",
      "\n",
      "iteration 12\n",
      "loss\n",
      "tensor(4.)\n",
      "\n",
      "iteration 13\n",
      "loss\n",
      "tensor(4.)\n",
      "\n",
      "iteration 14\n",
      "loss\n",
      "tensor(4.)\n",
      "\n",
      "iteration 15\n",
      "loss\n",
      "tensor(4.)\n",
      "\n",
      "iteration 16\n",
      "loss\n",
      "tensor(4.)\n",
      "\n",
      "iteration 17\n",
      "loss\n",
      "tensor(4.)\n",
      "\n",
      "iteration 18\n",
      "loss\n",
      "tensor(4.)\n",
      "\n",
      "iteration 19\n",
      "loss\n",
      "tensor(4.)\n",
      "\n",
      "iteration 20\n",
      "loss\n",
      "tensor(4.)\n",
      "\n",
      "iteration 21\n",
      "loss\n",
      "tensor(4.)\n",
      "\n",
      "iteration 22\n",
      "loss\n",
      "tensor(4.)\n",
      "\n",
      "iteration 23\n",
      "loss\n",
      "tensor(4.)\n",
      "\n",
      "iteration 24\n",
      "loss\n",
      "tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "a = torch.empty(1,2).normal_()\n",
    "l = []\n",
    "l.append(a)\n",
    "inp = tuple(l)\n",
    "print('input')\n",
    "print(inp)\n",
    "\n",
    "print('dim = ', inp[0].size(0),' input =', inp[0].size(1))\n",
    "nb_out1 = 3\n",
    "print('to dim = ', inp[0].size(0),' nb_out_1 =', nb_out1)\n",
    "nb_out2 = 4\n",
    "print('to dim = ', inp[0].size(0),' nb_out_2 =', nb_out2)\n",
    "\n",
    "\n",
    "nb_iterations = 25\n",
    "print(nb_iterations)\n",
    "\n",
    "lin1 = Linear(inp[0].size(0),inp[0].size(1),nb_out1)\n",
    "lin2 = Linear(inp[0].size(0),nb_out1,nb_out2)\n",
    "R1 = ReLU()\n",
    "R2 = ReLU()\n",
    "N = LossMSE()\n",
    "\n",
    "for i in range (nb_iterations):\n",
    "    print()\n",
    "    print('iteration', i)\n",
    "    \n",
    "    f1 = lin1.forward(inp,nb_layer = 2)\n",
    "    t1 = R1.forward(f1)\n",
    "    f2 = lin2.forward(t1,nb_layer = 3)\n",
    "    output = R2.forward(f2)\n",
    "\n",
    "    t1 = torch.empty(output[0].size(0),output[0].size(1)).fill_(-1)\n",
    "    ta = []\n",
    "    ta.append(t1)\n",
    "    t = tuple(ta)\n",
    "    lo = N.forward(output, target = t)\n",
    "    print('loss')\n",
    "    print(lo)\n",
    "    dl = N.backward(output, target= t)\n",
    "    \n",
    "    br2 = R2.backward(dl)\n",
    "    bf2 = lin2.backward(br2)\n",
    "    br1 = R1.backward(bf2)\n",
    "    bf1 = lin1.backward(br1)\n",
    "\n",
    "    lr = 0.01\n",
    "    lin1.updateparam(lr)\n",
    "    lin2.updateparam(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEQUENTIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
