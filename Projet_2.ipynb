{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x2f5b500c9c8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build networks combining fully connected layers, Tanh, and ReLU,\n",
    "#run the forward and backward passes,\n",
    "#optimize parameters with SGD for MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LINEAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LINEAR MODULE (FULLY CONNECTED LAYER)\n",
    "class Linear(object):\n",
    "    def __init__(self):\n",
    "        k_root = math.sqrt(1/in_feature)\n",
    "        self.weight = torch.empty(in_feature,out_feature).uniform_(-k_p,k_p)\n",
    "        self.bias = torch.empty(out_feature).uniform_(-k_p,k_p)\n",
    "        self.grad_weight = None\n",
    "        self.grad_bias = None\n",
    "    \n",
    "    def forward(self , *input, in_feature, out_feature):\n",
    "        if(isinstance(input, tuple)):\n",
    "            liste = []\n",
    "            t = input[0]\n",
    "            for x in t:\n",
    "                liste.append(x.matmul(weight)+bias) \n",
    "            output = tuple(liste)\n",
    "        else:\n",
    "            output = input.matmul(weight)+bias  \n",
    "        \n",
    "    def backward(self , *gradwrtoutput):\n",
    "        derivative = []\n",
    "        gradaccumulated = []\n",
    "        for x in self.input:\n",
    "            #derivative.append(1 - torch.tanh(x).pow(2))\n",
    "\n",
    "        for i in range (len(derivative)):\n",
    "            #gradaccumulated.append(derivative[i]*gradwrtoutput[i])\n",
    "        output = tuple(gradaccumulated)\n",
    "        return output \n",
    "        \n",
    "    def param(self):\n",
    "        output = [self.weight, self.grad_weight, self.bias, self.grad_bias]\n",
    "        return output\n",
    "    \n",
    "    #https://pytorch.org/docs/stable/nn.html#linear-layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0678, -0.0091],\n",
      "        [ 0.3138,  0.0206],\n",
      "        [ 0.0406,  0.1020],\n",
      "        [-0.0521, -0.2344],\n",
      "        [ 0.0756, -0.2297],\n",
      "        [ 0.3056,  0.0490],\n",
      "        [-0.0034,  0.0890],\n",
      "        [ 0.0847,  0.1470],\n",
      "        [ 0.2353, -0.0367],\n",
      "        [ 0.3008, -0.2754]])\n",
      "tensor([ 0.2924, -0.2238])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 4, 7, 2])\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.empty(3,4,7,10).normal_()\n",
    "print(x1)\n",
    "in_feature = 10\n",
    "out_feature = 2\n",
    "k = 1/in_feature\n",
    "k_p = math.sqrt(k)\n",
    "y = torch.empty(in_feature,out_feature).uniform_(-k_p,k_p)\n",
    "print(y)\n",
    "bias = torch.empty(out_feature).uniform_(-k_p,k_p)\n",
    "print(bias)\n",
    "print(bias.size())\n",
    "z1 = x1.matmul(y) + bias\n",
    "print(z1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RELU (problème dérivée)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RELU MODULE\n",
    "class ReLU():\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "    \n",
    "    def forward(self, *input): \n",
    "        liste = []\n",
    "        self.input = input\n",
    "        for x in input:\n",
    "            liste.append(torch.max(x,torch.empty(x.size(0), x.size(1)).fill_(0))) \n",
    "        output = tuple(liste)\n",
    "        return output\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        derivative = []\n",
    "        gradaccumulated = []\n",
    "        for x in self.input:\n",
    "            derivative.append((x.sign()+1)/2) \n",
    "            #Si >0, sign donne 1, +1 /2 donc 1 (d(x)/dx = 1)\n",
    "            #Si =0, sign donne 0, +1 /2 donc 0.5 => Normalement on a pas de cas qui vaut 0, mais à voir\n",
    "            #Si <0, sign donne -1, +1 /2 donc 0 (d(0)/dx = 0) \n",
    "        for i in range (len(derivative)):\n",
    "            gradaccumulated.append(derivative[i]*gradwrtoutput[i])\n",
    "        output = tuple(gradaccumulated)\n",
    "        return output   \n",
    "\n",
    "    def param(self): \n",
    "        return []\n",
    "    \n",
    "#backward should get as input a tensor or a tuple of tensors containing the gradient of the \n",
    "#loss with respect to the module’s output, accumulate the gradient wrt the parameters, \n",
    "#and return a tensor or a tuple of tensors containing the gradient of the loss wrt the module’s input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.]])\n",
      "tensor([[-2., -2.]])\n",
      "tensor([[2., 2.]])\n",
      "tensor([[2., 2.]])\n",
      "ReLU of x, y\n",
      "(tensor([[3., 3.]]), tensor([[0., 0.]]))\n",
      "grad\n",
      "(tensor([[2., 2.]]), tensor([[0., 0.]]))\n"
     ]
    }
   ],
   "source": [
    "T = ReLU()\n",
    "x = torch.empty(1,2).fill_(3)\n",
    "y = torch.empty(1,2).fill_(-2)\n",
    "grad1 = torch.empty(1,2).fill_(2)\n",
    "grad2 = torch.empty(1,2).fill_(2)\n",
    "print(x)\n",
    "print(y)\n",
    "print(grad1)\n",
    "print(grad2)\n",
    "t = T.forward(x, y)\n",
    "print('ReLU of x, y')\n",
    "print(t)\n",
    "grad = T.backward(grad1,grad2)\n",
    "print('grad')\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TANH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TANH MODULE\n",
    "class Tanh():\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        \n",
    "    def forward(self, *input): \n",
    "        liste = []\n",
    "        self.input = input\n",
    "        for x in input:\n",
    "            liste.append(torch.tanh(x)) \n",
    "        output = tuple(liste)\n",
    "        return output\n",
    "    \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        derivative = []\n",
    "        gradaccumulated = []\n",
    "        for x in self.input:\n",
    "            derivative.append(1 - torch.tanh(x).pow(2))\n",
    "\n",
    "        for i in range (len(derivative)):\n",
    "            gradaccumulated.append(derivative[i]*gradwrtoutput[i])\n",
    "        output = tuple(gradaccumulated)\n",
    "        return output   \n",
    "\n",
    "    def param(self):\n",
    "        return [] #Pas de param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.]])\n",
      "tensor([[2., 2.]])\n",
      "tensor([[2., 2.]])\n",
      "tensor([[2., 2.]])\n",
      "tanh of x, y\n",
      "(tensor([[0.7616, 0.7616]]), tensor([[0.9640, 0.9640]]))\n",
      "grad\n",
      "(tensor([[0.8399, 0.8399]]), tensor([[0.1413, 0.1413]]))\n"
     ]
    }
   ],
   "source": [
    "T = Tanh()\n",
    "x = torch.empty(1,2).fill_(1)\n",
    "y = torch.empty(1,2).fill_(2)\n",
    "grad1 = torch.empty(1,2).fill_(2)\n",
    "grad2 = torch.empty(1,2).fill_(2)\n",
    "print(x)\n",
    "print(y)\n",
    "print(grad1)\n",
    "print(grad2)\n",
    "t = T.forward(x, y)\n",
    "print('tanh of x, y')\n",
    "print(t)\n",
    "grad = T.backward(grad1,grad2)\n",
    "print('grad')\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOSSMSE (normalement juste, toujours à la fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOSSMSE MODULE\n",
    "#Maybe: convert to list/tensor the tuple\n",
    "    \n",
    "class LossMSE():\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.gradloss = None\n",
    "    \n",
    "    def forward(self, *input, target): #Si plusieurs input et target comment on fait?\n",
    "        liste = []\n",
    "        for i in range (len(input)):\n",
    "            liste.append((input[i]-target[i]).pow(2)) \n",
    "        output = tuple(liste)\n",
    "        self.loss = output\n",
    "        return output\n",
    "        \n",
    "    def backward(self):\n",
    "        gradaccumulated = []\n",
    "        for x in self.loss:\n",
    "            gradaccumulated.append(2*((x).pow(1/2))) #on obtient 2*(sqrt((x-target).pow(2))) du coup 2*(x-target)\n",
    "        output = tuple(gradaccumulated)      \n",
    "        self.gradloss = output\n",
    "        return output\n",
    "\n",
    "    def param(self):\n",
    "        return [] #No param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.]])\n",
      "tensor([[2., 2.]])\n",
      "tensor([[-1., -1.]])\n",
      "tensor([[-1., -1.]])\n",
      "loss\n",
      "(tensor([[4., 4.]]), tensor([[9., 9.]]))\n",
      "dloss\n",
      "(tensor([[4., 4.]]), tensor([[6., 6.]]))\n"
     ]
    }
   ],
   "source": [
    "N = LossMSE()\n",
    "x = torch.empty(1,2).fill_(1)\n",
    "y = torch.empty(1,2).fill_(2)\n",
    "print(x)\n",
    "print(y)\n",
    "t = torch.empty(1,2).fill_(-1)\n",
    "t2 = torch.empty(1,2).fill_(-1)\n",
    "print(t)\n",
    "print(t2)\n",
    "l = N.forward(x, y , target= [t, t2])\n",
    "print('loss')\n",
    "print(l)\n",
    "dl = N.backward()\n",
    "print('dloss')\n",
    "print(dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEQUENTIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SEQUENTIAL MODULE (to combine several modules in basic sequential structure)\n",
    "class Sequential(object):\n",
    "    def forward(self , *input):\n",
    "        raise NotImplementedError\n",
    "    def backward(self , *gradwrtoutput):\n",
    "        raise NotImplementedError\n",
    "    def param(self): \n",
    "        return []\n",
    "    \n",
    "#forward should get for input, and returns, a tensor or a tuple of tensors.\n",
    "\n",
    "#backward should get as input a tensor or a tuple of tensors containing the gradient of the \n",
    "#loss with respect to the module’s output, accumulate the gradient wrt the parameters, \n",
    "#and return a tensor or a tuple of tensors containing the gradient of the loss wrt the module’s input.\n",
    "\n",
    "#param should return a list of pairs, each composed of a parameter tensor, and a gradient tensor of same size. \n",
    "#This list should be empty for parameterless modules (e.g. ReLU).\n",
    "#Some modules may requires additional methods, and some modules may keep track of information from the forward \n",
    "#pass to be used in the backward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = torch.empty(1000,2).uniform_(0,1)  #x et y\n",
    "training_classes = torch.empty(1000,1)\n",
    "testing_set = torch.empty(1000,2).uniform_(0,1)  #x et y\n",
    "testing_classes = torch.empty(1000,1)\n",
    "\n",
    "r = torch.empty(1,1).fill_(1/(2*math.pi)).pow(1/2)\n",
    "\n",
    "for i in range (1000):\n",
    "    if (training_set[i].pow(2).sum()).pow(1/2).item() < r.item():\n",
    "        training_classes[i] = 1\n",
    "    else:\n",
    "        training_classes[i] = 0\n",
    "    \n",
    "    if (testing_set[i].pow(2).sum()).pow(1/2).item() < r.item():\n",
    "        testing_classes[i] = 1\n",
    "    else:\n",
    "        testing_classes[i] = 0\n",
    "\n",
    "#builds a network with two input units, two output units, three hidden layers of 25 units,\n",
    "#trains it with MSE, logging the loss,\n",
    "#computes and prints the ﬁnal train and the test errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
