{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x1b5791f5788>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build networks combining fully connected layers, Tanh, and ReLU,\n",
    "#run the forward and backward passes,\n",
    "#optimize parameters with SGD for MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    def forward(self , *input):\n",
    "        raise NotImplementedError\n",
    "    def backward(self , *gradwrtoutput):\n",
    "        raise NotImplementedError\n",
    "    def param(self): \n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LINEAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LINEAR MODULE (FULLY CONNECTED LAYER)\n",
    "class Linear(object):\n",
    "    def __init__(self):\n",
    "        k_root = math.sqrt(1/in_feature)\n",
    "        self.weight = torch.empty(in_feature,out_feature).uniform_(-k_p,k_p)\n",
    "        self.bias = torch.empty(out_feature).uniform_(-k_p,k_p)\n",
    "    \n",
    "    def forward(self , *input, in_feature, out_feature):\n",
    "        if(isinstance(input, tuple)):\n",
    "            liste = []\n",
    "            t = input[0]\n",
    "            for x in t:\n",
    "                liste.append(x.matmul(weight)+bias) \n",
    "            output = tuple(liste)\n",
    "        else:\n",
    "            output = input.matmul(weight)+bias  \n",
    "        \n",
    "    def backward(self , *gradwrtoutput):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def param(self):\n",
    "        output = [self.weight, self.grad_weight, self.bias, self.grad_bias]\n",
    "        [bias, grad_bias]\n",
    "        return []\n",
    "    \n",
    "    #https://pytorch.org/docs/stable/nn.html#linear-layers\n",
    "    \n",
    "#forward should get for input, and returns, a tensor or a tuple of tensors.\n",
    "\n",
    "#backward should get as input a tensor or a tuple of tensors containing the gradient of the \n",
    "#loss with respect to the module’s output, accumulate the gradient wrt the parameters, \n",
    "#and return a tensor or a tuple of tensors containing the gradient of the loss wrt the module’s input.\n",
    "\n",
    "#param should return a list of pairs, each composed of a parameter tensor, and a gradient tensor of same size. \n",
    "#This list should be empty for parameterless modules (e.g. ReLU).\n",
    "#Some modules may requires additional methods, and some modules may keep track of information from the forward \n",
    "#pass to be used in the backward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0678, -0.0091],\n",
      "        [ 0.3138,  0.0206],\n",
      "        [ 0.0406,  0.1020],\n",
      "        [-0.0521, -0.2344],\n",
      "        [ 0.0756, -0.2297],\n",
      "        [ 0.3056,  0.0490],\n",
      "        [-0.0034,  0.0890],\n",
      "        [ 0.0847,  0.1470],\n",
      "        [ 0.2353, -0.0367],\n",
      "        [ 0.3008, -0.2754]])\n",
      "tensor([ 0.2924, -0.2238])\n",
      "torch.Size([2])\n",
      "torch.Size([3, 4, 7, 2])\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.empty(3,4,7,10).normal_()\n",
    "print(x1)\n",
    "in_feature = 10\n",
    "out_feature = 2\n",
    "k = 1/in_feature\n",
    "k_p = math.sqrt(k)\n",
    "y = torch.empty(in_feature,out_feature).uniform_(-k_p,k_p)\n",
    "print(y)\n",
    "bias = torch.empty(out_feature).uniform_(-k_p,k_p)\n",
    "print(bias)\n",
    "print(bias.size())\n",
    "z1 = x1.matmul(y) + bias\n",
    "print(z1.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-146-5aa742f72be8>, line 25)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-146-5aa742f72be8>\"\u001b[1;36m, line \u001b[1;32m25\u001b[0m\n\u001b[1;33m    for (x,y) in (liste, self.grad)\u001b[0m\n\u001b[1;37m                                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#RELU MODULE\n",
    "class ReLU():\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.grad = None\n",
    "    \n",
    "    def forward(self, *input): \n",
    "        liste = []\n",
    "        self.input = input\n",
    "        for x in input:\n",
    "            liste.append(torch.max(x,torch.empty(x.size(0), x.size(1)).fill_(0))) \n",
    "        output = tuple(liste)\n",
    "        return output\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        liste = []\n",
    "        self.grad = gradwrtoutput\n",
    "        for x in self.input:\n",
    "            liste.append((x.sign()+1)/2) \n",
    "            #Si >0, sign donne 1, +1 /2 donc 1 (d(x)/dx = 1)\n",
    "            #Si =0, sign donne 0, +1 /2 donc 0.5 => Normalement on a pas de cas qui vaut 0, mais à voir\n",
    "            #Si <0, sign donne -1, +1 /2 donc 0 (d(0)/dx = 0) \n",
    "        print('liste')\n",
    "        print(tuple(liste))\n",
    "        for (x,y) in (liste, self.grad)\n",
    "            new.append(x+y)\n",
    "        print('liste+grad')\n",
    "        print(tuple(new))\n",
    "        #output = tuple(liste)\n",
    "        return new   \n",
    "\n",
    "    def param(self): \n",
    "        return []\n",
    "    \n",
    "#backward should get as input a tensor or a tuple of tensors containing the gradient of the \n",
    "#loss with respect to the module’s output, accumulate the gradient wrt the parameters, \n",
    "#and return a tensor or a tuple of tensors containing the gradient of the loss wrt the module’s input.\n",
    "\n",
    "#param should return a list of pairs, each composed of a parameter tensor, and a gradient tensor of same size. \n",
    "#This list should be empty for parameterless modules (e.g. ReLU).\n",
    "#Some modules may requires additional methods, and some modules may keep track of information from the forward \n",
    "#pass to be used in the backward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.]])\n",
      "tensor([[-2., -2.]])\n",
      "ReLU of x, y\n",
      "(tensor([[1., 1.]]), tensor([[0., 0.]]))\n",
      "liste\n",
      "(tensor([[1., 1.]]), tensor([[0., 0.]]))\n",
      "liste+grad\n",
      "[tensor([[1., 1.]]), tensor([[0., 0.]]), tensor([[0., 0.]]), tensor([[0., 0.]])]\n",
      "grad\n",
      "(tensor([[1., 1.]]), tensor([[0., 0.]]), tensor([[0., 0.]]), tensor([[0., 0.]]))\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "T = ReLU()\n",
    "x = torch.empty(1,2).fill_(1)\n",
    "y = torch.empty(1,2).fill_(-2)\n",
    "grad1 = torch.empty(1,2).fill_(0)\n",
    "grad2 = torch.empty(1,2).fill_(0)\n",
    "print(x)\n",
    "print(y)\n",
    "t = T.forward(x, y)\n",
    "print('ReLU of x, y')\n",
    "print(t)\n",
    "grad = T.backward(grad1,grad2)\n",
    "print('grad')\n",
    "print(grad)\n",
    "p = T.param()\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.]]) tensor([[True, True]])\n",
      "tensor([[-3., -3.]]) tensor([[0., 0.]])\n",
      "tensor([[0., 0.]]) tensor([[0.5000, 0.5000]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(1,2).fill_(1)\n",
    "y = torch.empty(1,2).fill_(-3)\n",
    "z = torch.empty(1,2).fill_(0)\n",
    "t = (x > torch.empty(x.size()).fill_(0))\n",
    "t2 = (y.sign()+1)/2\n",
    "t3 = (z.sign()+1)/2\n",
    "print(x, t)\n",
    "print(y, t2)\n",
    "print(z, t3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TANH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TANH MODULE\n",
    "class Tanh():\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        \n",
    "    def forward(self, *input): \n",
    "        liste = []\n",
    "        self.input = input\n",
    "        for x in input:\n",
    "            liste.append(torch.tanh(x)) \n",
    "        output = tuple(liste)\n",
    "        return output\n",
    "    \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        liste = []\n",
    "        for x in self.input:\n",
    "            liste.append(1 - torch.tanh(x).pow(2))\n",
    "        output = gradwrtoutput*tuple(liste)\n",
    "        return output\n",
    "\n",
    "    def param(self):\n",
    "        return [] #Pas de param\n",
    "\n",
    "\n",
    "#forward should get for input, and returns, a tensor or a tuple of tensors.\n",
    "\n",
    "#backward should get as input a tensor or a tuple of tensors containing the gradient of the \n",
    "#loss with respect to the module’s output, accumulate the gradient wrt the parameters, \n",
    "#and return a tensor or a tuple of tensors containing the gradient of the loss wrt the module’s input.\n",
    "\n",
    "#param should return a list of pairs, each composed of a parameter tensor, and a gradient tensor of same size. \n",
    "#This list should be empty for parameterless modules (e.g. ReLU).\n",
    "#Some modules may requires additional methods, and some modules may keep track of information from the forward \n",
    "#pass to be used in the backward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.]])\n",
      "tensor([[2., 2.]])\n",
      "tanh of x, y\n",
      "(tensor([[0.7616, 0.7616]]), tensor([[0.9640, 0.9640]]))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't multiply sequence by non-int of type 'tuple'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-179-f721966591b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tanh of x, y'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'grad'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-178-d9b7382dfbee>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, *gradwrtoutput)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[0mliste\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradwrtoutput\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mliste\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'tuple'"
     ]
    }
   ],
   "source": [
    "T = Tanh()\n",
    "x = torch.empty(1,2).fill_(1)\n",
    "y = torch.empty(1,2).fill_(2)\n",
    "print(x)\n",
    "print(y)\n",
    "t = T.forward(x, y)\n",
    "print('tanh of x, y')\n",
    "print(t)\n",
    "grad = T.backward()\n",
    "print('grad')\n",
    "print(grad)\n",
    "p = T.param()\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOSSMSE (normalement juste, toujours à la fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOSSMSE MODULE\n",
    "#Maybe: convert to list/tensor the tuple\n",
    "    \n",
    "class LossMSE():\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.gradloss = None\n",
    "    \n",
    "    def forward(self, *input, target): #Si plusieurs input et target comment on fait?\n",
    "        liste = []\n",
    "        for x in input:\n",
    "            liste.append((x-target).pow(2)) \n",
    "        output = tuple(liste)\n",
    "        self.loss = output\n",
    "        return output\n",
    "        \n",
    "    def backward(self):\n",
    "        liste = []\n",
    "        for x in self.loss:\n",
    "            liste.append(2*((x).pow(1/2))) #on obtient 2*(sqrt((x-target).pow(2))) du coup 2*(x-target)\n",
    "        output = tuple(liste)      \n",
    "        self.gradloss = output\n",
    "        return output\n",
    "\n",
    "    def param(self):\n",
    "        return [] #No param\n",
    "#forward should get for input, and returns, a tensor or a tuple of tensors.\n",
    "\n",
    "#backward should get as input a tensor or a tuple of tensors containing the gradient of the \n",
    "#loss with respect to the module’s output, accumulate the gradient wrt the parameters, \n",
    "#and return a tensor or a tuple of tensors containing the gradient of the loss wrt the module’s input.\n",
    "\n",
    "#param should return a list of pairs, each composed of a parameter tensor, and a gradient tensor of same size. \n",
    "#This list should be empty for parameterless modules (e.g. ReLU).\n",
    "#Some modules may requires additional methods, and some modules may keep track of information from the forward \n",
    "#pass to be used in the backward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.]])\n",
      "tensor([[2., 2.]])\n",
      "tensor([[-1., -1.]])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required keyword-only argument: 'target'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-180-2b1794a8aae1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mt2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() missing 1 required keyword-only argument: 'target'"
     ]
    }
   ],
   "source": [
    "N = LossMSE()\n",
    "x = torch.empty(1,2).fill_(1)\n",
    "y = torch.empty(1,2).fill_(2)\n",
    "print(x)\n",
    "print(y)\n",
    "t = torch.empty(1,2).fill_(-1)\n",
    "t2 = torch.empty(1,2).fill_(-1)\n",
    "print(t)\n",
    "l = N.forward(x, y ,t,t2)\n",
    "print('loss')\n",
    "print(l)\n",
    "dl = N.backward()\n",
    "print('dloss')\n",
    "print(dl)\n",
    "p = N.param()\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEQUENTIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SEQUENTIAL MODULE (to combine several modules in basic sequential structure)\n",
    "class Sequential(object):\n",
    "    def forward(self , *input):\n",
    "        raise NotImplementedError\n",
    "    def backward(self , *gradwrtoutput):\n",
    "        raise NotImplementedError\n",
    "    def param(self): \n",
    "        return []\n",
    "    \n",
    "#forward should get for input, and returns, a tensor or a tuple of tensors.\n",
    "\n",
    "#backward should get as input a tensor or a tuple of tensors containing the gradient of the \n",
    "#loss with respect to the module’s output, accumulate the gradient wrt the parameters, \n",
    "#and return a tensor or a tuple of tensors containing the gradient of the loss wrt the module’s input.\n",
    "\n",
    "#param should return a list of pairs, each composed of a parameter tensor, and a gradient tensor of same size. \n",
    "#This list should be empty for parameterless modules (e.g. ReLU).\n",
    "#Some modules may requires additional methods, and some modules may keep track of information from the forward \n",
    "#pass to be used in the backward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = torch.empty(1000,2).uniform_(0,1)  #x et y\n",
    "training_classes = torch.empty(1000,1)\n",
    "testing_set = torch.empty(1000,2).uniform_(0,1)  #x et y\n",
    "testing_classes = torch.empty(1000,1)\n",
    "\n",
    "r = torch.empty(1,1).fill_(1/(2*math.pi)).pow(1/2)\n",
    "\n",
    "for i in range (1000):\n",
    "    if (training_set[i].pow(2).sum()).pow(1/2).item() < r.item():\n",
    "        training_classes[i] = 1\n",
    "    else:\n",
    "        training_classes[i] = 0\n",
    "    \n",
    "    if (testing_set[i].pow(2).sum()).pow(1/2).item() < r.item():\n",
    "        testing_classes[i] = 1\n",
    "    else:\n",
    "        testing_classes[i] = 0\n",
    "\n",
    "#builds a network with two input units, two output units, three hidden layers of 25 units,\n",
    "#trains it with MSE, logging the loss,\n",
    "#computes and prints the ﬁnal train and the test errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
