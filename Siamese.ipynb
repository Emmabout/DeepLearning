{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import errno\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets.mnist\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_learn = True\n",
    "save_frequency = 2\n",
    "batch_size = 16\n",
    "lr = 0.001\n",
    "num_epochs = 10\n",
    "weight_decay = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_int(b):\n",
    "    return int(codecs.encode(b, 'hex'), 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_label_file(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        data = f.read()\n",
    "    assert get_int(data[:4]) == 2049\n",
    "    length = get_int(data[4:8])\n",
    "    parsed = np.frombuffer(data, dtype=np.uint8, offset=8)\n",
    "    return torch.from_numpy(parsed).view(length).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image_file(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        data = f.read()\n",
    "    assert get_int(data[:4]) == 2051\n",
    "    length = get_int(data[4:8])\n",
    "    num_rows = get_int(data[8:12])\n",
    "    num_cols = get_int(data[12:16])\n",
    "    images = []\n",
    "    parsed = np.frombuffer(data, dtype=np.uint8, offset=16)\n",
    "    return torch.from_numpy(parsed).view(length, num_rows, num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BalancedMNISTPair(torch.utils.data.Dataset):\n",
    "   \"\"\"Dataset that on each iteration provides two random pairs of\n",
    "   MNIST images. One pair is of the same number (positive sample), one\n",
    "   is of two different numbers (negative sample).\n",
    "   \"\"\"\n",
    "   urls = [\n",
    "      'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
    "      'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
    "      'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
    "      'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz',\n",
    "   ]\n",
    "   raw_folder = 'raw'\n",
    "   processed_folder = 'processed'\n",
    "   training_file = 'training.pt'\n",
    "   test_file = 'test.pt'\n",
    "   \n",
    "   def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
    "      self.root = os.path.expanduser(root)\n",
    "      self.transform = transform\n",
    "      self.target_transform = target_transform\n",
    "      self.train = train # training set or test set\n",
    "      \n",
    "      if download:\n",
    "         self.download()\n",
    "         \n",
    "      if not self._check_exists():\n",
    "         raise RuntimeError('Dataset not found.' + ' You can use download=True to download it')\n",
    "         \n",
    "      if self.train:\n",
    "         self.train_data, self.train_labels = torch.load(\n",
    "            os.path.join(self.root, self.processed_folder, self.training_file))\n",
    "         \n",
    "         train_labels_class = []\n",
    "         train_data_class = []\n",
    "         for i in range(10):\n",
    "            indices = torch.squeeze((self.train_labels == i).nonzero())\n",
    "            train_labels_class.append(torch.index_select(self.train_labels, 0, indices))\n",
    "            train_data_class.append(torch.index_select(self.train_data, 0, indices))\n",
    "            \n",
    "         # generate balanced pairs\n",
    "         self.train_data = []\n",
    "         self.train_labels = []\n",
    "         lengths = [x.shape[0] for x in train_labels_class]\n",
    "         for i in range(10):\n",
    "            for j in range(500): # create 500 pairs\n",
    "               rnd_cls = random.randint(0,8) # choose random class that is not the same class\n",
    "               if rnd_cls >= i:\n",
    "                  rnd_cls = rnd_cls + 1\n",
    "\n",
    "               rnd_dist = random.randint(0, 100)\n",
    "                  \n",
    "               self.train_data.append(torch.stack([train_data_class[i][j], train_data_class[i][j+rnd_dist], train_data_class[rnd_cls][j]]))\n",
    "               self.train_labels.append([1,0])\n",
    "\n",
    "         self.train_data = torch.stack(self.train_data)\n",
    "         self.train_labels = torch.tensor(self.train_labels)\n",
    "               \n",
    "      else:\n",
    "         self.test_data, self.test_labels = torch.load(\n",
    "            os.path.join(self.root, self.processed_folder, self.test_file))\n",
    "         \n",
    "         test_labels_class = []\n",
    "         test_data_class = []\n",
    "         for i in range(10):\n",
    "            indices = torch.squeeze((self.test_labels == i).nonzero())\n",
    "            test_labels_class.append(torch.index_select(self.test_labels, 0, indices))\n",
    "            test_data_class.append(torch.index_select(self.test_data, 0, indices))\n",
    "            \n",
    "         # generate balanced pairs\n",
    "         self.test_data = []\n",
    "         self.test_labels = []\n",
    "         lengths = [x.shape[0] for x in test_labels_class]\n",
    "         for i in range(10):\n",
    "            for j in range(500): # create 500 pairs\n",
    "               rnd_cls = random.randint(0,8) # choose random class that is not the same class\n",
    "               if rnd_cls >= i:\n",
    "                  rnd_cls = rnd_cls + 1\n",
    "\n",
    "               rnd_dist = random.randint(0, 100)\n",
    "                  \n",
    "               self.test_data.append(torch.stack([test_data_class[i][j], test_data_class[i][j+rnd_dist], test_data_class[rnd_cls][j]]))\n",
    "               self.test_labels.append([1,0])\n",
    "\n",
    "         self.test_data = torch.stack(self.test_data)\n",
    "         self.test_labels = torch.tensor(self.test_labels)\n",
    "         \n",
    "   def __getitem__(self, index):\n",
    "      if self.train:\n",
    "         imgs, target = self.train_data[index], self.train_labels[index]\n",
    "      else:\n",
    "         imgs, target = self.test_data[index], self.test_labels[index]\n",
    "         \n",
    "      img_ar = []\n",
    "      for i in range(len(imgs)):\n",
    "         img = Image.fromarray(imgs[i].numpy(), mode='L')\n",
    "         if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "         img_ar.append(img)\n",
    "         \n",
    "      if self.target_transform is not None:\n",
    "         target = self.target_transform(target)\n",
    "         \n",
    "      return img_ar, target\n",
    "   \n",
    "   def __len__(self):\n",
    "      if self.train:\n",
    "         return len(self.train_data)\n",
    "      else:\n",
    "         return len(self.test_data)\n",
    "      \n",
    "   def _check_exists(self):\n",
    "      return os.path.exists(os.path.join(self.root, self.processed_folder, self.training_file)) and \\\n",
    "         os.path.exists(os.path.join(self.root, self.processed_folder, self.test_file))\n",
    "   \n",
    "   def download(self):\n",
    "      \"\"\"Download the MNIST data if it doesn't exist in processed_folder already.\"\"\"\n",
    "      from six.moves import urllib\n",
    "      import gzip\n",
    "\n",
    "      if self._check_exists():\n",
    "         return\n",
    "\n",
    "      # download files\n",
    "      try:\n",
    "         os.makedirs(os.path.join(self.root, self.raw_folder))\n",
    "         os.makedirs(os.path.join(self.root, self.processed_folder))\n",
    "      except OSError as e:\n",
    "         if e.errno == errno.EEXIST:\n",
    "            pass\n",
    "         else:\n",
    "            raise\n",
    "\n",
    "      for url in self.urls:\n",
    "         print('Downloading ' + url)\n",
    "         data = urllib.request.urlopen(url)\n",
    "         filename = url.rpartition('/')[2]\n",
    "         file_path = os.path.join(self.root, self.raw_folder, filename)\n",
    "         with open(file_path, 'wb') as f:\n",
    "            f.write(data.read())\n",
    "         with open(file_path.replace('.gz', ''), 'wb') as out_f, \\\n",
    "               gzip.GzipFile(file_path) as zip_f:\n",
    "            out_f.write(zip_f.read())\n",
    "         os.unlink(file_path)\n",
    "\n",
    "      # process and save as torch files\n",
    "      print('Processing...')\n",
    "\n",
    "      training_set = (\n",
    "         read_image_file(os.path.join(self.root, self.raw_folder, 'train-images-idx3-ubyte')),\n",
    "         read_label_file(os.path.join(self.root, self.raw_folder, 'train-labels-idx1-ubyte'))\n",
    "      )\n",
    "      test_set = (\n",
    "         read_image_file(os.path.join(self.root, self.raw_folder, 't10k-images-idx3-ubyte')),\n",
    "         read_label_file(os.path.join(self.root, self.raw_folder, 't10k-labels-idx1-ubyte'))\n",
    "      )\n",
    "      with open(os.path.join(self.root, self.processed_folder, self.training_file), 'wb') as f:\n",
    "         torch.save(training_set, f)\n",
    "      with open(os.path.join(self.root, self.processed_folder, self.test_file), 'wb') as f:\n",
    "         torch.save(test_set, f)\n",
    "\n",
    "      print('Done!')\n",
    "\n",
    "   def __repr__(self):\n",
    "      fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
    "      fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
    "      tmp = 'train' if self.train is True else 'test'\n",
    "      fmt_str += '    Split: {}\\n'.format(tmp)\n",
    "      fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
    "      tmp = '    Transforms (if any): '\n",
    "      fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
    "      tmp = '    Target Transforms (if any): '\n",
    "      fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
    "      return fmt_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "      super().__init__()\n",
    "      \n",
    "      self.conv1 = nn.Conv2d(1, 64, 7)\n",
    "      self.pool1 = nn.MaxPool2d(2)\n",
    "      self.conv2 = nn.Conv2d(64, 128, 5)\n",
    "      self.conv3 = nn.Conv2d(128, 256, 5)\n",
    "      self.linear1 = nn.Linear(2304, 512)\n",
    "      \n",
    "      self.linear2 = nn.Linear(512, 2)\n",
    "      \n",
    "    def forward(self, data):\n",
    "      res = []\n",
    "      for i in range(2): # Siamese nets; sharing weights\n",
    "         x = data[i]\n",
    "         x = self.conv1(x)\n",
    "         x = F.relu(x)\n",
    "         x = self.pool1(x)\n",
    "         x = self.conv2(x)\n",
    "         x = F.relu(x)\n",
    "         x = self.conv3(x)\n",
    "         x = F.relu(x)\n",
    "         \n",
    "         x = x.view(x.shape[0], -1)\n",
    "         x = self.linear1(x)\n",
    "         res.append(F.relu(x))\n",
    "         \n",
    "      res = torch.abs(res[1] - res[0])\n",
    "      res = self.linear2(res)\n",
    "      return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, epoch, optimizer):\n",
    "   model.train()\n",
    "   \n",
    "   for batch_idx, (data, target) in enumerate(train_loader):\n",
    "      for i in range(len(data)):\n",
    "         data[i] = data[i].to(device)\n",
    "         \n",
    "      optimizer.zero_grad()\n",
    "      output_positive = model(data[:2])\n",
    "      output_negative = model(data[0:3:2])\n",
    "      \n",
    "      target = target.type(torch.LongTensor).to(device)\n",
    "      target_positive = torch.squeeze(target[:,0])\n",
    "      target_negative = torch.squeeze(target[:,1])\n",
    "      \n",
    "      loss_positive = F.cross_entropy(output_positive, target_positive)\n",
    "      loss_negative = F.cross_entropy(output_negative, target_negative)\n",
    "      \n",
    "      loss = loss_positive + loss_negative\n",
    "      loss.backward()\n",
    "      \n",
    "      optimizer.step()\n",
    "      if batch_idx % 10 == 0:\n",
    "         print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx*batch_size, len(train_loader.dataset), 100. * batch_idx*batch_size / len(train_loader.dataset),\n",
    "            loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, test_loader):\n",
    "   model.eval()\n",
    "   \n",
    "   with torch.no_grad():\n",
    "      accurate_labels = 0\n",
    "      all_labels = 0\n",
    "      loss = 0\n",
    "      for batch_idx, (data, target) in enumerate(test_loader):\n",
    "         for i in range(len(data)):\n",
    "            data[i] = data[i].to(device)\n",
    "            \n",
    "         output_positive = model(data[:2])\n",
    "         output_negative = model(data[0:3:2])\n",
    "            \n",
    "         target = target.type(torch.LongTensor).to(device)\n",
    "         target_positive = torch.squeeze(target[:,0])\n",
    "         target_negative = torch.squeeze(target[:,1])\n",
    "            \n",
    "         loss_positive = F.cross_entropy(output_positive, target_positive)\n",
    "         loss_negative = F.cross_entropy(output_negative, target_negative)\n",
    "            \n",
    "         loss = loss + loss_positive + loss_negative\n",
    "            \n",
    "         accurate_labels_positive = torch.sum(torch.argmax(output_positive, dim=1) == target_positive).cpu()\n",
    "         accurate_labels_negative = torch.sum(torch.argmax(output_negative, dim=1) == target_negative).cpu()\n",
    "            \n",
    "         accurate_labels = accurate_labels + accurate_labels_positive + accurate_labels_negative\n",
    "         all_labels = all_labels + len(target_positive) + len(target_negative)\n",
    "      \n",
    "      accuracy = 100. * accurate_labels / all_labels\n",
    "      print('Test accuracy: {}/{} ({:.3f}%)\\tLoss: {:.6f}'.format(accurate_labels, all_labels, accuracy, loss))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneshot(model, device, data):\n",
    "   model.eval()\n",
    "\n",
    "   with torch.no_grad():\n",
    "      for i in range(len(data)):\n",
    "            data[i] = data[i].to(device)\n",
    "      \n",
    "      output = model(data)\n",
    "      return torch.squeeze(torch.argmax(output, dim=1)).cpu().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n",
      "Train Epoch: 0 [0/5000 (0%)]\tLoss: 1.386217\n",
      "Train Epoch: 0 [160/5000 (3%)]\tLoss: 1.370871\n",
      "Train Epoch: 0 [320/5000 (6%)]\tLoss: 1.245360\n",
      "Train Epoch: 0 [480/5000 (10%)]\tLoss: 1.516690\n",
      "Train Epoch: 0 [640/5000 (13%)]\tLoss: 1.202069\n",
      "Train Epoch: 0 [800/5000 (16%)]\tLoss: 1.112195\n",
      "Train Epoch: 0 [960/5000 (19%)]\tLoss: 0.948196\n",
      "Train Epoch: 0 [1120/5000 (22%)]\tLoss: 1.005022\n",
      "Train Epoch: 0 [1280/5000 (26%)]\tLoss: 1.027648\n",
      "Train Epoch: 0 [1440/5000 (29%)]\tLoss: 0.943189\n",
      "Train Epoch: 0 [1600/5000 (32%)]\tLoss: 0.877081\n",
      "Train Epoch: 0 [1760/5000 (35%)]\tLoss: 0.787850\n",
      "Train Epoch: 0 [1920/5000 (38%)]\tLoss: 0.747192\n",
      "Train Epoch: 0 [2080/5000 (42%)]\tLoss: 0.786230\n",
      "Train Epoch: 0 [2240/5000 (45%)]\tLoss: 0.855382\n",
      "Train Epoch: 0 [2400/5000 (48%)]\tLoss: 0.803591\n",
      "Train Epoch: 0 [2560/5000 (51%)]\tLoss: 1.053822\n",
      "Train Epoch: 0 [2720/5000 (54%)]\tLoss: 0.722630\n",
      "Train Epoch: 0 [2880/5000 (58%)]\tLoss: 0.836073\n",
      "Train Epoch: 0 [3040/5000 (61%)]\tLoss: 1.104322\n",
      "Train Epoch: 0 [3200/5000 (64%)]\tLoss: 0.805573\n",
      "Train Epoch: 0 [3360/5000 (67%)]\tLoss: 1.062588\n",
      "Train Epoch: 0 [3520/5000 (70%)]\tLoss: 0.776128\n",
      "Train Epoch: 0 [3680/5000 (74%)]\tLoss: 0.927225\n",
      "Train Epoch: 0 [3840/5000 (77%)]\tLoss: 0.591770\n",
      "Train Epoch: 0 [4000/5000 (80%)]\tLoss: 0.526581\n",
      "Train Epoch: 0 [4160/5000 (83%)]\tLoss: 0.554722\n",
      "Train Epoch: 0 [4320/5000 (86%)]\tLoss: 0.685162\n",
      "Train Epoch: 0 [4480/5000 (90%)]\tLoss: 0.564883\n",
      "Train Epoch: 0 [4640/5000 (93%)]\tLoss: 0.568516\n",
      "Train Epoch: 0 [4800/5000 (96%)]\tLoss: 0.665117\n",
      "Train Epoch: 0 [4960/5000 (99%)]\tLoss: 0.469166\n",
      "Test accuracy: 8441/10000 (84.410%)\tLoss: 231.483810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emmab\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:360: UserWarning: Couldn't retrieve source code for container of type Net. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/5000 (0%)]\tLoss: 0.549878\n",
      "Train Epoch: 1 [160/5000 (3%)]\tLoss: 0.663159\n",
      "Train Epoch: 1 [320/5000 (6%)]\tLoss: 0.321047\n",
      "Train Epoch: 1 [480/5000 (10%)]\tLoss: 0.204485\n",
      "Train Epoch: 1 [640/5000 (13%)]\tLoss: 0.749056\n",
      "Train Epoch: 1 [800/5000 (16%)]\tLoss: 0.691567\n",
      "Train Epoch: 1 [960/5000 (19%)]\tLoss: 0.733901\n",
      "Train Epoch: 1 [1120/5000 (22%)]\tLoss: 0.255967\n",
      "Train Epoch: 1 [1280/5000 (26%)]\tLoss: 0.268745\n",
      "Train Epoch: 1 [1440/5000 (29%)]\tLoss: 0.501447\n",
      "Train Epoch: 1 [1600/5000 (32%)]\tLoss: 0.552750\n",
      "Train Epoch: 1 [1760/5000 (35%)]\tLoss: 0.187757\n",
      "Train Epoch: 1 [1920/5000 (38%)]\tLoss: 0.565385\n",
      "Train Epoch: 1 [2080/5000 (42%)]\tLoss: 0.365695\n",
      "Train Epoch: 1 [2240/5000 (45%)]\tLoss: 0.427638\n",
      "Train Epoch: 1 [2400/5000 (48%)]\tLoss: 0.696115\n",
      "Train Epoch: 1 [2560/5000 (51%)]\tLoss: 0.223233\n",
      "Train Epoch: 1 [2720/5000 (54%)]\tLoss: 0.296878\n",
      "Train Epoch: 1 [2880/5000 (58%)]\tLoss: 0.674573\n",
      "Train Epoch: 1 [3040/5000 (61%)]\tLoss: 0.568990\n",
      "Train Epoch: 1 [3200/5000 (64%)]\tLoss: 0.260316\n",
      "Train Epoch: 1 [3360/5000 (67%)]\tLoss: 0.627205\n",
      "Train Epoch: 1 [3520/5000 (70%)]\tLoss: 0.242943\n",
      "Train Epoch: 1 [3680/5000 (74%)]\tLoss: 0.199657\n",
      "Train Epoch: 1 [3840/5000 (77%)]\tLoss: 0.300568\n",
      "Train Epoch: 1 [4000/5000 (80%)]\tLoss: 0.644451\n",
      "Train Epoch: 1 [4160/5000 (83%)]\tLoss: 0.257123\n",
      "Train Epoch: 1 [4320/5000 (86%)]\tLoss: 0.086709\n",
      "Train Epoch: 1 [4480/5000 (90%)]\tLoss: 0.174172\n",
      "Train Epoch: 1 [4640/5000 (93%)]\tLoss: 0.290996\n",
      "Train Epoch: 1 [4800/5000 (96%)]\tLoss: 0.692652\n",
      "Train Epoch: 1 [4960/5000 (99%)]\tLoss: 0.494965\n",
      "Test accuracy: 9166/10000 (91.660%)\tLoss: 140.421051\n",
      "Train Epoch: 2 [0/5000 (0%)]\tLoss: 0.326429\n",
      "Train Epoch: 2 [160/5000 (3%)]\tLoss: 0.164478\n",
      "Train Epoch: 2 [320/5000 (6%)]\tLoss: 0.378603\n",
      "Train Epoch: 2 [480/5000 (10%)]\tLoss: 0.194848\n",
      "Train Epoch: 2 [640/5000 (13%)]\tLoss: 0.287364\n",
      "Train Epoch: 2 [800/5000 (16%)]\tLoss: 0.267965\n",
      "Train Epoch: 2 [960/5000 (19%)]\tLoss: 0.179687\n",
      "Train Epoch: 2 [1120/5000 (22%)]\tLoss: 0.230727\n",
      "Train Epoch: 2 [1280/5000 (26%)]\tLoss: 0.627321\n",
      "Train Epoch: 2 [1440/5000 (29%)]\tLoss: 0.713507\n",
      "Train Epoch: 2 [1600/5000 (32%)]\tLoss: 0.090918\n",
      "Train Epoch: 2 [1760/5000 (35%)]\tLoss: 0.070197\n",
      "Train Epoch: 2 [1920/5000 (38%)]\tLoss: 0.171501\n",
      "Train Epoch: 2 [2080/5000 (42%)]\tLoss: 0.160926\n",
      "Train Epoch: 2 [2240/5000 (45%)]\tLoss: 0.881193\n",
      "Train Epoch: 2 [2400/5000 (48%)]\tLoss: 0.348729\n",
      "Train Epoch: 2 [2560/5000 (51%)]\tLoss: 0.168298\n",
      "Train Epoch: 2 [2720/5000 (54%)]\tLoss: 0.258451\n",
      "Train Epoch: 2 [2880/5000 (58%)]\tLoss: 0.056805\n",
      "Train Epoch: 2 [3040/5000 (61%)]\tLoss: 0.474851\n",
      "Train Epoch: 2 [3200/5000 (64%)]\tLoss: 0.221751\n",
      "Train Epoch: 2 [3360/5000 (67%)]\tLoss: 0.311634\n",
      "Train Epoch: 2 [3520/5000 (70%)]\tLoss: 0.158663\n",
      "Train Epoch: 2 [3680/5000 (74%)]\tLoss: 0.054678\n",
      "Train Epoch: 2 [3840/5000 (77%)]\tLoss: 0.151972\n",
      "Train Epoch: 2 [4000/5000 (80%)]\tLoss: 0.284250\n",
      "Train Epoch: 2 [4160/5000 (83%)]\tLoss: 0.150839\n",
      "Train Epoch: 2 [4320/5000 (86%)]\tLoss: 0.219673\n",
      "Train Epoch: 2 [4480/5000 (90%)]\tLoss: 0.354921\n",
      "Train Epoch: 2 [4640/5000 (93%)]\tLoss: 0.182503\n",
      "Train Epoch: 2 [4800/5000 (96%)]\tLoss: 0.274426\n",
      "Train Epoch: 2 [4960/5000 (99%)]\tLoss: 0.223947\n",
      "Test accuracy: 9177/10000 (91.770%)\tLoss: 146.703049\n",
      "Train Epoch: 3 [0/5000 (0%)]\tLoss: 0.078900\n",
      "Train Epoch: 3 [160/5000 (3%)]\tLoss: 0.403672\n",
      "Train Epoch: 3 [320/5000 (6%)]\tLoss: 0.201161\n",
      "Train Epoch: 3 [480/5000 (10%)]\tLoss: 0.081043\n",
      "Train Epoch: 3 [640/5000 (13%)]\tLoss: 0.073890\n",
      "Train Epoch: 3 [800/5000 (16%)]\tLoss: 0.177696\n",
      "Train Epoch: 3 [960/5000 (19%)]\tLoss: 0.070509\n",
      "Train Epoch: 3 [1120/5000 (22%)]\tLoss: 0.185739\n",
      "Train Epoch: 3 [1280/5000 (26%)]\tLoss: 0.030507\n",
      "Train Epoch: 3 [1440/5000 (29%)]\tLoss: 0.027548\n",
      "Train Epoch: 3 [1600/5000 (32%)]\tLoss: 0.187250\n",
      "Train Epoch: 3 [1760/5000 (35%)]\tLoss: 0.034133\n",
      "Train Epoch: 3 [1920/5000 (38%)]\tLoss: 0.123009\n",
      "Train Epoch: 3 [2080/5000 (42%)]\tLoss: 0.093674\n",
      "Train Epoch: 3 [2240/5000 (45%)]\tLoss: 0.179433\n",
      "Train Epoch: 3 [2400/5000 (48%)]\tLoss: 0.214470\n",
      "Train Epoch: 3 [2560/5000 (51%)]\tLoss: 0.032190\n",
      "Train Epoch: 3 [2720/5000 (54%)]\tLoss: 0.046877\n",
      "Train Epoch: 3 [2880/5000 (58%)]\tLoss: 0.144238\n",
      "Train Epoch: 3 [3040/5000 (61%)]\tLoss: 0.161568\n",
      "Train Epoch: 3 [3200/5000 (64%)]\tLoss: 0.225783\n",
      "Train Epoch: 3 [3360/5000 (67%)]\tLoss: 0.109790\n",
      "Train Epoch: 3 [3520/5000 (70%)]\tLoss: 0.173006\n",
      "Train Epoch: 3 [3680/5000 (74%)]\tLoss: 0.260534\n",
      "Train Epoch: 3 [3840/5000 (77%)]\tLoss: 0.020449\n",
      "Train Epoch: 3 [4000/5000 (80%)]\tLoss: 0.029630\n",
      "Train Epoch: 3 [4160/5000 (83%)]\tLoss: 0.058427\n",
      "Train Epoch: 3 [4320/5000 (86%)]\tLoss: 0.610261\n",
      "Train Epoch: 3 [4480/5000 (90%)]\tLoss: 0.234757\n",
      "Train Epoch: 3 [4640/5000 (93%)]\tLoss: 0.039719\n",
      "Train Epoch: 3 [4800/5000 (96%)]\tLoss: 0.232986\n",
      "Train Epoch: 3 [4960/5000 (99%)]\tLoss: 0.281725\n",
      "Test accuracy: 9412/10000 (94.120%)\tLoss: 107.377838\n",
      "Train Epoch: 4 [0/5000 (0%)]\tLoss: 0.122577\n",
      "Train Epoch: 4 [160/5000 (3%)]\tLoss: 0.196198\n",
      "Train Epoch: 4 [320/5000 (6%)]\tLoss: 0.077888\n",
      "Train Epoch: 4 [480/5000 (10%)]\tLoss: 0.072433\n",
      "Train Epoch: 4 [640/5000 (13%)]\tLoss: 0.005877\n",
      "Train Epoch: 4 [800/5000 (16%)]\tLoss: 0.055950\n",
      "Train Epoch: 4 [960/5000 (19%)]\tLoss: 0.172769\n",
      "Train Epoch: 4 [1120/5000 (22%)]\tLoss: 0.017338\n",
      "Train Epoch: 4 [1280/5000 (26%)]\tLoss: 0.340638\n",
      "Train Epoch: 4 [1440/5000 (29%)]\tLoss: 0.041112\n",
      "Train Epoch: 4 [1600/5000 (32%)]\tLoss: 0.031121\n",
      "Train Epoch: 4 [1760/5000 (35%)]\tLoss: 0.026765\n",
      "Train Epoch: 4 [1920/5000 (38%)]\tLoss: 0.195575\n",
      "Train Epoch: 4 [2080/5000 (42%)]\tLoss: 0.034542\n",
      "Train Epoch: 4 [2240/5000 (45%)]\tLoss: 0.155731\n",
      "Train Epoch: 4 [2400/5000 (48%)]\tLoss: 0.051140\n",
      "Train Epoch: 4 [2560/5000 (51%)]\tLoss: 0.058541\n",
      "Train Epoch: 4 [2720/5000 (54%)]\tLoss: 0.011104\n",
      "Train Epoch: 4 [2880/5000 (58%)]\tLoss: 0.014915\n",
      "Train Epoch: 4 [3040/5000 (61%)]\tLoss: 0.004441\n",
      "Train Epoch: 4 [3200/5000 (64%)]\tLoss: 0.090916\n",
      "Train Epoch: 4 [3360/5000 (67%)]\tLoss: 0.015569\n",
      "Train Epoch: 4 [3520/5000 (70%)]\tLoss: 0.224360\n",
      "Train Epoch: 4 [3680/5000 (74%)]\tLoss: 0.037819\n",
      "Train Epoch: 4 [3840/5000 (77%)]\tLoss: 0.065627\n",
      "Train Epoch: 4 [4000/5000 (80%)]\tLoss: 0.167698\n",
      "Train Epoch: 4 [4160/5000 (83%)]\tLoss: 0.045682\n",
      "Train Epoch: 4 [4320/5000 (86%)]\tLoss: 0.011428\n",
      "Train Epoch: 4 [4480/5000 (90%)]\tLoss: 0.286056\n",
      "Train Epoch: 4 [4640/5000 (93%)]\tLoss: 0.013906\n",
      "Train Epoch: 4 [4800/5000 (96%)]\tLoss: 0.207826\n",
      "Train Epoch: 4 [4960/5000 (99%)]\tLoss: 0.005617\n",
      "Test accuracy: 9352/10000 (93.520%)\tLoss: 139.492203\n",
      "Train Epoch: 5 [0/5000 (0%)]\tLoss: 0.054381\n",
      "Train Epoch: 5 [160/5000 (3%)]\tLoss: 0.080686\n",
      "Train Epoch: 5 [320/5000 (6%)]\tLoss: 0.013482\n",
      "Train Epoch: 5 [480/5000 (10%)]\tLoss: 0.009017\n",
      "Train Epoch: 5 [640/5000 (13%)]\tLoss: 0.028037\n",
      "Train Epoch: 5 [800/5000 (16%)]\tLoss: 0.079942\n",
      "Train Epoch: 5 [960/5000 (19%)]\tLoss: 0.011174\n",
      "Train Epoch: 5 [1120/5000 (22%)]\tLoss: 0.102425\n",
      "Train Epoch: 5 [1280/5000 (26%)]\tLoss: 0.251308\n",
      "Train Epoch: 5 [1440/5000 (29%)]\tLoss: 0.149876\n",
      "Train Epoch: 5 [1600/5000 (32%)]\tLoss: 0.005548\n",
      "Train Epoch: 5 [1760/5000 (35%)]\tLoss: 0.021326\n",
      "Train Epoch: 5 [1920/5000 (38%)]\tLoss: 0.015632\n",
      "Train Epoch: 5 [2080/5000 (42%)]\tLoss: 0.029102\n",
      "Train Epoch: 5 [2240/5000 (45%)]\tLoss: 0.001289\n",
      "Train Epoch: 5 [2400/5000 (48%)]\tLoss: 0.051042\n",
      "Train Epoch: 5 [2560/5000 (51%)]\tLoss: 0.023802\n",
      "Train Epoch: 5 [2720/5000 (54%)]\tLoss: 0.048663\n",
      "Train Epoch: 5 [2880/5000 (58%)]\tLoss: 0.056848\n",
      "Train Epoch: 5 [3040/5000 (61%)]\tLoss: 0.052474\n",
      "Train Epoch: 5 [3200/5000 (64%)]\tLoss: 0.007387\n",
      "Train Epoch: 5 [3360/5000 (67%)]\tLoss: 0.006890\n",
      "Train Epoch: 5 [3520/5000 (70%)]\tLoss: 0.019655\n",
      "Train Epoch: 5 [3680/5000 (74%)]\tLoss: 0.023923\n",
      "Train Epoch: 5 [3840/5000 (77%)]\tLoss: 0.003473\n",
      "Train Epoch: 5 [4000/5000 (80%)]\tLoss: 0.030508\n",
      "Train Epoch: 5 [4160/5000 (83%)]\tLoss: 0.006858\n",
      "Train Epoch: 5 [4320/5000 (86%)]\tLoss: 0.005735\n",
      "Train Epoch: 5 [4480/5000 (90%)]\tLoss: 0.152890\n",
      "Train Epoch: 5 [4640/5000 (93%)]\tLoss: 0.038238\n",
      "Train Epoch: 5 [4800/5000 (96%)]\tLoss: 0.065757\n",
      "Train Epoch: 5 [4960/5000 (99%)]\tLoss: 0.026902\n",
      "Test accuracy: 9494/10000 (94.940%)\tLoss: 96.647659\n",
      "Train Epoch: 6 [0/5000 (0%)]\tLoss: 0.332646\n",
      "Train Epoch: 6 [160/5000 (3%)]\tLoss: 0.140342\n",
      "Train Epoch: 6 [320/5000 (6%)]\tLoss: 0.013385\n",
      "Train Epoch: 6 [480/5000 (10%)]\tLoss: 0.169412\n",
      "Train Epoch: 6 [640/5000 (13%)]\tLoss: 0.005211\n",
      "Train Epoch: 6 [800/5000 (16%)]\tLoss: 0.007215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [960/5000 (19%)]\tLoss: 0.013675\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-f226c1a9a82f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m    \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-f226c1a9a82f>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m       \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m          \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m          \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m          \u001b[1;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m&\u001b[0m \u001b[0msave_frequency\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-3e132a5a214b>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, device, train_loader, epoch, optimizer)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m       \u001b[0moutput_positive\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m       \u001b[0moutput_negative\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-07ecf89662fb>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     20\u001b[0m          \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m          \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m          \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m          \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[1;34m(self, input, weight)\u001b[0m\n\u001b[0;32m    340\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    341\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[1;32m--> 342\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def main():\n",
    "   device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "   trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "   \n",
    "   model = Net().to(device)\n",
    "   \n",
    "   if do_learn: # training mode\n",
    "      train_loader = torch.utils.data.DataLoader(BalancedMNISTPair('../data', train=True, download=True, transform=trans), batch_size=batch_size, shuffle=True)\n",
    "      test_loader = torch.utils.data.DataLoader(BalancedMNISTPair('../data', train=False, download=True, transform=trans), batch_size=batch_size, shuffle=False)\n",
    "      \n",
    "      optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "      for epoch in range(num_epochs):\n",
    "         train(model, device, train_loader, epoch, optimizer)\n",
    "         test(model, device, test_loader)\n",
    "         if epoch & save_frequency == 0:\n",
    "            torch.save(model, 'siamese_{:03}.pt'.format(epoch))\n",
    "   else: # prediction\n",
    "      prediction_loader = torch.utils.data.DataLoader(BalancedMNISTPair('../data', train=False, download=True, transform=trans), batch_size=1, shuffle=True)\n",
    "      model.load_state_dict(torch.load(load_model_path))\n",
    "      data = []\n",
    "      data.extend(next(iter(prediction_loader))[0][:3:2])\n",
    "      same = oneshot(model, device, data)\n",
    "      if same > 0:\n",
    "         print('These two images are of the same number')\n",
    "      else:\n",
    "         print('These two images are not of the same number')\n",
    "         \n",
    "if __name__ == '__main__':\n",
    "   main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
