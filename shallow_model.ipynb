{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "\n",
    "import dlc_practical_prologue as prologue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000 #nb of pairs\n",
    "\n",
    "#generate pairs\n",
    "train_input, train_target, train_classes, test_input, test_target, test_classes = prologue.generate_pair_sets(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize the input\n",
    "train_input/=255\n",
    "test_input/=255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_target = torch.empty(N,2)\n",
    "new_test_target = torch.empty(N,2)\n",
    "for i in range(N):\n",
    "    if train_target[i] == 1 :\n",
    "        new_train_target[i,0] = 0\n",
    "        new_train_target[i,1] = 1\n",
    "        \n",
    "    else:\n",
    "        new_train_target[i,0] = 1\n",
    "        new_train_target[i,1] = 0\n",
    "        \n",
    "    if test_target[i] == 1:\n",
    "        new_test_target[i,0] = 0\n",
    "        new_test_target[i,1] = 1\n",
    "        \n",
    "    else:\n",
    "        new_test_target[i,0] = 1\n",
    "        new_test_target[i,1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_shallow_model():\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(392, 400),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(400, 500),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(500, 600),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(600, 2),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, nb_hidden):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(392, 784, kernel_size=4)\n",
    "        self.conv2 = nn.Conv1d(784, 1568, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(256, nb_hidden)\n",
    "        self.fc2 = nn.Linear(nb_hidden, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=3))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2))\n",
    "        x = F.relu(self.fc1(x.view(-1, 256)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_input, train_target,lr):\n",
    "    epoch = 25\n",
    "    eta = 0.2\n",
    "    mini_batches = 100\n",
    "    optimizer = optim.Adam(model.parameters(), lr)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    for e in range(epoch):\n",
    "        sum_loss = 0\n",
    "        \n",
    "        for b in range(0, train_input.size(0), mini_batches):\n",
    "            \n",
    "            output = model(train_input.narrow(0, b, mini_batches).reshape(mini_batches, 1, -1))\n",
    "            #print('shapes',output.squeeze(1).shape, train_target.narrow(0, b, mini_batches).shape)\n",
    "            loss = criterion(output.squeeze(1), train_target.narrow(0, b, mini_batches))\n",
    "            loss.requires_grad_()\n",
    "            #print(\"output\", output.squeeze(1), \"train\", train_target.narrow(0, b, mini_batches))\n",
    "            model.zero_grad()\n",
    "            #print(\"output\", output, \"shape\", output.shape)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_loss = sum_loss + loss.item()\n",
    "            \n",
    "            \"\"\"#print(\"LOSS\",loss.item())\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    p -= eta*p.grad\n",
    "                    #print(\"grads\", p.grad)\"\"\"\n",
    "        print(e, sum_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors(model, test_input, target):\n",
    "    nb_errors = 0\n",
    "    mini_batch_size = 100\n",
    "    \n",
    "    for b in range(0, test_input.size(0), mini_batch_size):\n",
    "        output = model(test_input.narrow(0, b, mini_batch_size).reshape(mini_batch_size, 1, -1))\n",
    "        _, predicted_class = output.max(2)\n",
    "        #print(output)\n",
    "        #print(predicted_class, output, target)\n",
    "        #print(\"pred classes\",predicted_class.shape, \"output\", output.shape, \"target\", target.shape)\n",
    "        for k in range(mini_batch_size):\n",
    "            \n",
    "            if target[b + k, predicted_class[k]] <= 0:\n",
    "                nb_errors = nb_errors + 1\n",
    "\n",
    "    return nb_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 6.451932311058044\n",
      "1 4.967446804046631\n",
      "2 4.133762747049332\n",
      "3 3.3266296982765198\n",
      "4 2.4942222237586975\n",
      "5 1.8490521013736725\n",
      "6 1.6737718358635902\n",
      "7 0.9952446706593037\n",
      "8 0.6227876208722591\n",
      "9 0.6888872403651476\n",
      "10 1.0454186145216227\n",
      "11 1.2577249705791473\n",
      "12 3.8523196950554848\n",
      "13 3.1034164652228355\n",
      "14 1.6116936951875687\n",
      "15 0.7796970121562481\n",
      "16 0.17608672473579645\n",
      "17 0.053130764747038484\n",
      "18 0.01135848433477804\n",
      "19 0.006402816972695291\n",
      "20 0.0036853097553830594\n",
      "21 0.0023685156629653648\n",
      "22 0.0019043087158934213\n",
      "23 0.0014951023113098927\n",
      "24 0.001208610054163728\n",
      "Learning rate : 0.001\n",
      "train error 0.00%% 0/1000\n",
      "test error 15.60%% 156/1000\n",
      "0 6.234408348798752\n",
      "1 5.805599391460419\n",
      "2 4.47316586971283\n",
      "3 3.4486809372901917\n",
      "4 2.969285398721695\n",
      "5 2.0315786823630333\n",
      "6 1.9706301167607307\n",
      "7 1.361767716705799\n",
      "8 0.6641023680567741\n",
      "9 0.3192249396815896\n",
      "10 0.40221479209139943\n",
      "11 0.6118102530017495\n",
      "12 1.483733182772994\n",
      "13 0.7836460210382938\n",
      "14 0.8937353361397982\n",
      "15 1.125158446840942\n",
      "16 1.9786163493990898\n",
      "17 2.972267374396324\n",
      "18 1.6788113042712212\n",
      "19 0.636424454394728\n",
      "20 0.11182112153619528\n",
      "21 0.04999576564296149\n",
      "22 0.24092875463247765\n",
      "23 0.4412656140048057\n",
      "24 0.22578585520386696\n",
      "Learning rate : 0.005\n",
      "train error 1.90%% 19/1000\n",
      "test error 20.00%% 200/1000\n",
      "0 7.574694216251373\n",
      "1 5.448244512081146\n",
      "2 4.435785293579102\n",
      "3 3.8284014761447906\n",
      "4 3.3290463984012604\n",
      "5 2.708546370267868\n",
      "6 2.132596403360367\n",
      "7 1.5512807741761208\n",
      "8 2.2350098490715027\n",
      "9 2.694638431072235\n",
      "10 1.532058134675026\n",
      "11 1.9065434485673904\n",
      "12 1.3340438157320023\n",
      "13 1.1577842347323895\n",
      "14 0.844405610114336\n",
      "15 0.9081751331686974\n",
      "16 1.1382409781217575\n",
      "17 1.3597053289413452\n",
      "18 1.7877943068742752\n",
      "19 2.226374566555023\n",
      "20 1.9005217179656029\n",
      "21 1.92556531727314\n",
      "22 1.662167266011238\n",
      "23 1.0772187411785126\n",
      "24 0.27502517285756767\n",
      "Learning rate : 0.01\n",
      "train error 0.90%% 9/1000\n",
      "test error 18.90%% 189/1000\n",
      "0 671.2893059849739\n",
      "1 8.332092642784119\n",
      "2 7.643026888370514\n",
      "3 6.923050403594971\n",
      "4 6.829471230506897\n",
      "5 6.837752521038055\n",
      "6 6.836306512355804\n",
      "7 6.836688995361328\n",
      "8 6.836375892162323\n",
      "9 6.836511135101318\n",
      "10 6.836419939994812\n",
      "11 6.8364662528038025\n",
      "12 6.83644026517868\n",
      "13 6.8364503383636475\n",
      "14 6.836456835269928\n",
      "15 6.836458325386047\n",
      "16 6.836455702781677\n",
      "17 6.836455941200256\n",
      "18 6.836453557014465\n",
      "19 6.836453080177307\n",
      "20 6.836450636386871\n",
      "21 6.836448967456818\n",
      "22 6.836447477340698\n",
      "23 6.836446225643158\n",
      "24 6.836444795131683\n",
      "Learning rate : 0.05\n",
      "train error 43.30%% 433/1000\n",
      "test error 44.60%% 446/1000\n",
      "0 5758.95296049118\n",
      "1 64.39970004558563\n",
      "2 18.327767312526703\n",
      "3 8.085273802280426\n",
      "4 6.987646460533142\n",
      "5 6.858282804489136\n",
      "6 6.813215553760529\n",
      "7 6.757832229137421\n",
      "8 6.66101598739624\n",
      "9 6.489800035953522\n",
      "10 6.181883931159973\n",
      "11 5.743541717529297\n",
      "12 5.333340883255005\n",
      "13 5.020644545555115\n",
      "14 4.765921205282211\n",
      "15 4.581033706665039\n",
      "16 4.433709233999252\n",
      "17 4.296017825603485\n",
      "18 4.189634680747986\n",
      "19 4.075081288814545\n",
      "20 3.9632252752780914\n",
      "21 3.8572840094566345\n",
      "22 3.8074333369731903\n",
      "23 3.64754655957222\n",
      "24 3.724928766489029\n",
      "Learning rate : 0.1\n",
      "train error 15.90%% 159/1000\n",
      "test error 23.50%% 235/1000\n"
     ]
    }
   ],
   "source": [
    "# good result : lr = 0.1 : no overfitting, good test error ~20%\n",
    "\n",
    "for lr in [0.001,0.005, 0.01, 0.05, 0.1]:\n",
    "    model = create_shallow_model()\n",
    "    train_model(model, train_input, new_train_target, lr)\n",
    "    nb_train_errors = compute_nb_errors(model, train_input, new_train_target)\n",
    "    nb_test_errors = compute_nb_errors(model, test_input, new_test_target)\n",
    "    print(\"Learning rate :\", lr)\n",
    "    print('train error {:0.2f}%% {:d}/{:d}'.format((100 * nb_train_errors) / train_input.size(0),\n",
    "                                                       nb_train_errors, train_input.size(0)))\n",
    "\n",
    "    print('test error {:0.2f}%% {:d}/{:d}'.format((100 * nb_test_errors) / test_input.size(0),\n",
    "                                                       nb_test_errors, test_input.size(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-4.9820e-03, -2.4232e-02, -4.3068e-02,  ...,  2.7130e-02,\n",
      "          2.2751e-02,  1.6514e-02],\n",
      "        [ 3.5037e-02, -9.9278e-03,  1.0671e-02,  ..., -4.6963e-02,\n",
      "         -1.7590e-02,  4.3742e-02],\n",
      "        [-1.6918e-02, -1.0712e-02,  3.1103e-02,  ...,  4.7036e-02,\n",
      "          3.4188e-02,  5.2456e-03],\n",
      "        ...,\n",
      "        [-4.5022e-02,  7.9573e-03,  5.3882e-04,  ..., -4.9431e-05,\n",
      "         -2.9987e-02, -3.7793e-02],\n",
      "        [-1.1372e-02,  3.5170e-02, -1.5145e-02,  ...,  7.8792e-04,\n",
      "          1.7646e-02,  4.6101e-02],\n",
      "        [-4.5752e-02,  2.6018e-02,  3.9071e-02,  ..., -4.0710e-02,\n",
      "          2.7416e-02,  2.6722e-05]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0284,  0.0502, -0.0079, -0.0008,  0.0088, -0.0015,  0.0201,  0.0247,\n",
      "        -0.0094, -0.0036, -0.0157, -0.0010,  0.0500, -0.0166, -0.0146,  0.0115,\n",
      "        -0.0361, -0.0108, -0.0382,  0.0082,  0.0129, -0.0005,  0.0133,  0.0401,\n",
      "         0.0114, -0.0208,  0.0195,  0.0072,  0.0135,  0.0179,  0.0140, -0.0143,\n",
      "         0.0301, -0.0258,  0.0309,  0.0385, -0.0379,  0.0013,  0.0389, -0.0052,\n",
      "         0.0364,  0.0272,  0.0028,  0.0156, -0.0166,  0.0433,  0.0250,  0.0398,\n",
      "         0.0092, -0.0039,  0.0124, -0.0100, -0.0329,  0.0045, -0.0281,  0.0109,\n",
      "         0.0122, -0.0281,  0.0164,  0.0475,  0.0120,  0.0351, -0.0071, -0.0361,\n",
      "         0.0464, -0.0488, -0.0446,  0.0169, -0.0289, -0.0217,  0.0086, -0.0221,\n",
      "         0.0399, -0.0011, -0.0017, -0.0203, -0.0264, -0.0306,  0.0340,  0.0168,\n",
      "        -0.0123,  0.0262,  0.0331,  0.0341, -0.0341, -0.0417,  0.0083, -0.0197,\n",
      "         0.0255, -0.0387, -0.0098, -0.0356,  0.0236, -0.0298,  0.0392,  0.0392,\n",
      "        -0.0440,  0.0280, -0.0141, -0.0038, -0.0022,  0.0420,  0.0146, -0.0170,\n",
      "         0.0068,  0.0220,  0.0093,  0.0097, -0.0320, -0.0198, -0.0157,  0.0274,\n",
      "        -0.0171, -0.0172,  0.0216,  0.0455, -0.0333,  0.0031,  0.0350, -0.0062,\n",
      "         0.0364,  0.0237, -0.0069, -0.0161,  0.0053, -0.0068,  0.0291, -0.0127,\n",
      "         0.0403,  0.0171,  0.0246, -0.0331,  0.0063,  0.0466, -0.0147,  0.0395,\n",
      "        -0.0066,  0.0410, -0.0416, -0.0161,  0.0035, -0.0193,  0.0215,  0.0319,\n",
      "        -0.0162,  0.0078,  0.0236, -0.0269,  0.0054, -0.0091, -0.0490, -0.0379,\n",
      "        -0.0498,  0.0375,  0.0141,  0.0344,  0.0365,  0.0071,  0.0330, -0.0365,\n",
      "         0.0352,  0.0366,  0.0111,  0.0266, -0.0422, -0.0365, -0.0299,  0.0236,\n",
      "         0.0118,  0.0168,  0.0406, -0.0349, -0.0276,  0.0431,  0.0232,  0.0405,\n",
      "        -0.0403, -0.0380, -0.0417, -0.0371,  0.0029,  0.0261,  0.0082,  0.0363,\n",
      "         0.0087,  0.0337, -0.0167, -0.0407, -0.0334,  0.0456,  0.0199, -0.0382,\n",
      "         0.0346, -0.0470,  0.0413,  0.0372, -0.0069, -0.0006,  0.0442, -0.0318,\n",
      "         0.0460,  0.0271,  0.0008,  0.0332, -0.0041, -0.0006,  0.0292,  0.0082,\n",
      "         0.0227,  0.0496,  0.0069,  0.0121,  0.0342, -0.0033, -0.0233,  0.0129,\n",
      "         0.0078, -0.0227, -0.0293,  0.0247, -0.0159, -0.0005, -0.0042, -0.0369,\n",
      "        -0.0395, -0.0408,  0.0372, -0.0215,  0.0487,  0.0396, -0.0271, -0.0143,\n",
      "        -0.0447, -0.0231,  0.0336, -0.0178,  0.0468,  0.0135,  0.0474, -0.0192,\n",
      "        -0.0052, -0.0296,  0.0065, -0.0465,  0.0427,  0.0420, -0.0156,  0.0038,\n",
      "        -0.0067, -0.0399,  0.0256, -0.0495,  0.0216, -0.0031, -0.0207, -0.0133,\n",
      "        -0.0150, -0.0043, -0.0063, -0.0011, -0.0124, -0.0496, -0.0298, -0.0086,\n",
      "        -0.0014, -0.0015,  0.0113, -0.0219, -0.0027,  0.0134, -0.0118,  0.0059,\n",
      "        -0.0118, -0.0079, -0.0140, -0.0460,  0.0037,  0.0057, -0.0122, -0.0284,\n",
      "         0.0291, -0.0251, -0.0397, -0.0028,  0.0469, -0.0299, -0.0461,  0.0086,\n",
      "         0.0013, -0.0149, -0.0279,  0.0091,  0.0449,  0.0429,  0.0298, -0.0420,\n",
      "         0.0396,  0.0080,  0.0479, -0.0125, -0.0457, -0.0263, -0.0164, -0.0121,\n",
      "        -0.0131,  0.0018,  0.0168,  0.0440, -0.0030,  0.0081, -0.0432,  0.0360,\n",
      "        -0.0305, -0.0157, -0.0374, -0.0033,  0.0104,  0.0186, -0.0009, -0.0445,\n",
      "        -0.0250, -0.0283,  0.0189,  0.0246,  0.0003,  0.0451,  0.0056, -0.0351,\n",
      "         0.0240, -0.0406,  0.0185, -0.0146, -0.0316, -0.0084, -0.0242, -0.0130,\n",
      "        -0.0388,  0.0387, -0.0143,  0.0424, -0.0152,  0.0274,  0.0336,  0.0443,\n",
      "         0.0021, -0.0337,  0.0268, -0.0110,  0.0147,  0.0370,  0.0012,  0.0102,\n",
      "         0.0152, -0.0158,  0.0360,  0.0129,  0.0226, -0.0302, -0.0130,  0.0018,\n",
      "        -0.0303, -0.0137,  0.0142,  0.0408,  0.0329, -0.0491, -0.0500, -0.0013,\n",
      "        -0.0101, -0.0129,  0.0126, -0.0082,  0.0184,  0.0374,  0.0327,  0.0442,\n",
      "         0.0068,  0.0008, -0.0277,  0.0331,  0.0300, -0.0219,  0.0353, -0.0504,\n",
      "         0.0347, -0.0445,  0.0150,  0.0153, -0.0465,  0.0149, -0.0068,  0.0166,\n",
      "         0.0168,  0.0483, -0.0038,  0.0368, -0.0071,  0.0349,  0.0343, -0.0014,\n",
      "        -0.0389, -0.0220, -0.0060,  0.0068, -0.0326,  0.0131,  0.0435, -0.0355,\n",
      "        -0.0270,  0.0143, -0.0426,  0.0409, -0.0207, -0.0102,  0.0256, -0.0212,\n",
      "        -0.0472,  0.0147,  0.0065,  0.0341,  0.0070,  0.0487,  0.0311,  0.0275,\n",
      "         0.0036, -0.0065,  0.0090, -0.0504, -0.0175,  0.0470,  0.0114, -0.0047,\n",
      "        -0.0412, -0.0235,  0.0069,  0.0188, -0.0174, -0.0459, -0.0313, -0.0202,\n",
      "        -0.0119, -0.0006, -0.0241,  0.0115,  0.0342,  0.0083,  0.0072, -0.0065,\n",
      "        -0.0086, -0.0291, -0.0059,  0.0357, -0.0148, -0.0354, -0.0285,  0.0118,\n",
      "         0.0442, -0.0131, -0.0201,  0.0188, -0.0076,  0.0314, -0.0055,  0.0121,\n",
      "         0.0120,  0.0336,  0.0302,  0.0392,  0.0444, -0.0099, -0.0232, -0.0122,\n",
      "        -0.0172,  0.0372,  0.0354, -0.0334, -0.0405,  0.0155,  0.0154, -0.0144,\n",
      "         0.0020,  0.0086, -0.0313,  0.0462,  0.0430,  0.0373, -0.0175,  0.0300,\n",
      "         0.0021,  0.0429, -0.0081, -0.0010,  0.0103, -0.0255, -0.0049, -0.0452,\n",
      "        -0.0149, -0.0079, -0.0112,  0.0122,  0.0208, -0.0261,  0.0456, -0.0351,\n",
      "         0.0243,  0.0092, -0.0165, -0.0090,  0.0250, -0.0332, -0.0069, -0.0389,\n",
      "         0.0344,  0.0153,  0.0203,  0.0036, -0.0210,  0.0164, -0.0169,  0.0284,\n",
      "         0.0292,  0.0155, -0.0502,  0.0129,  0.0191,  0.0436, -0.0407, -0.0288,\n",
      "         0.0434, -0.0036,  0.0161, -0.0190, -0.0021, -0.0124,  0.0211,  0.0181,\n",
      "         0.0017, -0.0371, -0.0093,  0.0210, -0.0146,  0.0077, -0.0141,  0.0503,\n",
      "        -0.0222,  0.0414,  0.0313, -0.0026, -0.0402, -0.0360, -0.0040, -0.0017,\n",
      "        -0.0228,  0.0317,  0.0449, -0.0411,  0.0465, -0.0416,  0.0321, -0.0068,\n",
      "         0.0072, -0.0412,  0.0145,  0.0226,  0.0178,  0.0463,  0.0150, -0.0063,\n",
      "         0.0145,  0.0444, -0.0062,  0.0049,  0.0420, -0.0232,  0.0281, -0.0321,\n",
      "         0.0221, -0.0505, -0.0428, -0.0397, -0.0483,  0.0462,  0.0299,  0.0139,\n",
      "        -0.0184, -0.0244, -0.0407, -0.0415,  0.0298, -0.0005, -0.0012, -0.0016,\n",
      "        -0.0012,  0.0221, -0.0222, -0.0484, -0.0342,  0.0129, -0.0234, -0.0465,\n",
      "        -0.0329,  0.0056, -0.0472,  0.0371,  0.0268,  0.0260, -0.0173,  0.0102,\n",
      "        -0.0408, -0.0150,  0.0293,  0.0486, -0.0463, -0.0091,  0.0416,  0.0294,\n",
      "        -0.0225, -0.0289,  0.0153, -0.0219, -0.0141, -0.0094,  0.0316, -0.0315,\n",
      "         0.0314, -0.0496,  0.0481,  0.0148,  0.0435,  0.0223, -0.0231, -0.0277,\n",
      "        -0.0019,  0.0460, -0.0033,  0.0192, -0.0472,  0.0007, -0.0369,  0.0231,\n",
      "        -0.0020, -0.0246, -0.0341, -0.0155, -0.0380, -0.0138,  0.0270, -0.0237,\n",
      "         0.0148,  0.0315, -0.0146, -0.0091, -0.0318,  0.0113, -0.0102,  0.0082,\n",
      "        -0.0154, -0.0123,  0.0086, -0.0188, -0.0027,  0.0279,  0.0028, -0.0252,\n",
      "         0.0135,  0.0322, -0.0241,  0.0039,  0.0091,  0.0011,  0.0170,  0.0237,\n",
      "         0.0504,  0.0014, -0.0479,  0.0439, -0.0255, -0.0037, -0.0135,  0.0418,\n",
      "         0.0099,  0.0429, -0.0032,  0.0379,  0.0494, -0.0302,  0.0488,  0.0456,\n",
      "         0.0101, -0.0090,  0.0181,  0.0404, -0.0123, -0.0074,  0.0179, -0.0292,\n",
      "         0.0160, -0.0341, -0.0494, -0.0294, -0.0110,  0.0285,  0.0244, -0.0136,\n",
      "        -0.0288, -0.0478,  0.0388,  0.0044, -0.0339,  0.0142,  0.0039,  0.0393,\n",
      "        -0.0175, -0.0476,  0.0343,  0.0353, -0.0076,  0.0292,  0.0177, -0.0191,\n",
      "         0.0442, -0.0332,  0.0462,  0.0009,  0.0191,  0.0443,  0.0400,  0.0347,\n",
      "         0.0063,  0.0116, -0.0435, -0.0342,  0.0270,  0.0294, -0.0106,  0.0481,\n",
      "         0.0228, -0.0430, -0.0150,  0.0146,  0.0204,  0.0309,  0.0132, -0.0003,\n",
      "         0.0358,  0.0195, -0.0471, -0.0436, -0.0233, -0.0231,  0.0130,  0.0224,\n",
      "        -0.0019,  0.0260, -0.0117, -0.0471, -0.0180, -0.0042, -0.0077, -0.0460,\n",
      "        -0.0303, -0.0220, -0.0401,  0.0326,  0.0061,  0.0154,  0.0136,  0.0147,\n",
      "         0.0385, -0.0080,  0.0217,  0.0424, -0.0338,  0.0220,  0.0360, -0.0047,\n",
      "         0.0141, -0.0426, -0.0250,  0.0355,  0.0326, -0.0095, -0.0003,  0.0153],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0112, -0.0257,  0.0346,  ..., -0.0103,  0.0142, -0.0071],\n",
      "        [-0.0185, -0.0333,  0.0019,  ..., -0.0010,  0.0184, -0.0082]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0116, -0.0258], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''for i in range(len(train_input)):\n",
    "            print(train_input[i].reshape(1, -1).shape)\n",
    "            output = model(train_input[i].reshape(1, -1))\n",
    "            loss = criterion(output, train_target[i].unsqueeze(0))\n",
    "            sum_loss = sum_loss + loss.item()\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    p = p - eta*p.grad'''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_target_changed(train):\n",
    "    t = torch.empty(train.size(0),2).zero_()\n",
    "    for n in range (train.size(0)):\n",
    "        t[n,int(train[n].item())] = 1\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 14, 14])\n",
      "torch.Size([1, 392])\n",
      "tensor([[0]])\n",
      "torch.Size([2, 1, 14])\n"
     ]
    }
   ],
   "source": [
    "print(train_input[0].shape)\n",
    "test = train_input[0].reshape(1, -1)\n",
    "print(test.shape)\n",
    "print(train_target[0].reshape(1,-1))\n",
    "test2 = train_input[0].narrow(1, 2, 1)\n",
    "print(test2.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
