{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x276fd81f108>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build networks combining fully connected layers, Tanh, and ReLU,\n",
    "#run the forward and backward passes,\n",
    "#optimize parameters with SGD for MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_set():\n",
    "    training_set = torch.empty(1000,2).uniform_(0,1)  #x et y\n",
    "    training_classes = torch.empty(1000)\n",
    "    testing_set = torch.empty(1000,2).uniform_(0,1)  #x et y\n",
    "    testing_classes = torch.empty(1000)\n",
    "\n",
    "    r = torch.empty(1,1).fill_(1/(2*math.pi)).pow(1/2)\n",
    "\n",
    "    for i in range (1000):\n",
    "        if ((training_set[i] - torch.Tensor([0.5,0.5])).pow(2).sum()).pow(1/2).item() < r.item():\n",
    "            training_classes[i] = 1\n",
    "        else:\n",
    "            training_classes[i] = 0\n",
    "\n",
    "        if ((testing_set[i] - torch.Tensor([0.5,0.5])).pow(2).sum()).pow(1/2).item() < r.item():\n",
    "            testing_classes[i] = 1\n",
    "        else:\n",
    "            testing_classes[i] = 0\n",
    "    return training_set, training_classes, testing_set, testing_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9580, 0.8544]) tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "training_set, training_classes, testing_set, testing_classes = generate_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(515)\n",
      "tensor(485)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure('1')\n",
    "x = training_set[:,0]\n",
    "y = training_set[:,1]\n",
    "plt.scatter(x[training_classes==1], y[training_classes==1], color='r')\n",
    "plt.scatter(x[training_classes==0], y[training_classes==0], color='b')\n",
    "\n",
    "print(sum(training_classes==0))\n",
    "print(sum(training_classes==1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'training_set = torch.empty(1000,2).uniform_(0,1)  #x et y\\ntraining_classes = torch.empty(1000,1)\\ntesting_set = torch.empty(1000,2).uniform_(0,1)  #x et y\\ntesting_classes = torch.empty(1000,1)\\n\\nr = torch.empty(1,1).fill_(1/(2*math.pi)).pow(1/2)\\n\\nfor i in range (1000):\\n    if (training_set[i].pow(2).sum()).pow(1/2).item() < r.item():\\n        training_classes[i] = 1\\n    else:\\n        training_classes[i] = 0\\n    \\n    if (testing_set[i].pow(2).sum()).pow(1/2).item() < r.item():\\n        testing_classes[i] = 1\\n    else:\\n        testing_classes[i] = 0\\n\\n#builds a network with two input units, two output units, three hidden layers of 25 units,\\n#trains it with MSE, logging the loss,\\n#computes and prints the ﬁnal train and the test errors.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"training_set = torch.empty(1000,2).uniform_(0,1)  #x et y\n",
    "training_classes = torch.empty(1000,1)\n",
    "testing_set = torch.empty(1000,2).uniform_(0,1)  #x et y\n",
    "testing_classes = torch.empty(1000,1)\n",
    "\n",
    "r = torch.empty(1,1).fill_(1/(2*math.pi)).pow(1/2)\n",
    "\n",
    "for i in range (1000):\n",
    "    if (training_set[i].pow(2).sum()).pow(1/2).item() < r.item():\n",
    "        training_classes[i] = 1\n",
    "    else:\n",
    "        training_classes[i] = 0\n",
    "    \n",
    "    if (testing_set[i].pow(2).sum()).pow(1/2).item() < r.item():\n",
    "        testing_classes[i] = 1\n",
    "    else:\n",
    "        testing_classes[i] = 0\n",
    "\n",
    "#builds a network with two input units, two output units, three hidden layers of 25 units,\n",
    "#trains it with MSE, logging the loss,\n",
    "#computes and prints the ﬁnal train and the test errors.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LINEAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LINEAR MODULE (FULLY CONNECTED LAYER)\n",
    "#mini batch : x[mini_batch_size * D]D=2\n",
    "class Linear(object):\n",
    "    def __init__(self, dimension, nb_data_in, nb_data_out):\n",
    "        #x = nb_data_in x dim in my calculation, but here transpose (dim x nb_data_in)\n",
    "        #y = nb_data_out x dim in my calculation, but here transpose (dim x nb_data_out)\n",
    "        k = math.sqrt(1/nb_data_in)\n",
    "        self.weight = torch.empty(nb_data_out,nb_data_in).uniform_(-k,k)\n",
    "        self.bias = torch.empty(nb_data_out,dimension).uniform_(-k,k)\n",
    "        self.grad_weight = None\n",
    "        self.grad_bias = None\n",
    "        self.input = None\n",
    "    \n",
    "    def forward(self , *input, nb_layer):\n",
    "        #All the calculation were done considering x = nb_data_in x dim (as seen in the lesson)\n",
    "        #So to simplify the comprehension, we do a first transpose, do the the calculations with this\n",
    "        #then transpose again after, to have the correct dimension output\n",
    "        print('input forward before')\n",
    "        print(input)\n",
    "        #save x with dim = dim x nb_data_in\n",
    "        if(nb_layer > 1):\n",
    "            input = input[0]\n",
    "            #pour le premier, on aura en entrée un ou plusieurs tensors (donne tuple quand rentré dans f)\n",
    "            \n",
    "        print('input forward after')\n",
    "        print(input)\n",
    "        #save x with dim = dim x nb_data_in    \n",
    "        self.input = input\n",
    "        output = []\n",
    "        \n",
    "        for x in input:\n",
    "            print('x')\n",
    "            print(x)\n",
    "            #x = dim x nb_data_in -> nb_data_in x dim\n",
    "            x_correct_dim = x.t()\n",
    "            #y = nb_data_out x dim\n",
    "            y_correct_dim = None\n",
    "\n",
    "            #dim analysis: [nb_data_out x nb_data_in] x [nb_data_in x dim] + [nb_data_out x dim]\n",
    "            y_correct_dim = ((self.weight).matmul(x_correct_dim)+self.bias)\n",
    "\n",
    "            #append y = [dim x nb_data_out] to respect lin module\n",
    "            output.append(y_correct_dim.t()) \n",
    "        return tuple(output)\n",
    "    \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        dl_dx  = None\n",
    "        dl_dw = []\n",
    "        dl_db = []\n",
    "        gradaccumulated = []\n",
    "        print('g bef')\n",
    "        print(gradwrtoutput)\n",
    "        gradwrtoutput = gradwrtoutput[0]\n",
    "        print('g aft')\n",
    "        print(gradwrtoutput)\n",
    "        print(len(gradwrtoutput))\n",
    "        \n",
    "        for i in range (len(gradwrtoutput)):\n",
    "            #X = dim x nb_data_in -> nb_data_in x dim\n",
    "            x_correct_dim = self.input[i].t()\n",
    "            \n",
    "            #X = dim x nb_data_out -> nb_data_out x dim\n",
    "            print('g i')\n",
    "            #if(len(gradwrtoutput) == 1):\n",
    "                \n",
    "            #print(gradwrtoutput.t())\n",
    "            #print(gradwrtoutput.t().size())\n",
    "            grad_correct_dim = gradwrtoutput[i].t()\n",
    "            print(grad_correct_dim)\n",
    "            \n",
    "            #dl_dx = w^T x dl_dy (gradwrtoutput)\n",
    "            #dim analysis: nb_data_in x dim = [nb_data_in x nb_data_out] x [nb_data_out x dim]\n",
    "            dl_dx = (self.weight).t().matmul(grad_correct_dim)\n",
    "            #nb_data_in x dim -> dim x nb_data_in\n",
    "            gradaccumulated.append(dl_dx.t())\n",
    "            \n",
    "            #dl_db = dl_dy\n",
    "            #dim analysis: nb_data_out x dim (car b = nb_data_out x dim)\n",
    "            dl_db.append(grad_correct_dim)\n",
    "            \n",
    "            #dl_dw = dl_dy x X^T\n",
    "            #dim analysis: nb_data_out x nb_data_in = [nb_data_out x dim] x [dim x nb_data_in]\n",
    "            dl_dw.append(grad_correct_dim.matmul(x_correct_dim.t()))\n",
    "            \n",
    "        self.grad_weight = dl_dw\n",
    "        self.grad_bias = dl_db\n",
    "\n",
    "        return tuple(gradaccumulated)\n",
    "        \n",
    "    def param(self):\n",
    "        output = [[self.weight, self.grad_weight], [self.bias, self.grad_bias]]\n",
    "        return output\n",
    "    \n",
    "    #https://pytorch.org/docs/stable/nn.html#linear-layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "(tensor([[ 0.8324, -0.3725]]), tensor([[-0.6950, -1.0685]]))\n",
      "dim =  1  input = 2\n",
      "nb_out =  3\n",
      "input forward before\n",
      "((tensor([[ 0.8324, -0.3725]]), tensor([[-0.6950, -1.0685]])),)\n",
      "input forward after\n",
      "(tensor([[ 0.8324, -0.3725]]), tensor([[-0.6950, -1.0685]]))\n",
      "x\n",
      "tensor([[ 0.8324, -0.3725]])\n",
      "x\n",
      "tensor([[-0.6950, -1.0685]])\n",
      "forward\n",
      "(tensor([[-0.9116, -1.2593, -0.9299]]), tensor([[-0.3421, -0.4755, -0.4136]]))\n"
     ]
    }
   ],
   "source": [
    "a = torch.empty(1,2).normal_()\n",
    "b = torch.empty(a.size()).normal_()\n",
    "l = []\n",
    "l.append(a)\n",
    "l.append(b)\n",
    "i = tuple(l)\n",
    "print('input')\n",
    "#print(a,b)\n",
    "print(i)\n",
    "\n",
    "print('dim = ', i[0].size(0),' input =', i[0].size(1))\n",
    "nb_out = 3\n",
    "print('nb_out = ', nb_out)\n",
    "\n",
    "lin = Linear(i[0].size(0),i[0].size(1),nb_out)\n",
    "f = lin.forward(i,nb_layer = 2)\n",
    "print('forward')\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input\n",
      "tensor([[1.4728, 2.2418]])\n",
      "(tensor([[1.4728, 2.2418]]),)\n",
      "dim =  1  input = 2\n",
      "nb_out =  3\n",
      "input forward before\n",
      "((tensor([[1.4728, 2.2418]]),),)\n",
      "input forward after\n",
      "(tensor([[1.4728, 2.2418]]),)\n",
      "x\n",
      "tensor([[1.4728, 2.2418]])\n",
      "forward\n",
      "(tensor([[-1.5019,  1.0424,  0.6305]]),)\n"
     ]
    }
   ],
   "source": [
    "a = torch.empty(1,2).normal_()\n",
    "l = []\n",
    "l.append(a)\n",
    "i = tuple(l)\n",
    "print('input')\n",
    "print(a)\n",
    "print(i)\n",
    "\n",
    "print('dim = ', i[0].size(0),' input =', i[0].size(1))\n",
    "nb_out = 3\n",
    "print('nb_out = ', nb_out)\n",
    "\n",
    "lin = Linear(i[0].size(0),i[0].size(1),nb_out)\n",
    "f = lin.forward(i,nb_layer = 2)\n",
    "print('forward')\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad\n",
      "(tensor([[-0.5406, -0.9405, -1.2741]]),)\n",
      "g bef\n",
      "((tensor([[-0.5406, -0.9405, -1.2741]]),),)\n",
      "g aft\n",
      "(tensor([[-0.5406, -0.9405, -1.2741]]),)\n",
      "1\n",
      "g i\n",
      "tensor([[-0.5406],\n",
      "        [-0.9405],\n",
      "        [-1.2741]])\n",
      "gradaccumulated\n",
      "(tensor([[ 1.3945, -0.6135]]),)\n"
     ]
    }
   ],
   "source": [
    "g = []\n",
    "grad = torch.empty(i[0].size(0),nb_out).normal_()\n",
    "g.append(grad)\n",
    "gr = tuple(g)\n",
    "print('grad')\n",
    "print(gr)\n",
    "output = lin.backward(gr)\n",
    "print('gradaccumulated')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = lin.param()\n",
    "print('weight and grad')\n",
    "print('must be of size', nb_out, ' x ', a.size(1))\n",
    "print('size', p[0][0].size(0), p[0][0].size(1))\n",
    "#print(p[0][0], p[0][1])\n",
    "print('bias and grad')\n",
    "print('must be of size', nb_out, ' x ', a.size(0))\n",
    "print('size', p[1][0].size(0), p[1][0].size(1))\n",
    "#print(p[1][0], p[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RELU MODULE\n",
    "class ReLU():\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "    \n",
    "    def forward(self, *input):\n",
    "        print('input ReLU sans correction')\n",
    "        print(input)\n",
    "        \n",
    "        input = input[0]\n",
    "        liste = []\n",
    "        self.input = input\n",
    "        print('input corrected')\n",
    "        print(input)\n",
    "        for x in input:\n",
    "            print(x)\n",
    "            liste.append(torch.max(x,torch.zeros_like(x))) \n",
    "        output = tuple(liste)\n",
    "        return output\n",
    "        \n",
    "    def backward(self, *gradwrtoutput): \n",
    "        #add input as param ? \n",
    "        #No, parameters are w and b when linear, but input is saved has if we have to give backward and input\n",
    "        #in the case they're both tuple of tensors of size not know\n",
    "        #it's complicated (maybe impossible) to code it with the * (let the size be whatever we want)\n",
    "        derivative = []\n",
    "        gradaccumulated = []\n",
    "        print('gradwrtoutput before')\n",
    "        print(gradwrtoutput)\n",
    "        gradwrtoutput = gradwrtoutput[0]\n",
    "        print('gradwrtoutput after')\n",
    "        print(gradwrtoutput)\n",
    "        \n",
    "        for x in self.input:\n",
    "            dx = (x>=0).float()\n",
    "            derivative.append(dx)\n",
    "        for i in range (len(derivative)):\n",
    "            gradaccumulated.append(derivative[i]*gradwrtoutput[i])\n",
    "        output = tuple(gradaccumulated)\n",
    "        return output   \n",
    "\n",
    "    def param(self): \n",
    "        return []\n",
    "    \n",
    "#backward should get as input a tensor or a tuple of tensors containing the gradient of the \n",
    "#loss with respect to the module’s output, accumulate the gradient wrt the parameters, \n",
    "#and return a tensor or a tuple of tensors containing the gradient of the loss wrt the module’s input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.]])\n",
      "tensor([[-2., -2.]])\n",
      "(tensor([[3., 3.]]), tensor([[-2., -2.]]))\n",
      "input ReLU sans correction\n",
      "((tensor([[3., 3.]]), tensor([[-2., -2.]])),)\n",
      "input corrected\n",
      "(tensor([[3., 3.]]), tensor([[-2., -2.]]))\n",
      "tensor([[3., 3.]])\n",
      "tensor([[-2., -2.]])\n",
      "ReLU of x, y\n",
      "(tensor([[3., 3.]]), tensor([[0., 0.]]))\n"
     ]
    }
   ],
   "source": [
    "R = ReLU()\n",
    "x = torch.empty(1,2).fill_(3)\n",
    "y = torch.empty(1,2).fill_(-2)\n",
    "l = []\n",
    "l.append(x)\n",
    "l.append(y)\n",
    "l = tuple(l)\n",
    "print(x)\n",
    "print(y)\n",
    "print(l)\n",
    "r = R.forward(l)\n",
    "print('ReLU of x, y')\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[2., 2.]]), tensor([[2., 2.]]))\n",
      "gradwrtoutput before\n",
      "((tensor([[2., 2.]]), tensor([[2., 2.]])),)\n",
      "gradwrtoutput after\n",
      "(tensor([[2., 2.]]), tensor([[2., 2.]]))\n",
      "grad\n",
      "(tensor([[2., 2.]]), tensor([[0., 0.]]))\n"
     ]
    }
   ],
   "source": [
    "grad1 = torch.empty(1,2).fill_(2)\n",
    "grad2 = torch.empty(1,2).fill_(2)\n",
    "g = []\n",
    "g.append(grad1)\n",
    "g.append(grad2)\n",
    "gr = tuple(g)\n",
    "\n",
    "print(gr)\n",
    "grad = R.backward(gr)\n",
    "print('grad')\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TANH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TANH MODULE\n",
    "class Tanh():\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        \n",
    "    def forward(self, *input): \n",
    "        output = []\n",
    "        self.input = input\n",
    "        for x in input:\n",
    "            output.append(torch.tanh(x)) \n",
    "        return tuple(output)\n",
    "    \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        output = []\n",
    "        for i in range (len(gradwrtoutput)):\n",
    "            # derivative = (1 - torch.tanh(x).pow(2))\n",
    "            output.append((1 - torch.tanh(self.input[i]).pow(2))*gradwrtoutput[i])\n",
    "        return tuple(output)   \n",
    "\n",
    "    def param(self):\n",
    "        return [] #Pas de param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.]]) tensor([[2., 2.]])\n",
      "tensor([[2., 2.]]) tensor([[2., 2.]])\n",
      "tanh of x, y\n",
      "(tensor([[0.7616, 0.7616]]), tensor([[0.9640, 0.9640]]))\n",
      "True\n",
      "grad\n",
      "(tensor([[0.8399, 0.8399]]), tensor([[0.1413, 0.1413]]))\n"
     ]
    }
   ],
   "source": [
    "T = Tanh()\n",
    "x = torch.empty(1,2).fill_(1)\n",
    "y = torch.empty(1,2).fill_(2)\n",
    "grad1 = torch.empty(1,2).fill_(2)\n",
    "grad2 = torch.empty(1,2).fill_(2)\n",
    "print(x, y)\n",
    "print(grad1, grad2)\n",
    "t = T.forward(x, y)\n",
    "print('tanh of x, y')\n",
    "print(t)\n",
    "print(isinstance(t,tuple))\n",
    "grad = T.backward(grad1,grad2)\n",
    "print('grad')\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOSSMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOSSMSE MODULE\n",
    "    \n",
    "class LossMSE():\n",
    "    #def __init__(self):\n",
    "        #Besoin d'aucun je pense\n",
    "        #self.loss = None\n",
    "        #self.gradloss = None\n",
    "    \n",
    "    def forward(self, *input, target): \n",
    "        #Si plusieurs input et target comment on fait?\n",
    "        #On aura un tuple de tensor qui consistera l'input, puis target = tuple of tensor target\n",
    "        liste = torch.Tensor([])\n",
    "        #Utilisation de cette écriture pour pouvoir faire somme sur \"liste\" (l = [] marche pas avec sum())\n",
    "        for i in range (len(input)):\n",
    "            liste = torch.cat((liste,(input[i]-target[i]).pow(2)))\n",
    "        loss = torch.sum(liste)/len(input)\n",
    "        #self.loss = loss même pas besoin\n",
    "        return loss\n",
    "        \n",
    "    def backward(self, *input, target):\n",
    "        gradaccumulated = []\n",
    "        #dloss = torch.Tensor([])\n",
    "        #Problem with this definition, is that we cannot have a tuple after (demandé dans la donnée)\n",
    "        dloss = []\n",
    "        for i,x in enumerate(input):\n",
    "            #dloss = torch.cat((dloss, 2*(x - target[i])/len(input)))\n",
    "            dloss.append(2*(x - target[i])/len(input))\n",
    "        #self.gradloss = dloss même pas besoin\n",
    "        #Backward must give a tensor or a tuple of tensors donc obligé de convertir\n",
    "        return tuple(dloss)\n",
    "\n",
    "    def param(self):\n",
    "        return [] #No param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input tensor([[1., 1.]]) tensor([[2., 2.]])\n",
      "target tensor([[-1., -1.]]) tensor([[-1., -1.]])\n",
      "loss tensor(13.)\n",
      "dloss (tensor([[2., 2.]]), tensor([[3., 3.]]))\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "N = LossMSE()\n",
    "x = torch.empty(1,2).fill_(1)\n",
    "y = torch.empty(1,2).fill_(2)\n",
    "print(\"input\", x, y)\n",
    "t = torch.empty(1,2).fill_(-1)\n",
    "t2 = torch.empty(1,2).fill_(-1)\n",
    "print(\"target\", t, t2)\n",
    "l = N.forward(x, y , target= [t, t2])\n",
    "print('loss', l)\n",
    "dl = N.backward(x, y, target=[t, t2])\n",
    "print('dloss', dl)\n",
    "print(isinstance(dl,tuple))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEQUENTIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SEQUENTIAL MODULE (to combine several modules in basic sequential structure)\n",
    "class Sequential(object):\n",
    "    def forward(self , *input):\n",
    "        raise NotImplementedError\n",
    "    def backward(self , *gradwrtoutput):\n",
    "        raise NotImplementedError\n",
    "    def param(self): \n",
    "        return []\n",
    "    \n",
    "#forward should get for input, and returns, a tensor or a tuple of tensors.\n",
    "\n",
    "#backward should get as input a tensor or a tuple of tensors containing the gradient of the \n",
    "#loss with respect to the module’s output, accumulate the gradient wrt the parameters, \n",
    "#and return a tensor or a tuple of tensors containing the gradient of the loss wrt the module’s input.\n",
    "\n",
    "#param should return a list of pairs, each composed of a parameter tensor, and a gradient tensor of same size. \n",
    "#This list should be empty for parameterless modules (e.g. ReLU).\n",
    "#Some modules may requires additional methods, and some modules may keep track of information from the forward \n",
    "#pass to be used in the backward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
