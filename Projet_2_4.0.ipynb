{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x24ee2a55a08>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_set():\n",
    "    training_set = torch.empty(1000,2).uniform_(0,1)  #x et y\n",
    "    training_classes = torch.empty(1000)\n",
    "    testing_set = torch.empty(1000,2).uniform_(0,1)  #x et y\n",
    "    testing_classes = torch.empty(1000)\n",
    "\n",
    "    r = torch.empty(1,1).fill_(1/(2*math.pi)).pow(1/2)\n",
    "\n",
    "    for i in range (1000):\n",
    "        if ((training_set[i] - torch.Tensor([0.5,0.5])).pow(2).sum()).pow(1/2).item() < r.item():\n",
    "            training_classes[i] = 1\n",
    "        else:\n",
    "            training_classes[i] = 0\n",
    "\n",
    "        if ((testing_set[i] - torch.Tensor([0.5,0.5])).pow(2).sum()).pow(1/2).item() < r.item():\n",
    "            testing_classes[i] = 1\n",
    "        else:\n",
    "            testing_classes[i] = 0\n",
    "    return training_set, training_classes, testing_set, testing_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set, training_classes, testing_set, testing_classes = generate_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(504)\n",
      "tensor(496)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure('1')\n",
    "x = training_set[:,0]\n",
    "y = training_set[:,1]\n",
    "plt.scatter(x[training_classes==1], y[training_classes==1], color='r')\n",
    "plt.scatter(x[training_classes==0], y[training_classes==0], color='b')\n",
    "\n",
    "print(sum(training_classes==0))\n",
    "print(sum(training_classes==1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LINEAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LINEAR MODULE (FULLY CONNECTED LAYER)\n",
    "class Linear(object):\n",
    "    def __init__(self, dimension, nb_data_in, nb_data_out):\n",
    "        #x = nb_data_in x dim in my calculation, but here transpose (dim x nb_data_in)\n",
    "        #y = nb_data_out x dim in my calculation, but here transpose (dim x nb_data_out)\n",
    "        k = math.sqrt(1/nb_data_in)\n",
    "        self.weight = torch.empty(nb_data_out,nb_data_in).uniform_(-k,k)\n",
    "        self.bias = torch.empty(nb_data_out,dimension).uniform_(-k,k)\n",
    "        self.grad_weight = None\n",
    "        self.grad_bias = None\n",
    "        self.input = None\n",
    "        \n",
    "    def updateparam(self, lr):\n",
    "        for i in range(len(self.grad_weight)): \n",
    "            self.weight -= lr * self.grad_weight\n",
    "            self.bias -= lr * self.grad_bias\n",
    "    \n",
    "    def forward(self , input, nb_layer):\n",
    "        #All the calculation were done considering x = nb_data_in x dim (as seen in the lesson)\n",
    "        #So to simplify the comprehension, we do a first transpose, do the the calculations with this\n",
    "        #then transpose again after, to have the correct dimension output\n",
    "       \n",
    "        #save x with dim = dim x nb_data_in    \n",
    "        self.input = input\n",
    "        output = None\n",
    "        \n",
    "        #x = dim x nb_data_in -> nb_data_in x dim\n",
    "        x_correct_dim = input.t()\n",
    "        #y = nb_data_out x dim\n",
    "        y_correct_dim = None\n",
    "\n",
    "        #dim analysis: [nb_data_out x nb_data_in] x [nb_data_in x dim] + [nb_data_out x dim]\n",
    "        y_correct_dim = ((self.weight).matmul(x_correct_dim)+self.bias)\n",
    "\n",
    "        #append y = [dim x nb_data_out] to respect lin module \n",
    "        return (y_correct_dim.t())\n",
    "    \n",
    "    def backward(self, gradwrtoutput):\n",
    "        dl_dx  = None\n",
    "        dl_dw = None\n",
    "        dl_db = None\n",
    "        gradaccumulated = None\n",
    "        \n",
    "        #X = dim x nb_data_in -> nb_data_in x dim\n",
    "        x_correct_dim = (self.input).t()\n",
    "\n",
    "        #X = dim x nb_data_out -> nb_data_out x dim\n",
    "        grad_correct_dim = gradwrtoutput.t()\n",
    "\n",
    "        #dl_dx = w^T x dl_dy (gradwrtoutput)\n",
    "        #dim analysis: nb_data_in x dim = [nb_data_in x nb_data_out] x [nb_data_out x dim]\n",
    "        dl_dx = (self.weight).t().matmul(grad_correct_dim)\n",
    "        #nb_data_in x dim -> dim x nb_data_in\n",
    "        gradaccumulated = dl_dx.t()\n",
    "\n",
    "        #dl_db = dl_dy\n",
    "        #dim analysis: nb_data_out x dim (car b = nb_data_out x dim)\n",
    "        self.grad_bias = grad_correct_dim\n",
    "\n",
    "        #dl_dw = dl_dy x X^T\n",
    "        #dim analysis: nb_data_out x nb_data_in = [nb_data_out x dim] x [dim x nb_data_in]\n",
    "        self.grad_weight = grad_correct_dim.matmul(x_correct_dim.t())\n",
    "            \n",
    "        return gradaccumulated\n",
    "        \n",
    "    def param(self):\n",
    "        output = [[self.weight, self.grad_weight], [self.bias, self.grad_bias]]\n",
    "        return output\n",
    "    \n",
    "    #https://pytorch.org/docs/stable/nn.html#linear-layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RELU MODULE\n",
    "class ReLU():\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return torch.max(input,torch.zeros_like(x))\n",
    "        \n",
    "    def backward(self, gradwrtoutput): \n",
    "        dx = ((self.input)>=0).float()\n",
    "        return dx*gradwrtoutput \n",
    "\n",
    "    def param(self): \n",
    "        return []\n",
    "    \n",
    "#backward should get as input a tensor or a tuple of tensors containing the gradient of the \n",
    "#loss with respect to the module’s output, accumulate the gradient wrt the parameters, \n",
    "#and return a tensor or a tuple of tensors containing the gradient of the loss wrt the module’s input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TANH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TANH MODULE\n",
    "class Tanh():\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return torch.tanh(input)\n",
    "    \n",
    "    def backward(self, gradwrtoutput):\n",
    "        return (1 - torch.tanh(self.input).pow(2))*gradwrtoutput \n",
    "\n",
    "    def param(self):\n",
    "        return [] #Pas de param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOSSMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOSSMSE MODULE\n",
    "    \n",
    "class LossMSE():\n",
    "    \n",
    "    def forward(self, input, target): \n",
    "        loss = torch.mean((input-target).pow(2))\n",
    "        return loss\n",
    "        \n",
    "    def backward(self, input, target):\n",
    "        target = target.unsqueeze(0)\n",
    "        #print(type(input), input.size(), input, type(target),target.size(), target)\n",
    "        dloss = (2*(input - target))/(input.size(1))\n",
    "        return dloss\n",
    "\n",
    "    def param(self):\n",
    "        return [] #No param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_into_2(classes):\n",
    "    t = torch.empty(classes.size(0),2).zero_()\n",
    "    for n in range (classes.size(0)):\n",
    "        t[n,int(classes[n].item())] = 1\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient 2 layer Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "\n",
      "iteration 0\n",
      "loss : tensor(275.0952)\n",
      "\n",
      "iteration 1\n",
      "loss : tensor(254.7361)\n",
      "\n",
      "iteration 2\n",
      "loss : tensor(253.7498)\n",
      "\n",
      "iteration 3\n",
      "loss : tensor(252.8886)\n",
      "\n",
      "iteration 4\n",
      "loss : tensor(252.0308)\n",
      "\n",
      "iteration 5\n",
      "loss : tensor(251.1161)\n",
      "\n",
      "iteration 6\n",
      "loss : tensor(250.0982)\n",
      "\n",
      "iteration 7\n",
      "loss : tensor(248.9326)\n",
      "\n",
      "iteration 8\n",
      "loss : tensor(247.5675)\n",
      "\n",
      "iteration 9\n",
      "loss : tensor(245.9406)\n",
      "\n",
      "iteration 10\n",
      "loss : tensor(243.9708)\n",
      "\n",
      "iteration 11\n",
      "loss : tensor(241.5573)\n",
      "\n",
      "iteration 12\n",
      "loss : tensor(238.5749)\n",
      "\n",
      "iteration 13\n",
      "loss : tensor(234.8779)\n",
      "\n",
      "iteration 14\n",
      "loss : tensor(230.3055)\n",
      "\n",
      "iteration 15\n",
      "loss : tensor(224.6931)\n",
      "\n",
      "iteration 16\n",
      "loss : tensor(217.8980)\n",
      "\n",
      "iteration 17\n",
      "loss : tensor(209.8297)\n",
      "\n",
      "iteration 18\n",
      "loss : tensor(200.4906)\n",
      "\n",
      "iteration 19\n",
      "loss : tensor(190.0112)\n",
      "\n",
      "iteration 20\n",
      "loss : tensor(178.6503)\n",
      "\n",
      "iteration 21\n",
      "loss : tensor(166.7588)\n",
      "\n",
      "iteration 22\n",
      "loss : tensor(154.7162)\n",
      "\n",
      "iteration 23\n",
      "loss : tensor(142.8907)\n",
      "\n",
      "iteration 24\n",
      "loss : tensor(131.6328)\n",
      "\n",
      "iteration 25\n",
      "loss : tensor(121.2895)\n",
      "\n",
      "iteration 26\n",
      "loss : tensor(112.1740)\n",
      "\n",
      "iteration 27\n",
      "loss : tensor(104.5011)\n",
      "\n",
      "iteration 28\n",
      "loss : tensor(98.3271)\n",
      "\n",
      "iteration 29\n",
      "loss : tensor(93.5527)\n",
      "\n",
      "iteration 30\n",
      "loss : tensor(89.9702)\n",
      "\n",
      "iteration 31\n",
      "loss : tensor(87.3368)\n",
      "\n",
      "iteration 32\n",
      "loss : tensor(85.4240)\n",
      "\n",
      "iteration 33\n",
      "loss : tensor(84.0403)\n",
      "\n",
      "iteration 34\n",
      "loss : tensor(83.0369)\n",
      "\n",
      "iteration 35\n",
      "loss : tensor(82.3046)\n",
      "\n",
      "iteration 36\n",
      "loss : tensor(81.7626)\n",
      "\n",
      "iteration 37\n",
      "loss : tensor(81.3545)\n",
      "\n",
      "iteration 38\n",
      "loss : tensor(81.0405)\n",
      "\n",
      "iteration 39\n",
      "loss : tensor(80.7927)\n",
      "\n",
      "iteration 40\n",
      "loss : tensor(80.5917)\n",
      "\n",
      "iteration 41\n",
      "loss : tensor(80.4235)\n",
      "\n",
      "iteration 42\n",
      "loss : tensor(80.2793)\n",
      "\n",
      "iteration 43\n",
      "loss : tensor(80.1511)\n",
      "\n",
      "iteration 44\n",
      "loss : tensor(80.0356)\n",
      "\n",
      "iteration 45\n",
      "loss : tensor(79.9292)\n",
      "\n",
      "iteration 46\n",
      "loss : tensor(79.8291)\n",
      "\n",
      "iteration 47\n",
      "loss : tensor(79.7340)\n",
      "\n",
      "iteration 48\n",
      "loss : tensor(79.6425)\n",
      "\n",
      "iteration 49\n",
      "loss : tensor(79.5537)\n"
     ]
    }
   ],
   "source": [
    "nb_iterations = 50\n",
    "print(nb_iterations)\n",
    "\n",
    "lin1 = Linear(1,2,25)\n",
    "lin2 = Linear(1,25,2)\n",
    "T1 = Tanh()\n",
    "L = LossMSE()\n",
    "  \n",
    "\n",
    "for i in range (nb_iterations):\n",
    "    print()\n",
    "    print('iteration', i)\n",
    "    loss_training = 0\n",
    "    \n",
    "    #1000 x 1 -> 1000 x 2:  [1 0] and [0 1]\n",
    "    train_target = class_into_2(training_classes);\n",
    "    test_target = class_into_2(testing_classes);\n",
    "    \n",
    "    #Etape Update valeur par valeur\n",
    "    for j in range(training_set.size(0)):\n",
    "        f1 = lin1.forward(training_set[j].unsqueeze(0), nb_layer = 1)\n",
    "        #print(f1)\n",
    "        t1 = T1.forward(f1)\n",
    "        #print(t1)\n",
    "        output = (lin2.forward(t1, nb_layer = 2))\n",
    "        #print('output sortie :', output)\n",
    "\n",
    "\n",
    "        loss = L.forward(output, target = train_target[j])\n",
    "        loss_training = loss_training + loss\n",
    "        \n",
    "        #print('output entré dans backward :',output.size(), output)\n",
    "        dl = L.backward(output, target = train_target[j])\n",
    "        #print('output entré dans lin1 backward :', dl.size(), dl)\n",
    "        #print('dl',dl)\n",
    "        bf2 = lin2.backward(dl)\n",
    "        #print('bf2', bf2.size(), bf2)\n",
    "        bt1 = T1.backward(bf2)\n",
    "        #print('bt1', bt1)\n",
    "        bf1 = lin1.backward(bt1)\n",
    "        #print('bf1',bf1)\n",
    "\n",
    "        lr = 0.001 #>0.025 donne des meilleurs résultats (sinon oscille ou bloque)\n",
    "        #print(lin1.param()[0][0])\n",
    "        lin1.updateparam(lr)\n",
    "        lin2.updateparam(lr)\n",
    "        #print(lin1.param()[0][0])\n",
    "    print('loss :', loss_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " errors training:  47\n",
      " errors testing:  49\n"
     ]
    }
   ],
   "source": [
    "#Test après\n",
    "errors_training = 0\n",
    "for j in range(training_set.size(0)):\n",
    "        f1 = lin1.forward(training_set[j].unsqueeze(0), nb_layer = 1)\n",
    "        t1 = T1.forward(f1)\n",
    "        output = (lin2.forward(t1, nb_layer = 2))\n",
    "\n",
    "        pred = output.squeeze().max(0)[1].item() # 1 x 2 -> 2\n",
    "        if train_target[j, pred].item() < 0.5: \n",
    "            #print(output, train_target[j])\n",
    "            errors_training = errors_training + 1\n",
    "\n",
    "print(' errors training: ', errors_training)\n",
    "\n",
    "errors_testing = 0\n",
    "for j in range(testing_set.size(0)):\n",
    "    f1 = lin1.forward(testing_set[j].unsqueeze(0), nb_layer = 1)\n",
    "    t1 = T1.forward(f1)\n",
    "    output = (lin2.forward(t1, nb_layer = 2))\n",
    "\n",
    "    pred = output.squeeze().max(0)[1].item() # 1 x 2 -> 2\n",
    "    if test_target[j, pred] < 0.5: \n",
    "        #print(output, test_target[j])\n",
    "        errors_testing = errors_testing + 1\n",
    "\n",
    "print(' errors testing: ', errors_testing,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEQUENTIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
