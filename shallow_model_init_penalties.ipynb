{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "\n",
    "import dlc_practical_prologue as prologue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-9ac5deb0902f>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-9ac5deb0902f>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    Need to implement bad init & L2/L2 penalties on other files ?\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "###QUESTIONS TO TA \n",
    "Need to implement bad init & L2/L2 penalties on other files ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000 #nb of pairs\n",
    "\n",
    "#generate pairs\n",
    "train_input, train_target, train_classes, test_input, test_target, test_classes = prologue.generate_pair_sets(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize the input\n",
    "train_input/=255\n",
    "test_input/=255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_target = torch.empty(N,2)\n",
    "new_test_target = torch.empty(N,2)\n",
    "for i in range(N):\n",
    "    if train_target[i] == 1 :\n",
    "        new_train_target[i,0] = 0\n",
    "        new_train_target[i,1] = 1\n",
    "        \n",
    "    else:\n",
    "        new_train_target[i,0] = 1\n",
    "        new_train_target[i,1] = 0\n",
    "        \n",
    "    if test_target[i] == 1:\n",
    "        new_test_target[i,0] = 0\n",
    "        new_test_target[i,1] = 1\n",
    "        \n",
    "    else:\n",
    "        new_test_target[i,0] = 1\n",
    "        new_test_target[i,1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_shallow_model():\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(392, 400),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(400, 500),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(500, 600),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(600, 2),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, nb_hidden):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(392, 784, kernel_size=4)\n",
    "        self.conv2 = nn.Conv1d(784, 1568, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(256, nb_hidden)\n",
    "        self.fc2 = nn.Linear(nb_hidden, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=3))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2))\n",
    "        x = F.relu(self.fc1(x.view(-1, 256)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_input, train_target,lr):\n",
    "    epoch = 25\n",
    "    eta = 0.2\n",
    "    mini_batches = 100\n",
    "    optimizer = optim.Adam(model.parameters(), lr)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    for e in range(epoch):\n",
    "        sum_loss = 0\n",
    "        \n",
    "        for b in range(0, train_input.size(0), mini_batches):\n",
    "            \n",
    "            output = model(train_input.narrow(0, b, mini_batches).reshape(mini_batches, 1, -1))\n",
    "            #print('shapes',output.squeeze(1).shape, train_target.narrow(0, b, mini_batches).shape)\n",
    "            loss = criterion(output.squeeze(1), train_target.narrow(0, b, mini_batches))\n",
    "            loss.requires_grad_()\n",
    "            #print(\"output\", output.squeeze(1), \"train\", train_target.narrow(0, b, mini_batches))\n",
    "            model.zero_grad()\n",
    "            #print(\"output\", output, \"shape\", output.shape)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_loss = sum_loss + loss.item()\n",
    "            \n",
    "            \"\"\"#print(\"LOSS\",loss.item())\n",
    "            with torch.no_grad():\n",
    "                for p in model.parameters():\n",
    "                    p -= eta*p.grad\n",
    "                    #print(\"grads\", p.grad)\"\"\"\n",
    "        print(e, sum_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors(model, test_input, target):\n",
    "    nb_errors = 0\n",
    "    mini_batch_size = 100\n",
    "    \n",
    "    for b in range(0, test_input.size(0), mini_batch_size):\n",
    "        output = model(test_input.narrow(0, b, mini_batch_size).reshape(mini_batch_size, 1, -1))\n",
    "        _, predicted_class = output.max(2)\n",
    "        #print(output)\n",
    "        #print(predicted_class, output, target)\n",
    "        #print(\"pred classes\",predicted_class.shape, \"output\", output.shape, \"target\", target.shape)\n",
    "        for k in range(mini_batch_size):\n",
    "            \n",
    "            if target[b + k, predicted_class[k]] <= 0:\n",
    "                nb_errors = nb_errors + 1\n",
    "\n",
    "    return nb_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 8.402923345565796\n",
      "1 5.945102274417877\n",
      "2 4.912848860025406\n",
      "3 3.8082814812660217\n",
      "4 3.276098310947418\n",
      "5 2.627827078104019\n",
      "6 2.5889915376901627\n",
      "7 2.188942566514015\n",
      "8 2.0065199583768845\n",
      "9 1.7439365535974503\n",
      "10 1.7917822822928429\n",
      "11 1.6822655722498894\n",
      "12 1.8172352313995361\n",
      "13 2.0755162686109543\n",
      "14 1.6414236351847649\n",
      "15 2.259162649512291\n",
      "16 2.626695439219475\n",
      "17 2.4141112118959427\n",
      "18 2.0306322127580643\n",
      "19 1.6831796914339066\n",
      "20 1.6199790760874748\n",
      "21 1.7704265266656876\n",
      "22 1.1476543918251991\n",
      "23 0.842155896127224\n",
      "24 0.5380828846246004\n",
      "STD : -1\n",
      "train error 1.80%% 18/1000\n",
      "test error 22.50%% 225/1000\n",
      "0 7.201122462749481\n",
      "1 6.239093840122223\n",
      "2 5.026223450899124\n",
      "3 4.59284970164299\n",
      "4 4.002883940935135\n",
      "5 3.7058696150779724\n",
      "6 3.3506076335906982\n",
      "7 3.188093513250351\n",
      "8 3.497215837240219\n",
      "9 2.542859375476837\n",
      "10 2.392606094479561\n",
      "11 2.1573038697242737\n",
      "12 2.062415897846222\n",
      "13 2.746184542775154\n",
      "14 2.046277403831482\n",
      "15 1.7751050144433975\n",
      "16 1.9736983999609947\n",
      "17 2.1922746300697327\n",
      "18 3.6917563676834106\n",
      "19 3.6869016885757446\n",
      "20 2.6921729296445847\n",
      "21 1.7594616785645485\n",
      "22 1.456906870007515\n",
      "23 1.3390856757760048\n",
      "24 1.2146606631577015\n",
      "STD : 0.001\n",
      "train error 5.10%% 51/1000\n",
      "test error 22.60%% 226/1000\n",
      "0 7.419878304004669\n",
      "1 6.632640480995178\n",
      "2 5.945247709751129\n",
      "3 4.910058617591858\n",
      "4 4.223276525735855\n",
      "5 3.7881183326244354\n",
      "6 2.995766907930374\n",
      "7 2.7600638270378113\n",
      "8 2.1492481902241707\n",
      "9 2.1618339717388153\n",
      "10 2.05557332187891\n",
      "11 1.7310139536857605\n",
      "12 1.5682612359523773\n",
      "13 1.3764278069138527\n",
      "14 0.8532690405845642\n",
      "15 1.0897959396243095\n",
      "16 0.7029506377875805\n",
      "17 0.7823489755392075\n",
      "18 1.4893545806407928\n",
      "19 1.8599653020501137\n",
      "20 2.156385228037834\n",
      "21 2.431201606988907\n",
      "22 2.635001301765442\n",
      "23 1.697884477674961\n",
      "24 1.0993940718472004\n",
      "STD : 0.01\n",
      "train error 4.00%% 40/1000\n",
      "test error 24.60%% 246/1000\n",
      "0 43.15428924560547\n",
      "1 5.352957844734192\n",
      "2 4.568034797906876\n",
      "3 3.4191856682300568\n",
      "4 2.479163646697998\n",
      "5 2.1945039331912994\n",
      "6 1.734291099011898\n",
      "7 1.9345482513308525\n",
      "8 1.4700399860739708\n",
      "9 1.0054936110973358\n",
      "10 0.41245434433221817\n",
      "11 0.24445965909399092\n",
      "12 0.9298039465211332\n",
      "13 0.8190623000264168\n",
      "14 0.8708255346864462\n",
      "15 0.8665009485557675\n",
      "16 1.3941896446049213\n",
      "17 1.6609183475375175\n",
      "18 2.3565976843237877\n",
      "19 2.664524868130684\n",
      "20 1.720094084739685\n",
      "21 0.8544438518583775\n",
      "22 0.3384954798966646\n",
      "23 0.17673290497623384\n",
      "24 0.04340523498831317\n",
      "STD : 0.1\n",
      "train error 0.00%% 0/1000\n",
      "test error 21.00%% 210/1000\n",
      "0 126531.99462890625\n",
      "1 31374.43896484375\n",
      "2 15413.065734863281\n",
      "3 7031.679107666016\n",
      "4 4225.958541870117\n",
      "5 1771.9961776733398\n",
      "6 605.3764009475708\n",
      "7 115.22475236654282\n",
      "8 21.499631762504578\n",
      "9 8.116794574228322\n",
      "10 0.0\n",
      "11 0.32017600536346436\n",
      "12 0.0\n",
      "13 0.0\n",
      "14 0.0005151344812475145\n",
      "15 0.0\n",
      "16 0.0\n",
      "17 0.0\n",
      "18 0.0\n",
      "19 0.0\n",
      "20 0.0\n",
      "21 0.0\n",
      "22 0.0\n",
      "23 0.0\n",
      "24 0.0\n",
      "STD : 1.0\n",
      "train error 0.00%% 0/1000\n",
      "test error 20.80%% 208/1000\n",
      "0 728290316.0\n",
      "1 387081064.0\n",
      "2 251002886.0\n",
      "3 198607134.0\n",
      "4 146443607.0\n",
      "5 118084261.0\n",
      "6 91383429.0\n",
      "7 72080784.5\n",
      "8 57828148.0\n",
      "9 44841810.25\n",
      "10 36387711.25\n",
      "11 32018675.25\n",
      "12 28698018.875\n",
      "13 25434984.71875\n",
      "14 25663801.25\n",
      "15 16163365.96875\n",
      "16 14876766.875\n",
      "17 6835411.296875\n",
      "18 4764786.1953125\n",
      "19 3205353.0244140625\n",
      "20 2363006.7788085938\n",
      "21 1718056.832397461\n",
      "22 1057022.9228515625\n",
      "23 554032.1201171875\n",
      "24 469877.845703125\n",
      "STD : 10.0\n",
      "train error 0.20%% 2/1000\n",
      "test error 26.60%% 266/1000\n"
     ]
    }
   ],
   "source": [
    "##########################bad initialization (gaussian)\n",
    "lr = 0.01\n",
    "\n",
    "for std in [ -1, 1e-3, 1e-2, 1e-1, 1e-0, 1e1 ]:\n",
    "    model = create_shallow_model()\n",
    "    if std > 0:\n",
    "        with torch.no_grad():\n",
    "            for p in model.parameters(): p.normal_(0, std)\n",
    "    train_model(model, train_input, new_train_target, lr)\n",
    "    nb_train_errors = compute_nb_errors(model, train_input, new_train_target)\n",
    "    nb_test_errors = compute_nb_errors(model, test_input, new_test_target)\n",
    "    print(\"STD :\", std)\n",
    "    print('train error {:0.2f}%% {:d}/{:d}'.format((100 * nb_train_errors) / train_input.size(0),\n",
    "                                                       nb_train_errors, train_input.size(0)))\n",
    "\n",
    "    print('test error {:0.2f}%% {:d}/{:d}'.format((100 * nb_test_errors) / test_input.size(0),\n",
    "                                                       nb_test_errors, test_input.size(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 6.625135898590088\n",
      "1 4.947438418865204\n",
      "2 4.169629544019699\n",
      "3 3.2450728565454483\n",
      "4 2.4105163365602493\n",
      "5 1.6404950469732285\n",
      "6 1.7230835482478142\n",
      "7 0.69340805336833\n",
      "8 0.7366314390674233\n",
      "9 0.45100995246320963\n",
      "10 0.8801031894981861\n",
      "11 2.79973151255399\n",
      "12 3.566630057990551\n",
      "13 2.035114750266075\n",
      "14 1.0744341686367989\n",
      "15 0.5557735962793231\n",
      "16 0.1700974451377988\n",
      "17 0.059785716934129596\n",
      "18 0.017462995601817966\n",
      "19 0.008908532268833369\n",
      "20 0.005080530099803582\n",
      "21 0.0033747516135917976\n",
      "22 0.0023519350434071384\n",
      "23 0.001761031620844733\n",
      "24 0.0013520521897589788\n",
      "Learning rate : 0.001\n",
      "train error 0.00%% 0/1000\n",
      "test error 18.50%% 185/1000\n",
      "0 5.998960167169571\n",
      "1 5.243988454341888\n",
      "2 4.012353897094727\n",
      "3 2.954407900571823\n",
      "4 2.285946637392044\n",
      "5 2.193681515753269\n",
      "6 1.6064449548721313\n",
      "7 0.9908334277570248\n",
      "8 0.8561000153422356\n",
      "9 1.0393062457442284\n",
      "10 0.4872272238135338\n",
      "11 0.3219771031290293\n",
      "12 0.894184130243957\n",
      "13 0.5746839493513107\n",
      "14 1.2253377921879292\n",
      "15 0.664965683594346\n",
      "16 1.2762038130313158\n",
      "17 1.44581975415349\n",
      "18 2.220449436455965\n",
      "19 1.9002228900790215\n",
      "20 1.3532409965991974\n",
      "21 0.39618744142353535\n",
      "22 0.320828290306963\n",
      "23 0.34793152287602425\n",
      "24 0.13829619984608144\n",
      "Learning rate : 0.005\n",
      "train error 0.10%% 1/1000\n",
      "test error 19.20%% 192/1000\n",
      "0 7.960552871227264\n",
      "1 5.696180820465088\n",
      "2 4.716167837381363\n",
      "3 3.8501486778259277\n",
      "4 3.024627834558487\n",
      "5 2.4510267078876495\n",
      "6 2.1055017560720444\n",
      "7 1.7791092917323112\n",
      "8 1.5748028755187988\n",
      "9 1.513612523674965\n",
      "10 1.273576781153679\n",
      "11 1.0306072607636452\n",
      "12 1.2536228261888027\n",
      "13 1.1471668370068073\n",
      "14 1.1402612701058388\n",
      "15 1.5659250020980835\n",
      "16 2.8468419164419174\n",
      "17 3.038263365626335\n",
      "18 2.2986334413290024\n",
      "19 1.9561820775270462\n",
      "20 1.2670832723379135\n",
      "21 0.701306788250804\n",
      "22 0.40368402656167746\n",
      "23 0.30064050666987896\n",
      "24 0.38727552350610495\n",
      "Learning rate : 0.01\n",
      "train error 0.90%% 9/1000\n",
      "test error 21.30%% 213/1000\n",
      "0 578.50680321455\n",
      "1 7.130636930465698\n",
      "2 5.42759895324707\n",
      "3 5.244649022817612\n",
      "4 4.4577643275260925\n",
      "5 3.9980862736701965\n",
      "6 3.7493989765644073\n",
      "7 3.67135152220726\n",
      "8 3.668671727180481\n",
      "9 3.391621857881546\n",
      "10 3.581041470170021\n",
      "11 3.398286134004593\n",
      "12 3.890621840953827\n",
      "13 2.9387836009263992\n",
      "14 3.097500264644623\n",
      "15 3.113829106092453\n",
      "16 3.4808219522237778\n",
      "17 4.132320433855057\n",
      "18 4.3724848330020905\n",
      "19 3.917504698038101\n",
      "20 3.244714140892029\n",
      "21 3.0075883716344833\n",
      "22 2.7896141707897186\n",
      "23 2.724770501255989\n",
      "24 2.6458806842565536\n",
      "Learning rate : 0.05\n",
      "train error 16.90%% 169/1000\n",
      "test error 27.80%% 278/1000\n",
      "0 7886.726310968399\n",
      "1 49.668293952941895\n",
      "2 24.618989646434784\n",
      "3 10.233768999576569\n",
      "4 6.776614010334015\n",
      "5 5.50397327542305\n",
      "6 5.624303549528122\n",
      "7 5.096102863550186\n",
      "8 4.722803682088852\n",
      "9 4.608406901359558\n",
      "10 4.397765010595322\n",
      "11 4.224338948726654\n",
      "12 4.119752764701843\n",
      "13 4.008996546268463\n",
      "14 3.9011518955230713\n",
      "15 3.7902202904224396\n",
      "16 3.6755444705486298\n",
      "17 3.5813048481941223\n",
      "18 3.5147430300712585\n",
      "19 3.4503636360168457\n",
      "20 3.4468411952257156\n",
      "21 3.408456429839134\n",
      "22 3.4497267603874207\n",
      "23 3.88876873254776\n",
      "24 4.266611039638519\n",
      "Learning rate : 0.1\n",
      "train error 16.10%% 161/1000\n",
      "test error 25.50%% 255/1000\n"
     ]
    }
   ],
   "source": [
    "# good result : lr = 0.01 : no overfitting, good test error ~20%\n",
    "\n",
    "################good initialization (automatic from pytorch)\n",
    "\n",
    "for lr in [0.001,0.005, 0.01, 0.05, 0.1]:\n",
    "    model = create_shallow_model()\n",
    "    train_model(model, train_input, new_train_target, lr)\n",
    "    nb_train_errors = compute_nb_errors(model, train_input, new_train_target)\n",
    "    nb_test_errors = compute_nb_errors(model, test_input, new_test_target)\n",
    "    print(\"Learning rate :\", lr)\n",
    "    print('train error {:0.2f}%% {:d}/{:d}'.format((100 * nb_train_errors) / train_input.size(0),\n",
    "                                                       nb_train_errors, train_input.size(0)))\n",
    "\n",
    "    print('test error {:0.2f}%% {:d}/{:d}'.format((100 * nb_test_errors) / test_input.size(0),\n",
    "                                                       nb_test_errors, test_input.size(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_penalties(model, train_input, train_target,lr, L1, value): #L1 = 1 if L1 // =0 if L2\n",
    "    epoch = 25\n",
    "    eta = 0.2\n",
    "    mini_batches = 100\n",
    "    optimizer = optim.Adam(model.parameters(), lr)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    for e in range(epoch):\n",
    "        sum_loss = 0\n",
    "        \n",
    "        for b in range(0, train_input.size(0), mini_batches):\n",
    "            \n",
    "            output = model(train_input.narrow(0, b, mini_batches).reshape(mini_batches, 1, -1))\n",
    "            loss = criterion(output.squeeze(1), train_target.narrow(0, b, mini_batches))\n",
    "            \n",
    "            if L1 == 1:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                with torch.no_grad():\n",
    "                    for p in model.parameters():\n",
    "                        p.sub_(p.sign() * p.abs().clamp(max = value))\n",
    "\n",
    "            else:\n",
    "                for p in model.parameters():\n",
    "                    sum_loss += value * p.pow(2).sum()\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        if not L1:\n",
    "            print(e, sum_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(8.1133, grad_fn=<AddBackward0>)\n",
      "1 tensor(15.2108, grad_fn=<AddBackward0>)\n",
      "2 tensor(19.3217, grad_fn=<AddBackward0>)\n",
      "3 tensor(21.3958, grad_fn=<AddBackward0>)\n",
      "4 tensor(22.6093, grad_fn=<AddBackward0>)\n",
      "5 tensor(23.3564, grad_fn=<AddBackward0>)\n",
      "6 tensor(24.0225, grad_fn=<AddBackward0>)\n",
      "7 tensor(24.6368, grad_fn=<AddBackward0>)\n",
      "8 tensor(25.1696, grad_fn=<AddBackward0>)\n",
      "9 tensor(25.4054, grad_fn=<AddBackward0>)\n",
      "10 tensor(26.0577, grad_fn=<AddBackward0>)\n",
      "11 tensor(26.9290, grad_fn=<AddBackward0>)\n",
      "12 tensor(27.5305, grad_fn=<AddBackward0>)\n",
      "13 tensor(27.8460, grad_fn=<AddBackward0>)\n",
      "14 tensor(28.0629, grad_fn=<AddBackward0>)\n",
      "15 tensor(28.3894, grad_fn=<AddBackward0>)\n",
      "16 tensor(28.7183, grad_fn=<AddBackward0>)\n",
      "17 tensor(28.9581, grad_fn=<AddBackward0>)\n",
      "18 tensor(29.4380, grad_fn=<AddBackward0>)\n",
      "19 tensor(30.0903, grad_fn=<AddBackward0>)\n",
      "20 tensor(30.6240, grad_fn=<AddBackward0>)\n",
      "21 tensor(31.0830, grad_fn=<AddBackward0>)\n",
      "22 tensor(31.7106, grad_fn=<AddBackward0>)\n",
      "23 tensor(32.4900, grad_fn=<AddBackward0>)\n",
      "24 tensor(33.3167, grad_fn=<AddBackward0>)\n",
      "L2 : 0.001\n",
      "train error 0.60%% 6/1000\n",
      "test error 21.00%% 210/1000\n",
      "0 tensor(16.3119, grad_fn=<AddBackward0>)\n",
      "1 tensor(30.5967, grad_fn=<AddBackward0>)\n",
      "2 tensor(38.7454, grad_fn=<AddBackward0>)\n",
      "3 tensor(42.8015, grad_fn=<AddBackward0>)\n",
      "4 tensor(45.5178, grad_fn=<AddBackward0>)\n",
      "5 tensor(47.4836, grad_fn=<AddBackward0>)\n",
      "6 tensor(49.0257, grad_fn=<AddBackward0>)\n",
      "7 tensor(49.9878, grad_fn=<AddBackward0>)\n",
      "8 tensor(51.3073, grad_fn=<AddBackward0>)\n",
      "9 tensor(52.9512, grad_fn=<AddBackward0>)\n",
      "10 tensor(54.8015, grad_fn=<AddBackward0>)\n",
      "11 tensor(56.1551, grad_fn=<AddBackward0>)\n",
      "12 tensor(56.6787, grad_fn=<AddBackward0>)\n",
      "13 tensor(57.5970, grad_fn=<AddBackward0>)\n",
      "14 tensor(58.4533, grad_fn=<AddBackward0>)\n",
      "15 tensor(59.4169, grad_fn=<AddBackward0>)\n",
      "16 tensor(60.6971, grad_fn=<AddBackward0>)\n",
      "17 tensor(61.9632, grad_fn=<AddBackward0>)\n",
      "18 tensor(63.6455, grad_fn=<AddBackward0>)\n",
      "19 tensor(64.7869, grad_fn=<AddBackward0>)\n",
      "20 tensor(66.2355, grad_fn=<AddBackward0>)\n",
      "21 tensor(67.5265, grad_fn=<AddBackward0>)\n",
      "22 tensor(68.5295, grad_fn=<AddBackward0>)\n",
      "23 tensor(69.5704, grad_fn=<AddBackward0>)\n",
      "24 tensor(70.8472, grad_fn=<AddBackward0>)\n",
      "L2 : 0.002\n",
      "train error 0.70%% 7/1000\n",
      "test error 19.60%% 196/1000\n",
      "0 tensor(32.8200, grad_fn=<AddBackward0>)\n",
      "1 tensor(61.5310, grad_fn=<AddBackward0>)\n",
      "2 tensor(78.2598, grad_fn=<AddBackward0>)\n",
      "3 tensor(86.9144, grad_fn=<AddBackward0>)\n",
      "4 tensor(91.9634, grad_fn=<AddBackward0>)\n",
      "5 tensor(95.3558, grad_fn=<AddBackward0>)\n",
      "6 tensor(98.1914, grad_fn=<AddBackward0>)\n",
      "7 tensor(101.3691, grad_fn=<AddBackward0>)\n",
      "8 tensor(104.0831, grad_fn=<AddBackward0>)\n",
      "9 tensor(106.8310, grad_fn=<AddBackward0>)\n",
      "10 tensor(108.8376, grad_fn=<AddBackward0>)\n",
      "11 tensor(110.6974, grad_fn=<AddBackward0>)\n",
      "12 tensor(113.5367, grad_fn=<AddBackward0>)\n",
      "13 tensor(116.1957, grad_fn=<AddBackward0>)\n",
      "14 tensor(118.4381, grad_fn=<AddBackward0>)\n",
      "15 tensor(121.7019, grad_fn=<AddBackward0>)\n",
      "16 tensor(126.7511, grad_fn=<AddBackward0>)\n",
      "17 tensor(131.2670, grad_fn=<AddBackward0>)\n",
      "18 tensor(134.2635, grad_fn=<AddBackward0>)\n",
      "19 tensor(136.5780, grad_fn=<AddBackward0>)\n",
      "20 tensor(139.9918, grad_fn=<AddBackward0>)\n",
      "21 tensor(143.2057, grad_fn=<AddBackward0>)\n",
      "22 tensor(145.7173, grad_fn=<AddBackward0>)\n",
      "23 tensor(148.0023, grad_fn=<AddBackward0>)\n",
      "24 tensor(149.6962, grad_fn=<AddBackward0>)\n",
      "L2 : 0.004\n",
      "train error 0.60%% 6/1000\n",
      "test error 20.30%% 203/1000\n",
      "0 tensor(40.6541, grad_fn=<AddBackward0>)\n",
      "1 tensor(75.8366, grad_fn=<AddBackward0>)\n",
      "2 tensor(95.7078, grad_fn=<AddBackward0>)\n",
      "3 tensor(105.8374, grad_fn=<AddBackward0>)\n",
      "4 tensor(111.4445, grad_fn=<AddBackward0>)\n",
      "5 tensor(115.2562, grad_fn=<AddBackward0>)\n",
      "6 tensor(118.2868, grad_fn=<AddBackward0>)\n",
      "7 tensor(119.8753, grad_fn=<AddBackward0>)\n",
      "8 tensor(123.6394, grad_fn=<AddBackward0>)\n",
      "9 tensor(127.1158, grad_fn=<AddBackward0>)\n",
      "10 tensor(130.5642, grad_fn=<AddBackward0>)\n",
      "11 tensor(134.5545, grad_fn=<AddBackward0>)\n",
      "12 tensor(138.0984, grad_fn=<AddBackward0>)\n",
      "13 tensor(140.8008, grad_fn=<AddBackward0>)\n",
      "14 tensor(142.0048, grad_fn=<AddBackward0>)\n",
      "15 tensor(142.8417, grad_fn=<AddBackward0>)\n",
      "16 tensor(144.2193, grad_fn=<AddBackward0>)\n",
      "17 tensor(146.8534, grad_fn=<AddBackward0>)\n",
      "18 tensor(148.3970, grad_fn=<AddBackward0>)\n",
      "19 tensor(149.7676, grad_fn=<AddBackward0>)\n",
      "20 tensor(154.0170, grad_fn=<AddBackward0>)\n",
      "21 tensor(159.7301, grad_fn=<AddBackward0>)\n",
      "22 tensor(163.1393, grad_fn=<AddBackward0>)\n",
      "23 tensor(164.3598, grad_fn=<AddBackward0>)\n",
      "24 tensor(165.5594, grad_fn=<AddBackward0>)\n",
      "L2 : 0.005\n",
      "train error 1.60%% 16/1000\n",
      "test error 23.70%% 237/1000\n",
      "0 tensor(81.4420, grad_fn=<AddBackward0>)\n",
      "1 tensor(150.3838, grad_fn=<AddBackward0>)\n",
      "2 tensor(190.6517, grad_fn=<AddBackward0>)\n",
      "3 tensor(211.5069, grad_fn=<AddBackward0>)\n",
      "4 tensor(224.5228, grad_fn=<AddBackward0>)\n",
      "5 tensor(233.5732, grad_fn=<AddBackward0>)\n",
      "6 tensor(242.2029, grad_fn=<AddBackward0>)\n",
      "7 tensor(249.6685, grad_fn=<AddBackward0>)\n",
      "8 tensor(255.5493, grad_fn=<AddBackward0>)\n",
      "9 tensor(259.5420, grad_fn=<AddBackward0>)\n",
      "10 tensor(262.1826, grad_fn=<AddBackward0>)\n",
      "11 tensor(267.5509, grad_fn=<AddBackward0>)\n",
      "12 tensor(274.8300, grad_fn=<AddBackward0>)\n",
      "13 tensor(282.5257, grad_fn=<AddBackward0>)\n",
      "14 tensor(291.9431, grad_fn=<AddBackward0>)\n",
      "15 tensor(295.7645, grad_fn=<AddBackward0>)\n",
      "16 tensor(299.6993, grad_fn=<AddBackward0>)\n",
      "17 tensor(308.9440, grad_fn=<AddBackward0>)\n",
      "18 tensor(313.3019, grad_fn=<AddBackward0>)\n",
      "19 tensor(317.1669, grad_fn=<AddBackward0>)\n",
      "20 tensor(324.9103, grad_fn=<AddBackward0>)\n",
      "21 tensor(333.3659, grad_fn=<AddBackward0>)\n",
      "22 tensor(339.2951, grad_fn=<AddBackward0>)\n",
      "23 tensor(343.7666, grad_fn=<AddBackward0>)\n",
      "24 tensor(346.2012, grad_fn=<AddBackward0>)\n",
      "L2 : 0.01\n",
      "train error 2.00%% 20/1000\n",
      "test error 21.50%% 215/1000\n",
      "0 tensor(163.2256, grad_fn=<AddBackward0>)\n",
      "1 tensor(307.2095, grad_fn=<AddBackward0>)\n",
      "2 tensor(389.1869, grad_fn=<AddBackward0>)\n",
      "3 tensor(430.9388, grad_fn=<AddBackward0>)\n",
      "4 tensor(458.5741, grad_fn=<AddBackward0>)\n",
      "5 tensor(476.1405, grad_fn=<AddBackward0>)\n",
      "6 tensor(491.0103, grad_fn=<AddBackward0>)\n",
      "7 tensor(504.7296, grad_fn=<AddBackward0>)\n",
      "8 tensor(511.5727, grad_fn=<AddBackward0>)\n",
      "9 tensor(521.9997, grad_fn=<AddBackward0>)\n",
      "10 tensor(532.5090, grad_fn=<AddBackward0>)\n",
      "11 tensor(547.0157, grad_fn=<AddBackward0>)\n",
      "12 tensor(558.4100, grad_fn=<AddBackward0>)\n",
      "13 tensor(568.0041, grad_fn=<AddBackward0>)\n",
      "14 tensor(583.5219, grad_fn=<AddBackward0>)\n",
      "15 tensor(598.3835, grad_fn=<AddBackward0>)\n",
      "16 tensor(607.9968, grad_fn=<AddBackward0>)\n",
      "17 tensor(612.9597, grad_fn=<AddBackward0>)\n",
      "18 tensor(623.2249, grad_fn=<AddBackward0>)\n",
      "19 tensor(636.4119, grad_fn=<AddBackward0>)\n",
      "20 tensor(646.8217, grad_fn=<AddBackward0>)\n",
      "21 tensor(660.7362, grad_fn=<AddBackward0>)\n",
      "22 tensor(674.0718, grad_fn=<AddBackward0>)\n",
      "23 tensor(681.5302, grad_fn=<AddBackward0>)\n",
      "24 tensor(685.3208, grad_fn=<AddBackward0>)\n",
      "L2 : 0.02\n",
      "train error 3.90%% 39/1000\n",
      "test error 22.70%% 227/1000\n"
     ]
    }
   ],
   "source": [
    "########L2 penalty\n",
    "lr = 0.01\n",
    "for lambda_l2 in [0.001, 0.002, 0.004, 0.005, 0.010, 0.020]:\n",
    "    model = create_shallow_model()\n",
    "    train_model_penalties(model, train_input, new_train_target, lr, 0, lambda_l2)\n",
    "    nb_train_errors = compute_nb_errors(model, train_input, new_train_target)\n",
    "    nb_test_errors = compute_nb_errors(model, test_input, new_test_target)\n",
    "    print(\"L2 :\", lambda_l2)\n",
    "    print('train error {:0.2f}%% {:d}/{:d}'.format((100 * nb_train_errors) / train_input.size(0),\n",
    "                                                       nb_train_errors, train_input.size(0)))\n",
    "\n",
    "    print('test error {:0.2f}%% {:d}/{:d}'.format((100 * nb_test_errors) / test_input.size(0),\n",
    "                                                       nb_test_errors, test_input.size(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 : 0.001\n",
      "train error 5.50%% 55/1000\n",
      "test error 24.10%% 241/1000\n",
      "L1 : 0.002\n",
      "train error 7.30%% 73/1000\n",
      "test error 20.80%% 208/1000\n",
      "L1 : 0.004\n",
      "train error 8.80%% 88/1000\n",
      "test error 21.70%% 217/1000\n",
      "L1 : 0.005\n",
      "train error 10.00%% 100/1000\n",
      "test error 23.20%% 232/1000\n",
      "L1 : 0.01\n",
      "train error 44.90%% 449/1000\n",
      "test error 47.40%% 474/1000\n",
      "L1 : 0.02\n",
      "train error 44.90%% 449/1000\n",
      "test error 47.40%% 474/1000\n"
     ]
    }
   ],
   "source": [
    "########L1 penalty\n",
    "lr = 0.01\n",
    "for lambda_l1 in [0.001, 0.002, 0.004, 0.005, 0.010, 0.020]:\n",
    "    model = create_shallow_model()\n",
    "    train_model_penalties(model, train_input, new_train_target, lr, 1, lambda_l1)\n",
    "    nb_train_errors = compute_nb_errors(model, train_input, new_train_target)\n",
    "    nb_test_errors = compute_nb_errors(model, test_input, new_test_target)\n",
    "    print(\"L1 :\", lambda_l1)\n",
    "    print('train error {:0.2f}%% {:d}/{:d}'.format((100 * nb_train_errors) / train_input.size(0),\n",
    "                                                       nb_train_errors, train_input.size(0)))\n",
    "\n",
    "    print('test error {:0.2f}%% {:d}/{:d}'.format((100 * nb_test_errors) / test_input.size(0),\n",
    "                                                       nb_test_errors, test_input.size(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
