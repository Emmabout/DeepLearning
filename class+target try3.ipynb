{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "import dlc_practical_prologue as prologue\n",
    "\n",
    "mini_batch_size = 100\n",
    "N = 1000\n",
    "train_input, train_target, train_classes, test_input, test_target, test_classes = prologue.generate_pair_sets(N)\n",
    "\n",
    "#normalize the input\n",
    "train_input/=255\n",
    "test_input/=255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_shapes_Net = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Siamese_net_auxiliary(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Siamese_net_auxiliary, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(256, 10)\n",
    "        self.fc2 = nn.Linear(20, 2)\n",
    "\n",
    "    def forward(self, data):\n",
    "        if print_shapes_Net:\n",
    "            print(\"initial\", data.shape) #100 2 14 14\n",
    "            \n",
    "        class_layer = []\n",
    "        final_layer = []\n",
    "        for i in range(2):\n",
    "            x = data[:,i,:,:]\n",
    "            len0 = x.shape[0]\n",
    "            x = torch.reshape(x, (len0, 1, 14, 14))\n",
    "            \n",
    "            if print_shapes_Net:\n",
    "                print(\"X START\",x.shape) \n",
    "            \n",
    "            x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2, stride=2))\n",
    "            if print_shapes_Net:\n",
    "                print(\"conv1\",x.shape) \n",
    "                \n",
    "            x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2))\n",
    "            if print_shapes_Net:\n",
    "                print(\"conv2\",x.shape)\n",
    "            \n",
    "            x = F.relu(self.fc1(x.view(-1, 256)))\n",
    "            if print_shapes_Net:\n",
    "                print(\"fc1\",x.shape) \n",
    "                \n",
    "            final_layer.append(x)\n",
    "            class_layer.append(x.reshape(x.shape[0], 1, 10))\n",
    "            \n",
    "        final_layer = torch.cat((final_layer[1], final_layer[0]), 1)\n",
    "        class_layer = torch.cat((class_layer[1], class_layer[0]), 1)\n",
    "        \n",
    "        if print_shapes_Net:\n",
    "                print(\"class layer\",class_layer.shape)\n",
    "                \n",
    "        final_layer = self.fc2(final_layer)\n",
    "        if print_shapes_Net:\n",
    "            print(\"final\",final_layer.shape) \n",
    "            \n",
    "        return class_layer, final_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_input, train_classes, train_target, mini_batch_size, lr, nb_epoch):\n",
    "    criterion = nn.MSELoss() #CHANGE LOSS TO CROSS ENTROPY?\n",
    "    optimizer = optim.Adam(model.parameters(), lr)\n",
    "    \n",
    "    for e in range(nb_epoch):\n",
    "        sum_loss = 0\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "          \n",
    "            output_class, output_target = model(train_input.narrow(0, b, mini_batch_size))\n",
    "            \n",
    "            loss_class = criterion(output_class, train_classes.narrow(0, b, mini_batch_size))\n",
    "            loss_target = criterion(output_target, train_target.narrow(0, b, mini_batch_size))\n",
    "            model.zero_grad()\n",
    "            loss = loss_class + loss_target\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            sum_loss = sum_loss + loss.item()\n",
    "        #print(e, sum_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors_targets(model, input, target):\n",
    "    nb_errors = 0\n",
    "    _, output = model(input)\n",
    "    _, predicted_target = output.max(1) #max probabilities of target\n",
    "    \n",
    "    for b in range(1000):\n",
    "        if target[b,int(predicted_target[b])] <= 0:\n",
    "            nb_errors = nb_errors + 1\n",
    "            \n",
    "    return nb_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors_classes(model, input, target):\n",
    "    nb_errors = 0\n",
    "\n",
    "    output,_ = model(input)\n",
    "    _, predicted_classes = output.max(2)\n",
    "\n",
    "    for b in range(input.shape[0]):\n",
    "        if target[b][0][predicted_classes[b][0]] <= 0:\n",
    "            nb_errors = nb_errors + 1\n",
    "        if target[b][1][predicted_classes[b][1]] <= 0:\n",
    "            nb_errors = nb_errors + 1\n",
    "\n",
    "    return nb_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_target[1000,1]\n",
    "new_train_target = torch.empty(1000,2)\n",
    "new_test_target = torch.empty(1000,2)\n",
    "for i in range(1000):\n",
    "    if train_target[i] == 1 :\n",
    "        new_train_target[i,0] = 0\n",
    "        new_train_target[i,1] = 1\n",
    "        \n",
    "    else:\n",
    "        new_train_target[i,0] = 1\n",
    "        new_train_target[i,1] = 0\n",
    "        \n",
    "    if test_target[i] == 1:\n",
    "        new_test_target[i,0] = 0\n",
    "        new_test_target[i,1] = 1\n",
    "        \n",
    "    else:\n",
    "        new_test_target[i,0] = 1\n",
    "        new_test_target[i,1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_classes[1000, 2]\n",
    "new_train_classes = torch.zeros(1000, 2, 10)\n",
    "new_test_classes = torch.zeros(1000, 2, 10)\n",
    "\n",
    "for i in range(train_classes.shape[0]): #\n",
    "    new_train_classes[i][0][train_classes[i][0]] = 1\n",
    "    new_train_classes[i][1][train_classes[i][1]] = 1\n",
    "\n",
    "for i in range(test_classes.shape[0]):\n",
    "    new_test_classes[i][0][test_classes[i][0]] = 1\n",
    "    new_test_classes[i][1][test_classes[i][1]] = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.405638158321381\n",
      "1 3.6883074045181274\n",
      "2 3.4818137884140015\n",
      "3 3.273284137248993\n",
      "4 3.0642834901809692\n",
      "5 2.874796450138092\n",
      "6 2.6600130647420883\n",
      "7 2.4488677084445953\n",
      "8 2.2773241251707077\n",
      "9 2.128381237387657\n",
      "10 2.0064972639083862\n",
      "11 1.900394544005394\n",
      "12 1.8019713461399078\n",
      "13 1.7061333060264587\n",
      "14 1.630494847893715\n",
      "15 1.562531054019928\n",
      "16 1.4981640130281448\n",
      "17 1.4396350532770157\n",
      "18 1.4116232693195343\n",
      "19 1.4159819334745407\n",
      "20 1.5226670801639557\n",
      "21 1.4392471015453339\n",
      "22 1.3633665442466736\n",
      "23 1.3004753962159157\n",
      "24 1.267670951783657\n",
      "train error Net_auxiliary_loss 0.10% 1/1000\n",
      "test error Net_auxiliary_loss 20.60% 206/1000\n"
     ]
    }
   ],
   "source": [
    "####predict class of each digit\n",
    "for k in range(1):\n",
    "    model = Siamese_net_auxiliary()\n",
    "    lr = 0.005\n",
    "    nb_epoch = 25\n",
    "    train_model(model, train_input, new_train_classes, new_train_target, mini_batch_size, lr, nb_epoch)\n",
    "    \n",
    "    nb_train_errors = compute_nb_errors_targets(model, train_input, new_train_target)\n",
    "    print('train error Net_auxiliary_loss {:0.2f}% {:d}/{:d}'.format((100 * nb_train_errors) / train_input.size(0),\n",
    "                                                      nb_train_errors, train_input.size(0)))\n",
    "    nb_test_errors = compute_nb_errors_targets(model, test_input, new_test_target)\n",
    "    print('test error Net_auxiliary_loss {:0.2f}% {:d}/{:d}'.format((100 * nb_test_errors) / test_input.size(0),\n",
    "                                                    nb_test_errors, test_input.size(0)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train error Net_auxiliary_loss 179.10% 1791/1000\n",
      "test error Net_auxiliary_loss 179.50% 1795/1000\n"
     ]
    }
   ],
   "source": [
    "nb_train_errors_class = compute_nb_errors_classes(model, train_input, new_train_classes)\n",
    "print('train error Net_auxiliary_loss {:0.2f}% {:d}/{:d}'.format((100 * nb_train_errors_class) / train_input.size(0),\n",
    "                                                  nb_train_errors_class, train_input.size(0)))\n",
    "nb_test_errors_class = compute_nb_errors_classes(model, test_input, new_test_classes)\n",
    "print('test error Net_auxiliary_loss {:0.2f}% {:d}/{:d}'.format((100 * nb_test_errors_class) / test_input.size(0),\n",
    "                                                nb_test_errors_class, test_input.size(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR 0.0005\n",
      "train error Net_auxiliary_loss 9.20% 92/1000\n",
      "test error Net_auxiliary_loss 16.90% 169/1000\n",
      "train error Net_auxiliary_loss 171.80% 1718/1000\n",
      "test error Net_auxiliary_loss 178.70% 1787/1000\n",
      "LR 0.001\n",
      "train error Net_auxiliary_loss 7.00% 70/1000\n",
      "test error Net_auxiliary_loss 19.10% 191/1000\n",
      "train error Net_auxiliary_loss 175.70% 1757/1000\n",
      "test error Net_auxiliary_loss 179.10% 1791/1000\n",
      "LR 0.005\n",
      "train error Net_auxiliary_loss 0.90% 9/1000\n",
      "test error Net_auxiliary_loss 18.70% 187/1000\n",
      "train error Net_auxiliary_loss 180.50% 1805/1000\n",
      "test error Net_auxiliary_loss 181.40% 1814/1000\n",
      "LR 0.01\n",
      "train error Net_auxiliary_loss 0.40% 4/1000\n",
      "test error Net_auxiliary_loss 20.10% 201/1000\n",
      "train error Net_auxiliary_loss 180.00% 1800/1000\n",
      "test error Net_auxiliary_loss 182.30% 1823/1000\n",
      "LR 0.05\n",
      "train error Net_auxiliary_loss 44.90% 449/1000\n",
      "test error Net_auxiliary_loss 47.40% 474/1000\n",
      "train error Net_auxiliary_loss 181.80% 1818/1000\n",
      "test error Net_auxiliary_loss 179.00% 1790/1000\n",
      "LR 0.1\n",
      "train error Net_auxiliary_loss 44.90% 449/1000\n",
      "test error Net_auxiliary_loss 47.40% 474/1000\n",
      "train error Net_auxiliary_loss 180.10% 1801/1000\n",
      "test error Net_auxiliary_loss 178.30% 1783/1000\n",
      "LR 0.5\n",
      "train error Net_auxiliary_loss 55.10% 551/1000\n",
      "test error Net_auxiliary_loss 52.60% 526/1000\n",
      "train error Net_auxiliary_loss 180.40% 1804/1000\n",
      "test error Net_auxiliary_loss 180.70% 1807/1000\n"
     ]
    }
   ],
   "source": [
    "#### lr = 0.0005 seems to not overfit\n",
    "for lr in [0.0005, 0.001,0.005, 0.01, 0.05, 0.1, 0.5]:\n",
    "    model = Siamese_net_auxiliary()\n",
    "    nb_epoch = 25\n",
    "    train_model(model, train_input, new_train_classes, new_train_target, mini_batch_size, lr, nb_epoch)\n",
    "    \n",
    "    print(\"LR\", lr)\n",
    "    nb_train_errors = compute_nb_errors_targets(model, train_input, new_train_target)\n",
    "    print('train error Net_auxiliary_loss {:0.2f}% {:d}/{:d}'.format((100 * nb_train_errors) / train_input.size(0),\n",
    "                                                      nb_train_errors, train_input.size(0)))\n",
    "    nb_test_errors = compute_nb_errors_targets(model, test_input, new_test_target)\n",
    "    print('test error Net_auxiliary_loss {:0.2f}% {:d}/{:d}'.format((100 * nb_test_errors) / test_input.size(0),\n",
    "                                                    nb_test_errors, test_input.size(0)))\n",
    "    \n",
    "    nb_train_errors_class = compute_nb_errors_classes(model, train_input, new_train_classes)\n",
    "    print('train error Net_auxiliary_loss {:0.2f}% {:d}/{:d}'.format((100 * nb_train_errors_class) / train_input.size(0),\n",
    "                                                      nb_train_errors_class, train_input.size(0)))\n",
    "    nb_test_errors_class = compute_nb_errors_classes(model, test_input, new_test_classes)\n",
    "    print('test error Net_auxiliary_loss {:0.2f}% {:d}/{:d}'.format((100 * nb_test_errors_class) / test_input.size(0),\n",
    "                                                    nb_test_errors_class, test_input.size(0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train error Net_auxiliary_loss 16.80% 168/1000\n",
      "test error Net_auxiliary_loss 22.00% 220/1000\n",
      "train error Net_auxiliary_loss 176.80% 1768/1000\n",
      "test error Net_auxiliary_loss 180.00% 1800/1000\n",
      "train error Net_auxiliary_loss 11.70% 117/1000\n",
      "test error Net_auxiliary_loss 19.70% 197/1000\n",
      "train error Net_auxiliary_loss 176.70% 1767/1000\n",
      "test error Net_auxiliary_loss 179.40% 1794/1000\n",
      "train error Net_auxiliary_loss 11.20% 112/1000\n",
      "test error Net_auxiliary_loss 16.40% 164/1000\n",
      "train error Net_auxiliary_loss 177.10% 1771/1000\n",
      "test error Net_auxiliary_loss 180.00% 1800/1000\n",
      "train error Net_auxiliary_loss 14.60% 146/1000\n",
      "test error Net_auxiliary_loss 20.40% 204/1000\n",
      "train error Net_auxiliary_loss 176.50% 1765/1000\n",
      "test error Net_auxiliary_loss 180.50% 1805/1000\n",
      "train error Net_auxiliary_loss 44.90% 449/1000\n",
      "test error Net_auxiliary_loss 47.40% 474/1000\n",
      "train error Net_auxiliary_loss 180.90% 1809/1000\n",
      "test error Net_auxiliary_loss 181.10% 1811/1000\n",
      "train error Net_auxiliary_loss 14.30% 143/1000\n",
      "test error Net_auxiliary_loss 18.40% 184/1000\n",
      "train error Net_auxiliary_loss 179.10% 1791/1000\n",
      "test error Net_auxiliary_loss 180.50% 1805/1000\n",
      "train error Net_auxiliary_loss 20.50% 205/1000\n",
      "test error Net_auxiliary_loss 25.80% 258/1000\n",
      "train error Net_auxiliary_loss 176.00% 1760/1000\n",
      "test error Net_auxiliary_loss 179.20% 1792/1000\n",
      "train error Net_auxiliary_loss 10.00% 100/1000\n",
      "test error Net_auxiliary_loss 17.00% 170/1000\n",
      "train error Net_auxiliary_loss 177.80% 1778/1000\n",
      "test error Net_auxiliary_loss 179.80% 1798/1000\n",
      "train error Net_auxiliary_loss 11.40% 114/1000\n",
      "test error Net_auxiliary_loss 16.90% 169/1000\n",
      "train error Net_auxiliary_loss 175.50% 1755/1000\n",
      "test error Net_auxiliary_loss 181.40% 1814/1000\n",
      "train error Net_auxiliary_loss 14.80% 148/1000\n",
      "test error Net_auxiliary_loss 22.20% 222/1000\n",
      "train error Net_auxiliary_loss 178.80% 1788/1000\n",
      "test error Net_auxiliary_loss 180.00% 1800/1000\n"
     ]
    }
   ],
   "source": [
    "####predict class of each digit\n",
    "for k in range(10):\n",
    "    model = Siamese_net_auxiliary()\n",
    "    lr = 0.0005\n",
    "    nb_epoch = 25\n",
    "    train_model(model, train_input, new_train_classes, new_train_target, mini_batch_size, lr, nb_epoch)\n",
    "    \n",
    "    nb_train_errors = compute_nb_errors_targets(model, train_input, new_train_target)\n",
    "    print('train error Net_auxiliary_loss {:0.2f}% {:d}/{:d}'.format((100 * nb_train_errors) / train_input.size(0),\n",
    "                                                      nb_train_errors, train_input.size(0)))\n",
    "    nb_test_errors = compute_nb_errors_targets(model, test_input, new_test_target)\n",
    "    print('test error Net_auxiliary_loss {:0.2f}% {:d}/{:d}'.format((100 * nb_test_errors) / test_input.size(0),\n",
    "                                                    nb_test_errors, test_input.size(0)))\n",
    "    \n",
    "    nb_train_errors_class = compute_nb_errors_classes(model, train_input, new_train_classes)\n",
    "    print('train error Net_auxiliary_loss {:0.2f}% {:d}/{:d}'.format((100 * nb_train_errors_class) / train_input.size(0),\n",
    "                                                      nb_train_errors_class, train_input.size(0)))\n",
    "    nb_test_errors_class = compute_nb_errors_classes(model, test_input, new_test_classes)\n",
    "    print('test error Net_auxiliary_loss {:0.2f}% {:d}/{:d}'.format((100 * nb_test_errors_class) / test_input.size(0),\n",
    "                                                    nb_test_errors_class, test_input.size(0)))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
