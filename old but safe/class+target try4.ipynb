{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "import dlc_practical_prologue as prologue\n",
    "\n",
    "mini_batch_size = 100\n",
    "N = 1000\n",
    "train_input, train_target, train_classes, test_input, test_target, test_classes = prologue.generate_pair_sets(N)\n",
    "\n",
    "#normalize the input\n",
    "train_input/=255\n",
    "test_input/=255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_shapes_Net = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Siamese_net_auxiliary(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Siamese_net_auxiliary, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(256, 10)\n",
    "        self.fc2 = nn.Linear(20, 2)\n",
    "\n",
    "    def forward(self, data):\n",
    "        if print_shapes_Net:\n",
    "            print(\"initial\", data.shape) #100 2 14 14\n",
    "            \n",
    "        class_layer = []\n",
    "        final_layer = []\n",
    "        for i in range(2):\n",
    "            x = data[:,i,:,:]\n",
    "            len0 = x.shape[0]\n",
    "            x = torch.reshape(x, (len0, 1, 14, 14))\n",
    "            \n",
    "            if print_shapes_Net:\n",
    "                print(\"X START\",x.shape) \n",
    "            \n",
    "            x = F.relu(F.max_pool2d(self.conv1(x), kernel_size=2, stride=2))\n",
    "            if print_shapes_Net:\n",
    "                print(\"conv1\",x.shape) \n",
    "                \n",
    "            x = F.relu(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2))\n",
    "            if print_shapes_Net:\n",
    "                print(\"conv2\",x.shape)\n",
    "            \n",
    "            x = F.relu(self.fc1(x.view(-1, 256)))\n",
    "            if print_shapes_Net:\n",
    "                print(\"fc1\",x.shape) \n",
    "                \n",
    "            final_layer.append(x)\n",
    "            class_layer.append(x.reshape(x.shape[0], 1, 10))\n",
    "            \n",
    "        final_layer = torch.cat((final_layer[1], final_layer[0]), 1)\n",
    "        class_layer = torch.cat((class_layer[1], class_layer[0]), 1)\n",
    "        \n",
    "        if print_shapes_Net:\n",
    "                print(\"class layer\",class_layer.shape)\n",
    "                \n",
    "        final_layer = self.fc2(final_layer)\n",
    "        if print_shapes_Net:\n",
    "            print(\"final\",final_layer.shape) \n",
    "            \n",
    "        return class_layer, final_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_input, train_classes, train_target, mini_batch_size, lr, nb_epoch):\n",
    "    criterion = nn.MSELoss() #CHANGE LOSS TO CROSS ENTROPY?\n",
    "    optimizer = optim.Adam(model.parameters(), lr)\n",
    "    \n",
    "    for e in range(nb_epoch):\n",
    "        sum_loss = 0\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "          \n",
    "            output_class, output_target = model(train_input.narrow(0, b, mini_batch_size))\n",
    "            \n",
    "            loss_class = criterion(output_class, train_classes.narrow(0, b, mini_batch_size))\n",
    "            loss_target = criterion(output_target, train_target.narrow(0, b, mini_batch_size))\n",
    "            model.zero_grad()\n",
    "            loss = loss_class + loss_target\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            sum_loss = sum_loss + loss.item()\n",
    "        #print(e, sum_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors_targets(model, input, target):\n",
    "    nb_errors = 0\n",
    "    _, output = model(input)\n",
    "    _, predicted_target = output.max(1) #max probabilities of target\n",
    "\n",
    "    for b in range(1000):\n",
    "        if target[b,int(predicted_target[b])] <= 0:\n",
    "            nb_errors = nb_errors + 1\n",
    "            \n",
    "    return nb_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors_classes(model, input, target):\n",
    "    nb_errors = 0\n",
    "\n",
    "    output,_ = model(input)\n",
    "    _, predicted_classes = output.max(2)\n",
    "\n",
    "    for b in range(input.shape[0]):\n",
    "        if target[b][0][predicted_classes[b][0]] <= 0:\n",
    "            nb_errors = nb_errors + 1\n",
    "        if target[b][1][predicted_classes[b][1]] <= 0:\n",
    "            nb_errors = nb_errors + 1\n",
    "\n",
    "    return nb_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_target[1000,1]\n",
    "new_train_target = torch.empty(1000,2)\n",
    "new_test_target = torch.empty(1000,2)\n",
    "for i in range(1000):\n",
    "    if train_target[i] == 1 :\n",
    "        new_train_target[i,0] = 0\n",
    "        new_train_target[i,1] = 1\n",
    "        \n",
    "    else:\n",
    "        new_train_target[i,0] = 1\n",
    "        new_train_target[i,1] = 0\n",
    "        \n",
    "    if test_target[i] == 1:\n",
    "        new_test_target[i,0] = 0\n",
    "        new_test_target[i,1] = 1\n",
    "        \n",
    "    else:\n",
    "        new_test_target[i,0] = 1\n",
    "        new_test_target[i,1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_classes[1000, 2]\n",
    "new_train_classes = torch.zeros(1000, 2, 10)\n",
    "new_test_classes = torch.zeros(1000, 2, 10)\n",
    "\n",
    "for i in range(train_classes.shape[0]): #\n",
    "    new_train_classes[i][0][train_classes[i][0]] = 1\n",
    "    new_train_classes[i][1][train_classes[i][1]] = 1\n",
    "\n",
    "for i in range(test_classes.shape[0]):\n",
    "    new_test_classes[i][0][test_classes[i][0]] = 1\n",
    "    new_test_classes[i][1][test_classes[i][1]] = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "####predict class of each digit\n",
    "nb_repetitions = 10\n",
    "nb_train_errors = []\n",
    "nb_test_errors = []\n",
    "lr = 0.001\n",
    "nb_epoch = 25\n",
    "\n",
    "for i in range (nb_repetitions):\n",
    "    model = Siamese_net_auxiliary()\n",
    "    train_model(model, train_input, new_train_classes, new_train_target, mini_batch_size, lr, nb_epoch)\n",
    "    \n",
    "    nb_train_errors.append(compute_nb_errors_targets(model, train_input, new_train_target))\n",
    "    nb_test_errors.append(compute_nb_errors_targets(model, test_input, new_test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "###ANALYSES OF THE RESULTS\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train error 6.42% 64.20/1000\n",
      "Train error standard deviation : 41.94\n",
      "Average test error 16.96% 169.60/1000\n",
      "Test error standard deviation : 21.17\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "###ANALYSES OF THE RESULTS\n",
    "print('Average train error {:0.2f}% {:0.2f}/{:d}'.format((100 * np.mean(nb_train_errors)) / train_input.size(0),\n",
    "                                                      np.mean(nb_train_errors), train_input.size(0)))\n",
    "print(\"Train error standard deviation : {:0.2f}\".format(np.std(nb_train_errors)))\n",
    "\n",
    "print('Average test error {:0.2f}% {:0.2f}/{:d}'.format((100 * np.mean(nb_test_errors)) / test_input.size(0),\n",
    "                                                      np.mean(nb_test_errors), test_input.size(0)))\n",
    "print(\"Test error standard deviation : {:0.2f}\".format(np.std(nb_test_errors)))\n",
    "\n",
    "\n",
    "train_err = [x*100 / train_input.size(0) for x in nb_train_errors]\n",
    "test_err = [x*100 / test_input.size(0) for x in nb_test_errors]\n",
    "\n",
    "plt.figure\n",
    "plt.title(\"Train and test errors\")\n",
    "plt.boxplot([train_err, test_err], labels=['train error', 'test error'])\n",
    "plt.ylabel(\"Error percentage\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################\n",
    "#j'ai laissé tes trucs en bas au cas où"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "####predict class of each digit\n",
    "for k in range(1):\n",
    "    model = Siamese_net_auxiliary()\n",
    "    lr = 0.005\n",
    "    nb_epoch = 25\n",
    "    train_model(model, train_input, new_train_classes, new_train_target, mini_batch_size, lr, nb_epoch)\n",
    "    \n",
    "    nb_train_errors = compute_nb_errors_targets(model, train_input, new_train_target)\n",
    "    print('train error Net_auxiliary_loss {:0.2f}% {:d}/{:d}'.format((100 * nb_train_errors) / train_input.size(0),\n",
    "                                                      nb_train_errors, train_input.size(0)))\n",
    "    nb_test_errors = compute_nb_errors_targets(model, test_input, new_test_target)\n",
    "    print('test error Net_auxiliary_loss {:0.2f}% {:d}/{:d}'.format((100 * nb_test_errors) / test_input.size(0),\n",
    "                                                    nb_test_errors, test_input.size(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_train_errors = nb_train_errors.append(compute_nb_errors_targets(model, train_input, new_train_target))\n",
    "print('train error Net_auxiliary_loss {:0.2f}% {:d}/{:d}'.format((100 * nb_train_errors) / train_input.size(0),\n",
    "                                                  nb_train_errors, train_input.size(0)))\n",
    "nb_test_errors = nb_test_errors.append(compute_nb_errors_targets(model, test_input, new_test_target))\n",
    "print('test error Net_auxiliary_loss {:0.2f}% {:d}/{:d}'.format((100 * nb_test_errors) / test_input.size(0),\n",
    "                                                nb_test_errors, test_input.size(0)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_train_errors_class = compute_nb_errors_classes(model, train_input, new_train_classes)\n",
    "print('train error Net_auxiliary_loss {:0.2f}% {:d}/{:d}'.format((100 * nb_train_errors_class) / (2*train_input.size(0)),\n",
    "                                                  nb_train_errors_class, 2*train_input.size(0)))\n",
    "nb_test_errors_class = compute_nb_errors_classes(model, test_input, new_test_classes)\n",
    "print('test error Net_auxiliary_loss {:0.2f}% {:d}/{:d}'.format((100 * nb_test_errors_class) / (2*test_input.size(0)),\n",
    "                                                nb_test_errors_class, 2*test_input.size(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### lr = 0.0005 seems to not overfit\n",
    "for lr in [0.0005, 0.001,0.005, 0.01, 0.05, 0.1, 0.5]:\n",
    "    model = Siamese_net_auxiliary()\n",
    "    nb_epoch = 25\n",
    "    train_model(model, train_input, new_train_classes, new_train_target, mini_batch_size, lr, nb_epoch)\n",
    "    \n",
    "    print(\"LR\", lr)\n",
    "    nb_train_errors = compute_nb_errors_targets(model, train_input, new_train_target)\n",
    "    print('train error Net_auxiliary_loss {:0.2f}% {:d}/{:d}'.format((100 * nb_train_errors) / train_input.size(0),\n",
    "                                                      nb_train_errors, train_input.size(0)))\n",
    "    nb_test_errors = compute_nb_errors_targets(model, test_input, new_test_target)\n",
    "    print('test error Net_auxiliary_loss {:0.2f}% {:d}/{:d}'.format((100 * nb_test_errors) / test_input.size(0),\n",
    "                                                    nb_test_errors, test_input.size(0)))\n",
    "    \n",
    "    nb_train_errors_class = compute_nb_errors_classes(model, train_input, new_train_classes)\n",
    "    print('train error Net_auxiliary_loss {:0.2f}% {:d}/{:d}'.format((100 * nb_train_errors_class) / train_input.size(0),\n",
    "                                                      nb_train_errors_class, train_input.size(0)))\n",
    "    nb_test_errors_class = compute_nb_errors_classes(model, test_input, new_test_classes)\n",
    "    print('test error Net_auxiliary_loss {:0.2f}% {:d}/{:d}'.format((100 * nb_test_errors_class) / test_input.size(0),\n",
    "                                                    nb_test_errors_class, test_input.size(0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####predict class of each digit\n",
    "for k in range(10):\n",
    "    model = Siamese_net_auxiliary()\n",
    "    lr = 0.0005\n",
    "    nb_epoch = 25\n",
    "    train_model(model, train_input, new_train_classes, new_train_target, mini_batch_size, lr, nb_epoch)\n",
    "    \n",
    "    nb_train_errors = compute_nb_errors_targets(model, train_input, new_train_target)\n",
    "    print('train error Net_auxiliary_loss {:0.2f}% {:d}/{:d}'.format((100 * nb_train_errors) / train_input.size(0),\n",
    "                                                      nb_train_errors, train_input.size(0)))\n",
    "    nb_test_errors = compute_nb_errors_targets(model, test_input, new_test_target)\n",
    "    print('test error Net_auxiliary_loss {:0.2f}% {:d}/{:d}'.format((100 * nb_test_errors) / test_input.size(0),\n",
    "                                                    nb_test_errors, test_input.size(0)))\n",
    "    \n",
    "    nb_train_errors_class = compute_nb_errors_classes(model, train_input, new_train_classes)\n",
    "    print('train error Net_auxiliary_loss {:0.2f}% {:d}/{:d}'.format((100 * nb_train_errors_class) / train_input.size(0),\n",
    "                                                      nb_train_errors_class, train_input.size(0)))\n",
    "    nb_test_errors_class = compute_nb_errors_classes(model, test_input, new_test_classes)\n",
    "    print('test error Net_auxiliary_loss {:0.2f}% {:d}/{:d}'.format((100 * nb_test_errors_class) / test_input.size(0),\n",
    "                                                    nb_test_errors_class, test_input.size(0)))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
